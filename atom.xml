<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Quentin&#39;s Blog</title>
  
  <subtitle>每天进步一点点</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-02-27T17:22:51.289Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>ByiProX</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>从零开始学爬虫-05</title>
    <link href="http://yoursite.com/2018/02/28/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E7%88%AC%E8%99%AB-05/"/>
    <id>http://yoursite.com/2018/02/28/从零开始学爬虫-05/</id>
    <published>2018-02-27T17:17:37.000Z</published>
    <updated>2018-02-27T17:22:51.289Z</updated>
    
    <content type="html"><![CDATA[<h2 id="为什么要使用Cookie"><a href="#为什么要使用Cookie" class="headerlink" title="为什么要使用Cookie"></a>为什么要使用Cookie</h2><p>Cookie，指某些网站为了辨别用户身份、进行session跟踪而储存在用户本地终端上的数据（通常经过加密)。<br>比如说有些网站需要登录后才能访问某个页面，在登录之前，你想抓取某个页面内容，登陆前与登陆后是不同的，或者不允许的。<br>使用Cookie和使用代理IP一样，也需要创建一个自己的opener。在HTTP包中，提供了cookiejar模块，用于提供对Cookie的支持。</p><p><img src="http://img.blog.csdn.net/20170409144243654?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""><br><a id="more"></a></p><p>http.cookiejar功能强大，我们可以利用本模块的CookieJar类的对象来捕获cookie并在后续连接请求时重新发送，比如可以实现模拟登录功能。该模块主要的对象有CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar。</p><p><strong>它们的关系：</strong> CookieJar–派生–&gt;FileCookieJar–派生–&gt;MozillaCookieJar和LWPCookieJar</p><p><strong>工作原理：</strong> 创建一个带有cookie的opener，在访问登录的URL时，将登录后的cookie保存下来，然后利用这个cookie来访问其他网址。查看登录之后才能看到的信息。</p><p>同样，我们以实例进行讲解，爬取伯乐在线的面向对象的漂亮MM的邮箱联系方式。</p><h2 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h2><h3 id="1-背景介绍"><a href="#1-背景介绍" class="headerlink" title="(1).背景介绍"></a>(1).背景介绍</h3><p>在伯乐在线有这么一个有趣的模块，面向对象，它说白了就是提供了一个程序员(媛)网上相亲的平台。</p><p><strong>URL:</strong> <a href="http://date.jobbole.com/" target="_blank" rel="noopener">http://date.jobbole.com/</a></p><p>它的样子是这样的：</p><p><img src="http://img.blog.csdn.net/20170409144753813?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p><p>可以看到，这里有很多的相亲贴，随便点进去就会有网上相亲MM的详细信息，想获取MM的联系方式，需要积分，积分可以通过签到的方式获取。如果没有登陆账户，获取联系方式的地方是这个样子的：</p><p><img src="http://img.blog.csdn.net/20170409144912938?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p><p>如果登陆了账号，获取联系方式的地方是这个样子的：</p><p><img src="http://img.blog.csdn.net/20170409144955289?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p><p>想要爬取MM的联系邮箱，就需要用到我们本次讲到的知识，Cookie的使用。当然，首先你积分也得够。</p><p>在讲解之前，推荐一款抓包工具–Fiddler，可以在Google Chrome的Google商店下载这个插件，它的样子是这样的：</p><p><img src="http://img.blog.csdn.net/20170409145106869?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p><p>可以看到，通过这个插件，我们可以很容易找到Post的Form Data等信息，很方便，当然也可以用之前讲得浏览器审查元素的方式查看这些信息。</p><h3 id="2-过程分析"><a href="#2-过程分析" class="headerlink" title="(2).过程分析"></a>(2).过程分析</h3><p>在伯乐在线首页点击登陆的按钮，Fiddler的抓包内容如下：</p><p><img src="http://img.blog.csdn.net/20170409145240590?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p><p>从上图可以看出，真正请求的url是</p><p> <a href="http://www.jobbole.com/wp-admin/admin-ajax.php" target="_blank" rel="noopener">http://www.jobbole.com/wp-admin/admin-ajax.php</a></p><p>Form Data的内容记住，这些是我们编程需要用到的。user_login是用户名，user_pass是用户密码。</p><p>在点击取得联系邮箱按钮的时候，Fiddler的抓包内容如下：</p><p><img src="http://img.blog.csdn.net/20170409145403065?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p><p>从上图可以看出，此刻真正请求的url是</p><p> <a href="http://date.jobbole.com/wp-admin/admin-ajax.php" target="_blank" rel="noopener">http://date.jobbole.com/wp-admin/admin-ajax.php</a></p><p>同样Form Data中内容要记下来。postId是每个帖子的id。例如，打开一个相亲贴，它的URL是<a href="http://date.jobbole.com/4128/" target="_blank" rel="noopener">http://date.jobbole.com/4128/</a>，那么它的这个postId就是4128。为了简化程序，这里就不讲解如何自动获取这个postId了，本实例直接指定postId。如果想要自动获取，可以使用beautifulsoup解析<a href="http://date.jobbole.com/" target="_blank" rel="noopener">http://date.jobbole.com/</a>返回的信息。beautifulsoup的使用。有机会的话，会在后面的爬虫笔记中进行讲解。</p><h3 id="3-测试"><a href="#3-测试" class="headerlink" title="(3).测试"></a>(3).测试</h3><p><strong>1)将Cookie保存到变量中</strong></p><p>首先，我们先利用CookieJar对象实现获取cookie的功能，存储到变量中，先来感受一下,执行文件cookie01.py：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> http <span class="keyword">import</span> cookiejar</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment">#声明一个CookieJar对象实例来保存cookie</span></span><br><span class="line">    cookie = cookiejar.CookieJar()</span><br><span class="line">    <span class="comment">#利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler</span></span><br><span class="line">    handler=request.HTTPCookieProcessor(cookie)</span><br><span class="line">    <span class="comment">#通过CookieHandler创建opener</span></span><br><span class="line">    opener = request.build_opener(handler)</span><br><span class="line">    <span class="comment">#此处的open方法打开网页</span></span><br><span class="line">    response = opener.open(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line">    <span class="comment">#打印cookie信息</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> cookie:</span><br><span class="line">        print(<span class="string">'Name = %s'</span> % item.name)</span><br><span class="line">        print(<span class="string">'Value = %s'</span> % item.value)</span><br></pre></td></tr></table></figure><p>我们使用以上方法将cookie保存到变量中，然后打印出了cookie中的值，运行结果如下:</p><p><img src="http://img.blog.csdn.net/20170409145652613?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p><p><strong>2)保存Cookie到文件</strong></p><p>在上面的方法中，我们将cookie保存到了cookie这个变量中，如果我们想将cookie保存到文件中该怎么做呢？方便以后直接读取文件使用，这时，我们就要用到FileCookieJar这个对象了，在这里我们使用它的子类MozillaCookieJar来实现Cookie的保存，编写代码如下,文件为cookie02.py：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> http <span class="keyword">import</span> cookiejar</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment">#设置保存cookie的文件，同级目录下的cookie.txt</span></span><br><span class="line">    filename = <span class="string">'cookie.txt'</span></span><br><span class="line">    <span class="comment">#声明一个MozillaCookieJar对象实例来保存cookie，之后写入文件</span></span><br><span class="line">    cookie = cookiejar.MozillaCookieJar(filename)</span><br><span class="line">    <span class="comment">#利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler</span></span><br><span class="line">    handler=request.HTTPCookieProcessor(cookie)</span><br><span class="line">    <span class="comment">#通过CookieHandler创建opener</span></span><br><span class="line">    opener = request.build_opener(handler)</span><br><span class="line">    <span class="comment">#此处的open方法打开网页</span></span><br><span class="line">    response = opener.open(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line">    <span class="comment">#保存cookie到文件</span></span><br><span class="line">    cookie.save(ignore_discard=<span class="keyword">True</span>, ignore_expires=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure></p><p>cookie.save的参数说明：</p><ul><li><p>ignore_discard的意思是即使cookies将被丢弃也将它保存下来；</p></li><li><p>ignore_expires的意思是如果在该文件中cookies已经存在，则覆盖原文件写入。</p></li></ul><p>在这里，我们将这两个全部设置为True。</p><p>运行之后，cookies将被保存到cookie.txt文件中。我们可以查看自己查看下cookie.txt这个文件的内容。</p><p><strong>3)从文件中获取Cookie并访问</strong></p><p>我们已经做到把Cookie保存到文件中了，如果以后想使用，可以利用下面的方法来读取cookie并访问网站，感受一下cookie03.py：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> http <span class="keyword">import</span> cookiejar</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment">#设置保存cookie的文件的文件名,相对路径,也就是同级目录下</span></span><br><span class="line">    filename = <span class="string">'cookie.txt'</span></span><br><span class="line">    <span class="comment">#创建MozillaCookieJar实例对象</span></span><br><span class="line">    cookie = cookiejar.MozillaCookieJar()</span><br><span class="line">    <span class="comment">#从文件中读取cookie内容到变量</span></span><br><span class="line">    cookie.load(filename, ignore_discard=<span class="keyword">True</span>, ignore_expires=<span class="keyword">True</span>)</span><br><span class="line">    <span class="comment">#利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler</span></span><br><span class="line">    handler=request.HTTPCookieProcessor(cookie)</span><br><span class="line">    <span class="comment">#通过CookieHandler创建opener</span></span><br><span class="line">    opener = request.build_opener(handler)</span><br><span class="line">    <span class="comment">#此用opener的open方法打开网页</span></span><br><span class="line">    response = opener.open(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line">    <span class="comment">#打印信息</span></span><br><span class="line">    print(response.read().decode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure></p><p>了解到以上内容，我们那就可以开始正式编写模拟登陆伯乐在线的程序了。同时，我们也可以获取相亲MM的联系方式。</p><h3 id="4-编写代码"><a href="#4-编写代码" class="headerlink" title="(4).编写代码"></a>(4).编写代码</h3><p>我们利用CookieJar对象实现获取cookie的功能，存储到变量中。然后使用这个cookie变量创建opener，使用这个设置好cookie的opener即可模拟登陆，同笔记四中讲到的IP代理的使用方法类似。</p><p>创建cookie_test.py文件，编写代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> error</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</span><br><span class="line"><span class="keyword">from</span> http <span class="keyword">import</span> cookiejar</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment">#登陆地址</span></span><br><span class="line">    login_url = <span class="string">'http://www.jobbole.com/wp-admin/admin-ajax.php'</span>    </span><br><span class="line">    <span class="comment">#User-Agent信息                   </span></span><br><span class="line">    user_agent = <span class="string">r'Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36'</span></span><br><span class="line">    <span class="comment">#Headers信息</span></span><br><span class="line">    head = &#123;<span class="string">'User-Agnet'</span>: user_agent, <span class="string">'Connection'</span>: <span class="string">'keep-alive'</span>&#125;</span><br><span class="line">    <span class="comment">#登陆Form_Data信息</span></span><br><span class="line">    Login_Data = &#123;&#125;</span><br><span class="line">    Login_Data[<span class="string">'action'</span>] = <span class="string">'user_login'</span></span><br><span class="line">    Login_Data[<span class="string">'redirect_url'</span>] = <span class="string">'http://www.jobbole.com/'</span></span><br><span class="line">    Login_Data[<span class="string">'remember_me'</span>] = <span class="string">'0'</span>         <span class="comment">#是否一个月内自动登陆</span></span><br><span class="line">    Login_Data[<span class="string">'user_login'</span>] = <span class="string">'********'</span>       <span class="comment">#改成你自己的用户名</span></span><br><span class="line">    Login_Data[<span class="string">'user_pass'</span>] = <span class="string">'********'</span>        <span class="comment">#改成你自己的密码</span></span><br><span class="line">    <span class="comment">#使用urlencode方法转换标准格式</span></span><br><span class="line">    logingpostdata = parse.urlencode(Login_Data).encode(<span class="string">'utf-8'</span>)</span><br><span class="line">    <span class="comment">#声明一个CookieJar对象实例来保存cookie</span></span><br><span class="line">    cookie = cookiejar.CookieJar()</span><br><span class="line">    <span class="comment">#利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler</span></span><br><span class="line">    cookie_support = request.HTTPCookieProcessor(cookie)</span><br><span class="line">    <span class="comment">#通过CookieHandler创建opener</span></span><br><span class="line">    opener = request.build_opener(cookie_support)</span><br><span class="line">    <span class="comment">#创建Request对象</span></span><br><span class="line">    req1 = request.Request(url=login_url, data=logingpostdata, headers=head)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#面向对象地址</span></span><br><span class="line">    date_url = <span class="string">'http://date.jobbole.com/wp-admin/admin-ajax.php'</span></span><br><span class="line">    <span class="comment">#面向对象</span></span><br><span class="line">    Date_Data = &#123;&#125;</span><br><span class="line">    Date_Data[<span class="string">'action'</span>] = <span class="string">'get_date_contact'</span></span><br><span class="line">    Date_Data[<span class="string">'postId'</span>] = <span class="string">'4128'</span></span><br><span class="line">    <span class="comment">#使用urlencode方法转换标准格式</span></span><br><span class="line">    datepostdata = parse.urlencode(Date_Data).encode(<span class="string">'utf-8'</span>)</span><br><span class="line">    req2 = request.Request(url=date_url, data=datepostdata, headers=head)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment">#使用自己创建的opener的open方法</span></span><br><span class="line">        response1 = opener.open(req1)</span><br><span class="line">        response2 = opener.open(req2)</span><br><span class="line">        html = response2.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">        index = html.find(<span class="string">'jb_contact_email'</span>)</span><br><span class="line">        <span class="comment">#打印查询结果</span></span><br><span class="line">        print(<span class="string">'联系邮箱:%s'</span> % html[index+<span class="number">19</span>:<span class="number">-2</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">        <span class="keyword">if</span> hasattr(e, <span class="string">'code'</span>):</span><br><span class="line">            print(<span class="string">"HTTPError:%d"</span> % e.code)</span><br><span class="line">        <span class="keyword">elif</span> hasattr(e, <span class="string">'reason'</span>):</span><br><span class="line">            print(<span class="string">"URLError:%s"</span> % e.reason)</span><br></pre></td></tr></table></figure></p><h3 id="5-运行结果如下"><a href="#5-运行结果如下" class="headerlink" title="(5).运行结果如下"></a>(5).运行结果如下</h3><p><img src="http://img.blog.csdn.net/20170409150252854?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""><br>。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;为什么要使用Cookie&quot;&gt;&lt;a href=&quot;#为什么要使用Cookie&quot; class=&quot;headerlink&quot; title=&quot;为什么要使用Cookie&quot;&gt;&lt;/a&gt;为什么要使用Cookie&lt;/h2&gt;&lt;p&gt;Cookie，指某些网站为了辨别用户身份、进行session跟踪而储存在用户本地终端上的数据（通常经过加密)。&lt;br&gt;比如说有些网站需要登录后才能访问某个页面，在登录之前，你想抓取某个页面内容，登陆前与登陆后是不同的，或者不允许的。&lt;br&gt;使用Cookie和使用代理IP一样，也需要创建一个自己的opener。在HTTP包中，提供了cookiejar模块，用于提供对Cookie的支持。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://img.blog.csdn.net/20170409144243654?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Spider" scheme="http://yoursite.com/categories/Spider/"/>
    
      <category term="Urllib" scheme="http://yoursite.com/categories/Spider/Urllib/"/>
    
    
      <category term="Python3" scheme="http://yoursite.com/tags/Python3/"/>
    
      <category term="Spider" scheme="http://yoursite.com/tags/Spider/"/>
    
      <category term="Urllib" scheme="http://yoursite.com/tags/Urllib/"/>
    
  </entry>
  
  <entry>
    <title>从零开始学爬虫-04</title>
    <link href="http://yoursite.com/2018/02/28/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E7%88%AC%E8%99%AB-04/"/>
    <id>http://yoursite.com/2018/02/28/从零开始学爬虫-04/</id>
    <published>2018-02-27T17:06:56.000Z</published>
    <updated>2018-02-27T17:18:25.138Z</updated>
    
    <content type="html"><![CDATA[<h2 id="说在前面"><a href="#说在前面" class="headerlink" title="说在前面"></a>说在前面</h2><p>urllib_test10.py已经无法爬取了，因为原网站已经添加了防爬虫机制</p><h2 id="一-、为何要设置User-Agent"><a href="#一-、为何要设置User-Agent" class="headerlink" title="(一)、为何要设置User Agent"></a>(一)、为何要设置User Agent</h2><p>有一些网站不喜欢被爬虫程序访问，所以会检测连接对象，如果是爬虫程序，也就是非人点击访问，它就会不让你继续访问，所以为了要让程序可以正常运行，需要隐藏自己的爬虫程序的身份。此时，我们就可以通过设置User Agent的来达到隐藏身份的目的，User Agent的中文名为用户代理，简称UA。</p><p>User Agent存放于Headers中，服务器就是通过查看Headers中的User Agent来判断是谁在访问。在Python中，如果不设置User Agent，程序将使用默认的参数，那么这个User Agent就会有Python的字样，如果服务器检查User Agent，那么没有设置User Agent的Python程序将无法正常访问网站。</p><p>Python允许我们修改这个User Agent来模拟浏览器访问，它的强大毋庸置疑。<br><a id="more"></a></p><h2 id="二-、常见的User-Agent"><a href="#二-、常见的User-Agent" class="headerlink" title="(二)、常见的User Agent"></a>(二)、常见的User Agent</h2><h3 id="1-Android"><a href="#1-Android" class="headerlink" title="(1).Android"></a>(1).Android</h3><ul><li>Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19</li><li>Mozilla/5.0 (Linux; U; Android 4.0.4; en-gb; GT-I9300 Build/IMM76D) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30</li><li>Mozilla/5.0 (Linux; U; Android 2.2; en-gb; GT-P1000 Build/FROYO) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1</li></ul><h3 id="2-Firefox"><a href="#2-Firefox" class="headerlink" title="(2).Firefox"></a>(2).Firefox</h3><ul><li>Mozilla/5.0 (Windows NT 6.2; WOW64; rv:21.0) Gecko/20100101 Firefox/21.0</li><li>Mozilla/5.0 (Android; Mobile; rv:14.0) Gecko/14.0 Firefox/14.0</li></ul><h3 id="3-Google-Chrome"><a href="#3-Google-Chrome" class="headerlink" title="(3).Google Chrome"></a>(3).Google Chrome</h3><ul><li>Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36</li><li>Mozilla/5.0 (Linux; Android 4.0.4; Galaxy Nexus Build/IMM76B) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.133 Mobile Safari/535.19</li></ul><h3 id="4-iOS"><a href="#4-iOS" class="headerlink" title="(4).iOS"></a>(4).iOS</h3><ul><li>Mozilla/5.0 (iPad; CPU OS 5_0 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9A334 Safari/7534.48.3</li><li>Mozilla/5.0 (iPod; U; CPU like Mac OS X; en) AppleWebKit/420.1 (KHTML, like Gecko) Version/3.0 Mobile/3A101a Safari/419.3</li></ul><p>上面列举了Andriod、Firefox、Google Chrome、iOS的一些User Agent，直接copy就能用。</p><h2 id="三-、设置User-Agent的方法"><a href="#三-、设置User-Agent的方法" class="headerlink" title="(三)、设置User Agent的方法"></a>(三)、设置User Agent的方法</h2><p>先看下urllib.request.Request()</p><p> <img src="http://img.blog.csdn.net/20170303123244632?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="1"></p><p>从上图可以看出，在创建Request对象的时候，可以传入headers参数。因此，想要设置User Agent，有两种方法：</p><ul><li><p>1.在创建Request对象的时候，填入headers参数(包含User Agent信息)，这个Headers参数要求为字典；</p></li><li><p>2.在创建Request对象的时候不添加headers参数，在创建完成之后，使用add_header()的方法，添加headers。</p></li></ul><h3 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h3><p>创建文件urllib_test08.py，使用上面提到的Android的第一个User Agent，在创建Request对象的时候传入headers参数，编写代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    url = <span class="string">'http://www.csdn.net/'</span></span><br><span class="line">    head = &#123;&#125;</span><br><span class="line">    <span class="comment">#写入User Agent信息</span></span><br><span class="line">    head[<span class="string">'User-Agent'</span>] = <span class="string">'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19'</span></span><br><span class="line">    <span class="comment">#创建Request对象</span></span><br><span class="line">    req = request.Request(url, headers=head)</span><br><span class="line">    <span class="comment">#传入创建好的Request对象</span></span><br><span class="line">    response = request.urlopen(req)</span><br><span class="line">    <span class="comment">#读取响应信息并解码</span></span><br><span class="line">    html = response.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    <span class="comment">#打印信息</span></span><br><span class="line">    print(html)</span><br></pre></td></tr></table></figure></p><p>运行结果如下：</p><p> <img src="http://img.blog.csdn.net/20170303123738649?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="2"></p><h3 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h3><p>创建文件urllib_test09.py，使用上面提到的Android的第一个User Agent，在创建Request对象时不传入headers参数，创建之后使用add_header()方法，添加headers，编写代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment">#以CSDN为例，CSDN不更改User Agent是无法访问的</span></span><br><span class="line">    url = <span class="string">'http://www.csdn.net/'</span></span><br><span class="line">    <span class="comment">#创建Request对象</span></span><br><span class="line">    req = request.Request(url)</span><br><span class="line">    <span class="comment">#传入headers</span></span><br><span class="line">    req.add_header(<span class="string">'User-Agent'</span>, <span class="string">'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19'</span>)</span><br><span class="line">    <span class="comment">#传入创建好的Request对象</span></span><br><span class="line">    response = request.urlopen(req)</span><br><span class="line">    <span class="comment">#读取响应信息并解码</span></span><br><span class="line">    html = response.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    <span class="comment">#打印信息</span></span><br><span class="line">    print(html)</span><br></pre></td></tr></table></figure></p><p>运行结果和上一个方法是一样的。</p><h2 id="四-、IP代理的使用"><a href="#四-、IP代理的使用" class="headerlink" title="(四)、IP代理的使用"></a>(四)、IP代理的使用</h2><h3 id="1-为何使用IP代理"><a href="#1-为何使用IP代理" class="headerlink" title="(1).为何使用IP代理"></a>(1).为何使用IP代理</h3><p>User Agent已经设置好了，但是还应该考虑一个问题，程序的运行速度是很快的，如果我们利用一个爬虫程序在网站爬取东西，一个固定IP的访问频率就会很高，这不符合人为操作的标准，因为人操作不可能在几ms内，进行如此频繁的访问。所以一些网站会设置一个IP访问频率的阈值，如果一个IP访问频率超过这个阈值，说明这个不是人在访问，而是一个爬虫程序。</p><h3 id="2-一般步骤说明"><a href="#2-一般步骤说明" class="headerlink" title="(2).一般步骤说明"></a>(2).一般步骤说明</h3><p>一个很简单的解决办法就是设置延时，但是这显然不符合爬虫快速爬取信息的目的，所以另一种更好的方法就是使用IP代理。使用代理的步骤：</p><p><strong>(1)</strong> 调用urlib.request.ProxyHandler()，proxies参数为一个字典。</p><p> <img src="http://img.blog.csdn.net/20170303124421012?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="4"></p><p><strong>(2)</strong> 创建Opener(类似于urlopen，这个代开方式是我们自己定制的)</p><p><img src="http://img.blog.csdn.net/20170303124447169?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="5"></p><p><strong>(3)</strong> 安装Opener</p><p><img src="http://img.blog.csdn.net/20170303124507044?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="引用容"></p><p>使用install_opener方法之后，会将程序默认的urlopen方法替换掉。也就是说，如果使用install_opener之后，在该文件中，再次调用urlopen会使用自己创建好的opener。如果不想替换掉，只是想临时使用一下，可以使用opener.open(url)，这样就不会对程序默认的urlopen有影响。</p><h3 id="3-代理IP选取"><a href="#3-代理IP选取" class="headerlink" title="(3).代理IP选取"></a>(3).代理IP选取</h3><p>在写代码之前，先在代理IP网站选好一个IP地址，推荐西刺代理IP。</p><p>URL：<a href="http://www.xicidaili.com/" target="_blank" rel="noopener">http://www.xicidaili.com/</a></p><p>注意：当然也可以写个正则表达式从网站直接爬取IP，但是要记住不要太频繁爬取，加个延时什么的，太频繁给服务器带来压力了，服务器会直接把你block，不让你访问的，我就被封了两天。</p><p>从西刺网站选出信号好的IP，我的选择如下：(106.46.136.112:808)</p><p> <img src="http://img.blog.csdn.net/20170303124651091?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="6"></p><p>编写代码访问<a href="http://www.whatismyip.com.tw/" target="_blank" rel="noopener">http://www.whatismyip.com.tw/</a>，该网站是测试自己IP为多少的网址，服务器会返回访问者的IP。</p><h3 id="4-代码实例"><a href="#4-代码实例" class="headerlink" title="(4).代码实例"></a>(4).代码实例</h3><p>创建文件urllib_test10.py，编写代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment">#访问网址</span></span><br><span class="line">    url = <span class="string">'http://www.whatismyip.com.tw/'</span></span><br><span class="line">    <span class="comment">#这是代理IP</span></span><br><span class="line">    proxy = &#123;<span class="string">'http'</span>:<span class="string">'106.46.136.112:808'</span>&#125;</span><br><span class="line">    <span class="comment">#创建ProxyHandler</span></span><br><span class="line">    proxy_support = request.ProxyHandler(proxy)</span><br><span class="line">    <span class="comment">#创建Opener</span></span><br><span class="line">    opener = request.build_opener(proxy_support)</span><br><span class="line">    <span class="comment">#添加User Angent</span></span><br><span class="line">    opener.addheaders = [(<span class="string">'User-Agent'</span>,<span class="string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'</span>)]</span><br><span class="line">    <span class="comment">#安装OPener</span></span><br><span class="line">    request.install_opener(opener)</span><br><span class="line">    <span class="comment">#使用自己安装好的Opener</span></span><br><span class="line">    response = request.urlopen(url)</span><br><span class="line">    <span class="comment">#读取相应信息并解码</span></span><br><span class="line">    html = response.read().decode(<span class="string">"utf-8"</span>)</span><br><span class="line">    <span class="comment">#打印信息</span></span><br><span class="line">    print(html)</span><br></pre></td></tr></table></figure></p><p>运行结果如下：<br><img src="http://img.blog.csdn.net/20170303124823038?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="7"></p><p>从上图可以看出，访问的IP已经伪装成了106.46.136.112。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;说在前面&quot;&gt;&lt;a href=&quot;#说在前面&quot; class=&quot;headerlink&quot; title=&quot;说在前面&quot;&gt;&lt;/a&gt;说在前面&lt;/h2&gt;&lt;p&gt;urllib_test10.py已经无法爬取了，因为原网站已经添加了防爬虫机制&lt;/p&gt;
&lt;h2 id=&quot;一-、为何要设置User-Agent&quot;&gt;&lt;a href=&quot;#一-、为何要设置User-Agent&quot; class=&quot;headerlink&quot; title=&quot;(一)、为何要设置User Agent&quot;&gt;&lt;/a&gt;(一)、为何要设置User Agent&lt;/h2&gt;&lt;p&gt;有一些网站不喜欢被爬虫程序访问，所以会检测连接对象，如果是爬虫程序，也就是非人点击访问，它就会不让你继续访问，所以为了要让程序可以正常运行，需要隐藏自己的爬虫程序的身份。此时，我们就可以通过设置User Agent的来达到隐藏身份的目的，User Agent的中文名为用户代理，简称UA。&lt;/p&gt;
&lt;p&gt;User Agent存放于Headers中，服务器就是通过查看Headers中的User Agent来判断是谁在访问。在Python中，如果不设置User Agent，程序将使用默认的参数，那么这个User Agent就会有Python的字样，如果服务器检查User Agent，那么没有设置User Agent的Python程序将无法正常访问网站。&lt;/p&gt;
&lt;p&gt;Python允许我们修改这个User Agent来模拟浏览器访问，它的强大毋庸置疑。&lt;br&gt;
    
    </summary>
    
      <category term="Spider" scheme="http://yoursite.com/categories/Spider/"/>
    
      <category term="Urllib" scheme="http://yoursite.com/categories/Spider/Urllib/"/>
    
    
      <category term="Python3" scheme="http://yoursite.com/tags/Python3/"/>
    
      <category term="Spider" scheme="http://yoursite.com/tags/Spider/"/>
    
      <category term="Urllib" scheme="http://yoursite.com/tags/Urllib/"/>
    
  </entry>
  
  <entry>
    <title>从零开始学爬虫-03</title>
    <link href="http://yoursite.com/2018/02/28/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E7%88%AC%E8%99%AB-03/"/>
    <id>http://yoursite.com/2018/02/28/从零开始学爬虫-03/</id>
    <published>2018-02-27T16:57:01.000Z</published>
    <updated>2018-02-27T17:04:43.829Z</updated>
    
    <content type="html"><![CDATA[<h2 id="urllib-error"><a href="#urllib-error" class="headerlink" title="urllib.error"></a>urllib.error</h2><p>urllib.error可以接收有urllib.request产生的异常。urllib.error有两个方法，URLError和HTTPError。如下图所示：<br><img src="http://upload-images.jianshu.io/upload_images/2952111-165a6b7bb4f6e5af.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Screen Shot 2018-02-12 at 14.39.09.png"></p><p>URLError是OSError的一个子类，HTTPError是URLError的一个子类，服务器上HTTP的响应会返回一个状态码，根据这个HTTP状态码，我们可以知道我们的访问是否成功。例如第二个笔记中提到的200状态码，表示请求成功，再比如常见的404错误等。<br><a id="more"></a></p><h3 id="1-URLError"><a href="#1-URLError" class="headerlink" title="(1).URLError"></a>(1).URLError</h3><p>让我们先看下URLError的异常，创建文件urllib_test05.py，编写如下代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> error</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment">#一个不存在的连接</span></span><br><span class="line">    url = <span class="string">"http://www.dskfclyfiydl.com/"</span></span><br><span class="line">    req = request.Request(url)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = request.urlopen(req)</span><br><span class="line">        html = response.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">        print(html)</span><br><span class="line">    <span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">        print(e.reason)</span><br></pre></td></tr></table></figure></p><p>可以看到如下运行结果：</p><p><img src="http://upload-images.jianshu.io/upload_images/2952111-5e9dfdc6af1af203.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Screen Shot 2018-02-12 at 14.35.56.png"></p><h3 id="2-HTTPError"><a href="#2-HTTPError" class="headerlink" title="(2).HTTPError"></a>(2).HTTPError</h3><p>再看下HTTPError异常，创建文件urllib_test06.py，编写如下代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> error</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment">#一个不存在的连接</span></span><br><span class="line">    url = <span class="string">"http://www.douyu.com/wkx.html"</span></span><br><span class="line">    req = request.Request(url)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        responese = request.urlopen(req)</span><br><span class="line">        <span class="comment"># html = responese.read()</span></span><br><span class="line">    <span class="keyword">except</span> error.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">        print(e.code, <span class="string">'\n'</span> ,e.reason, <span class="string">'\n'</span>, e.headers)</span><br></pre></td></tr></table></figure></p><p>运行之后，我们可以看到404，这说明请求的资源没有在服务器上找到，<a href="http://www.douyu.com" target="_blank" rel="noopener">www.douyu.com</a> 这个服务器是存在的，但是我们要查找的 Jack_Cui.html 资源是没有的，所以抛出404异常。</p><p><img src="http://upload-images.jianshu.io/upload_images/2952111-877b52f32e81d2cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Screen Shot 2018-02-12 at 14.36.07.png"></p><h3 id="3-URLError和HTTPError混合使用"><a href="#3-URLError和HTTPError混合使用" class="headerlink" title="(3).URLError和HTTPError混合使用"></a>(3).URLError和HTTPError混合使用</h3><p>最后值得注意的一点是，如果想用HTTPError和URLError一起捕获异常，那么需要将HTTPError放在URLError的前面，因为HTTPError是URLError的一个子类。如果URLError放在前面，出现HTTP异常会先响应URLError，这样HTTPError就捕获不到错误信息了。</p><p><img src="http://upload-images.jianshu.io/upload_images/2952111-81c31b50ef0e4f0d?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><p>如果不用上面的方法，也可以使用hasattr函数判断URLError含有的属性，如果含有reason属性表明是URLError，如果含有code属性表明是HTTPError。创建文件urllib_test07.py，编写代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> error</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment">#一个不存在的连接</span></span><br><span class="line">    url = <span class="string">"http://www.douyu.com/wkx.html"</span></span><br><span class="line">    req = request.Request(url)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        responese = request.urlopen(req)</span><br><span class="line">    <span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">        <span class="keyword">if</span> hasattr(e, <span class="string">'code'</span>):</span><br><span class="line">            print(<span class="string">"HTTPError"</span>)</span><br><span class="line">            print(e.code)</span><br><span class="line">        <span class="keyword">elif</span> hasattr(e, <span class="string">'reason'</span>):</span><br><span class="line">            print(<span class="string">"URLError"</span>)</span><br><span class="line">            print(e.reason)</span><br></pre></td></tr></table></figure></p><p>运行结果如下：</p><p><img src="http://upload-images.jianshu.io/upload_images/2952111-9105667f71cd7051.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Screen Shot 2018-02-12 at 14.37.39.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;urllib-error&quot;&gt;&lt;a href=&quot;#urllib-error&quot; class=&quot;headerlink&quot; title=&quot;urllib.error&quot;&gt;&lt;/a&gt;urllib.error&lt;/h2&gt;&lt;p&gt;urllib.error可以接收有urllib.request产生的异常。urllib.error有两个方法，URLError和HTTPError。如下图所示：&lt;br&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/2952111-165a6b7bb4f6e5af.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;Screen Shot 2018-02-12 at 14.39.09.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;URLError是OSError的一个子类，HTTPError是URLError的一个子类，服务器上HTTP的响应会返回一个状态码，根据这个HTTP状态码，我们可以知道我们的访问是否成功。例如第二个笔记中提到的200状态码，表示请求成功，再比如常见的404错误等。&lt;br&gt;
    
    </summary>
    
      <category term="Spider" scheme="http://yoursite.com/categories/Spider/"/>
    
      <category term="Urllib" scheme="http://yoursite.com/categories/Spider/Urllib/"/>
    
    
      <category term="Python3" scheme="http://yoursite.com/tags/Python3/"/>
    
      <category term="Spider" scheme="http://yoursite.com/tags/Spider/"/>
    
      <category term="Urllib" scheme="http://yoursite.com/tags/Urllib/"/>
    
  </entry>
  
  <entry>
    <title>从零开始学爬虫-02</title>
    <link href="http://yoursite.com/2018/02/28/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E7%88%AC%E8%99%AB-02/"/>
    <id>http://yoursite.com/2018/02/28/从零开始学爬虫-02/</id>
    <published>2018-02-27T16:48:24.000Z</published>
    <updated>2018-02-27T17:04:42.158Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一个疑问尚未解决疑问，小弟在此跪求大牛解答一下"><a href="#一个疑问尚未解决疑问，小弟在此跪求大牛解答一下" class="headerlink" title="一个疑问尚未解决疑问，小弟在此跪求大牛解答一下"></a>一个疑问尚未解决疑问，小弟在此跪求大牛解答一下</h3><p>为什么把url里的 “_o” 删掉后就可以正常爬取呢？<br><a id="more"></a></p><h3 id="urlopen的url参数-Agent"><a href="#urlopen的url参数-Agent" class="headerlink" title="urlopen的url参数 Agent"></a>urlopen的url参数 Agent</h3><p>url不仅可以是一个字符串，例如:<a href="http://www.baidu.com。" target="_blank" rel="noopener">http://www.baidu.com。</a></p><p>url也可以是一个Request对象，这就需要我们先定义一个Request对象，然后将这个Request对象作为urlopen的参数使用，方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    req = request.Request(<span class="string">"http://fanyi.baidu.com/"</span>)</span><br><span class="line">    response = request.urlopen(req)</span><br><span class="line">    html = response.read()</span><br><span class="line">    html = html.decode(<span class="string">"utf-8"</span>)</span><br><span class="line">    print(html)</span><br></pre></td></tr></table></figure><p>同样，运行这段代码同样可以得到网页信息。可以看一下这段代码和上个笔记中代码的不同，对比一下就明白了。</p><p>urlopen()返回的对象，可以使用read()进行读取，同样也可以使用geturl()方法、info()方法、getcode()方法。</p><ul><li><p>geturl()返回的是一个url的字符串；</p></li><li><p>info()返回的是一些meta标记的元信息，包括一些服务器的信息；</p></li><li><p>getcode()返回的是HTTP的状态码，如果返回200表示请求成功。</p></li></ul><p>关于META标签和HTTP状态码的内容可以自行百度百科，里面有很详细的介绍。</p><p>了解到这些，我们就可以进行新一轮的测试，新建文件名urllib_test04.py，编写如下代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    req = request.Request(<span class="string">"http://fanyi.baidu.com/"</span>)</span><br><span class="line">    response = request.urlopen(req)</span><br><span class="line">    print(<span class="string">"geturl打印信息：%s"</span>%(response.geturl()))</span><br><span class="line">    print(<span class="string">'**********************************************'</span>)</span><br><span class="line">    print(<span class="string">"info打印信息：%s"</span>%(response.info()))</span><br><span class="line">    print(<span class="string">'**********************************************'</span>)</span><br><span class="line">    print(<span class="string">"getcode打印信息：%s"</span>%(response.getcode()))</span><br></pre></td></tr></table></figure></p><h3 id="urlopen的data参数"><a href="#urlopen的data参数" class="headerlink" title="urlopen的data参数"></a>urlopen的data参数</h3><p>我们可以使用data参数，向服务器发送数据。根据HTTP规范，GET用于信息获取，POST是向服务器提交数据的一种请求，再换句话说：</p><p>从客户端向服务器提交数据使用POST；</p><p>从服务器获得数据到客户端使用GET(GET也可以提交，暂不考虑)。</p><p>如果没有设置urlopen()函数的data参数，HTTP请求采用GET方式，也就是我们从服务器获取信息，如果我们设置data参数，HTTP请求采用POST方式，也就是我们向服务器传递数据。</p><p>data参数有自己的格式，它是一个基于application/x-<a href="http://www.form-urlencoded的格式，具体格式我们不用了解，" target="_blank" rel="noopener">www.form-urlencoded的格式，具体格式我们不用了解，</a> 因为我们可以使用urllib.parse.urlencode()函数将字符串自动转换成上面所说的格式。</p><h3 id="发送data实例"><a href="#发送data实例" class="headerlink" title="发送data实例"></a>发送data实例</h3><p>向有道翻译发送data，得到翻译结果。</p><h4 id="1-打开有道翻译界面，如下图所示："><a href="#1-打开有道翻译界面，如下图所示：" class="headerlink" title="(1).打开有道翻译界面，如下图所示："></a>(1).打开有道翻译界面，如下图所示：</h4><p> <img src="http://upload-images.jianshu.io/upload_images/2952111-31f629ec53534a43?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h4 id="2-鼠标右键检查，也就是审查元素，如下图所示："><a href="#2-鼠标右键检查，也就是审查元素，如下图所示：" class="headerlink" title="(2).鼠标右键检查，也就是审查元素，如下图所示："></a>(2).鼠标右键检查，也就是审查元素，如下图所示：</h4><p><img src="http://upload-images.jianshu.io/upload_images/2952111-f199c9cbcd80b40f?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h4 id="3-选择右侧出现的Network，如下图所示："><a href="#3-选择右侧出现的Network，如下图所示：" class="headerlink" title="(3).选择右侧出现的Network，如下图所示："></a>(3).选择右侧出现的Network，如下图所示：</h4><p> <img src="http://upload-images.jianshu.io/upload_images/2952111-4354c17b0169d4b9?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h4 id="4-在左侧输入翻译内容，输入Jack，如下图所示："><a href="#4-在左侧输入翻译内容，输入Jack，如下图所示：" class="headerlink" title="(4).在左侧输入翻译内容，输入Jack，如下图所示："></a>(4).在左侧输入翻译内容，输入Jack，如下图所示：</h4><p> <img src="http://upload-images.jianshu.io/upload_images/2952111-1a5e4f785e7bbccf?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h4 id="5-点击自动翻译按钮，我们就可以看到右侧出现的内容，如下图所示："><a href="#5-点击自动翻译按钮，我们就可以看到右侧出现的内容，如下图所示：" class="headerlink" title="(5).点击自动翻译按钮，我们就可以看到右侧出现的内容，如下图所示："></a>(5).点击自动翻译按钮，我们就可以看到右侧出现的内容，如下图所示：</h4><p><img src="http://upload-images.jianshu.io/upload_images/2952111-37b47520ec88de2e?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h4 id="6-点击上图红框中的内容，查看它的信息，如下图所示："><a href="#6-点击上图红框中的内容，查看它的信息，如下图所示：" class="headerlink" title="(6).点击上图红框中的内容，查看它的信息，如下图所示："></a>(6).点击上图红框中的内容，查看它的信息，如下图所示：</h4><p> <img src="http://upload-images.jianshu.io/upload_images/2952111-9b692ceecf538fd5?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><p><img src="http://upload-images.jianshu.io/upload_images/2952111-1dc2354ec47dc6ff?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h4 id="7-记住这些信息，这是我们一会儿写程序需要用到的。"><a href="#7-记住这些信息，这是我们一会儿写程序需要用到的。" class="headerlink" title="(7).记住这些信息，这是我们一会儿写程序需要用到的。"></a>(7).记住这些信息，这是我们一会儿写程序需要用到的。</h4><p>  新建文件translate_test.py，编写如下代码：<br> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment">#对应上图的Request URL</span></span><br><span class="line">    Request_URL = <span class="string">'http://fanyi.youdao.com/translate?smartresult=dict&amp;smartresult=rule&amp;sessionFrom=null'</span></span><br><span class="line">    <span class="comment">#创建Form_Data字典，存储上图的Form Data</span></span><br><span class="line">    Form_Data = &#123;&#125;</span><br><span class="line">    Form_Data[<span class="string">'type'</span>] = <span class="string">'AUTO'</span></span><br><span class="line">    Form_Data[<span class="string">'i'</span>] = <span class="string">'Jack'</span></span><br><span class="line">    Form_Data[<span class="string">'doctype'</span>] = <span class="string">'json'</span></span><br><span class="line">    Form_Data[<span class="string">'xmlVersion'</span>] = <span class="string">'1.8'</span></span><br><span class="line">    Form_Data[<span class="string">'keyfrom'</span>] = <span class="string">'fanyi.web'</span></span><br><span class="line">    Form_Data[<span class="string">'ue'</span>] = <span class="string">'ue:UTF-8'</span></span><br><span class="line">    Form_Data[<span class="string">'action'</span>] = <span class="string">'FY_BY_CLICKBUTTON'</span></span><br><span class="line">    <span class="comment">#使用urlencode方法转换标准格式</span></span><br><span class="line">    data = parse.urlencode(Form_Data).encode(<span class="string">'utf-8'</span>)</span><br><span class="line">    <span class="comment">#传递Request对象和转换完格式的数据</span></span><br><span class="line">    response = request.urlopen(Request_URL,data)</span><br><span class="line">    <span class="comment">#读取信息并解码</span></span><br><span class="line">    html = response.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    <span class="comment">#使用JSON</span></span><br><span class="line">    translate_results = json.loads(html)</span><br><span class="line">    <span class="comment">#找到翻译结果</span></span><br><span class="line">    translate_results = translate_results[<span class="string">'translateResult'</span>][<span class="number">0</span>][<span class="number">0</span>][<span class="string">'tgt'</span>]</span><br><span class="line">    <span class="comment">#打印翻译信息</span></span><br><span class="line">    print(<span class="string">"翻译的结果是：%s"</span> % translate_results)</span><br></pre></td></tr></table></figure></p><p>运行查看翻译结果</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;一个疑问尚未解决疑问，小弟在此跪求大牛解答一下&quot;&gt;&lt;a href=&quot;#一个疑问尚未解决疑问，小弟在此跪求大牛解答一下&quot; class=&quot;headerlink&quot; title=&quot;一个疑问尚未解决疑问，小弟在此跪求大牛解答一下&quot;&gt;&lt;/a&gt;一个疑问尚未解决疑问，小弟在此跪求大牛解答一下&lt;/h3&gt;&lt;p&gt;为什么把url里的 “_o” 删掉后就可以正常爬取呢？&lt;br&gt;
    
    </summary>
    
      <category term="Spider" scheme="http://yoursite.com/categories/Spider/"/>
    
      <category term="Urllib" scheme="http://yoursite.com/categories/Spider/Urllib/"/>
    
    
      <category term="Python3" scheme="http://yoursite.com/tags/Python3/"/>
    
      <category term="Spider" scheme="http://yoursite.com/tags/Spider/"/>
    
      <category term="Urllib" scheme="http://yoursite.com/tags/Urllib/"/>
    
  </entry>
  
  <entry>
    <title>从零开始学爬虫-01</title>
    <link href="http://yoursite.com/2018/02/28/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E7%88%AC%E8%99%AB-01/"/>
    <id>http://yoursite.com/2018/02/28/从零开始学爬虫-01/</id>
    <published>2018-02-27T16:37:37.000Z</published>
    <updated>2018-02-27T17:04:40.535Z</updated>
    
    <content type="html"><![CDATA[<h3 id="本节关键字"><a href="#本节关键字" class="headerlink" title="本节关键字"></a>本节关键字</h3><p><em>urllib | chardet</em></p><h3 id="urllib-简介"><a href="#urllib-简介" class="headerlink" title="urllib 简介"></a>urllib 简介</h3><p>在Python3.x中，我们可以使用urlib这个组件抓取网页，urllib是一个URL处理包，这个包中集合了一些处理URL的模块，如下：<br><a id="more"></a></p><blockquote><p>1.urllib.request模块是用来打开和读取URLs的；</p></blockquote><blockquote><p>2.urllib.error模块包含一些有urllib.request产生的错误，可以使用try进行捕捉处理；</p></blockquote><blockquote><p>3.urllib.parse模块包含了一些解析URLs的方法；</p></blockquote><blockquote><p>4.urllib.robotparser模块用来解析robots.txt文本文件.它提供了一个单独的RobotFileParser类，通过该类提供的can_fetch()方法测试爬虫是否可以下载一个页面。</p></blockquote><p>使用urllib.request.urlopen()这个接口函数就可以访问一个网站，读取并打印信息。<br>urlopen有一些可选参数，具体信息可以查阅Python自带的documentation。</p><h3 id="urllib-测试"><a href="#urllib-测试" class="headerlink" title="urllib 测试"></a>urllib 测试</h3><p>了解到这些，我们就可以写一个最简单的程序，文件名为urllib_test01.py，感受一个urllib库的魅力：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">"http://fanyi.baidu.com"</span>)</span><br><span class="line">    html = response.read()</span><br><span class="line">    print(html)</span><br></pre></td></tr></table></figure></p><p>urllib使用使用request.urlopen()访问和读取URLs信息，返回的对象response如同一个文本对象，我们可以调用read()，进行读取。再通过print()屏幕打印。</p><p>浏览器就是作为客户端从服务器端获取信息，然后将信息解析，再展示给我们的。但是显然他们都是二进制的乱码。</p><p>我们可以通过简单的decode()命令将网页的信息进行解码，并显示出来，我们新创建一个文件，命名为urllib_test02.py，编写如下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">"http://fanyi.baidu.com/"</span>)</span><br><span class="line">    html = response.read()</span><br><span class="line">    html = html.decode(<span class="string">"utf-8"</span>)</span><br><span class="line">    print(html)</span><br></pre></td></tr></table></figure><p>这样我们就可以得到这样的结果，显然解码后的信息看起来工整和舒服多了：</p><p>当然这个前提是我们已经知道了这个网页是使用utf-8编码的，怎么查看网页的编码方式呢？非常简单的方法是使用使用浏览器审查元素，只需要找到head标签开始位置的chareset，就知道网页是采用何种编码。</p><p>这样我们就知道了这个网站的编码方式，但是这需要我们每次都打开浏览器，并找下编码方式，显然有些费事，使用几行代码解决更加省事并且显得酷一些。</p><p>我们需要安装第三方库chardet，它是用来判断编码的模块。安装好后，我们就可以使用chardet.detect()方法，判断网页的编码方式了。至此，我们就可以编写一个小程序判断网页的编码方式了，新建文件名为chardet_test01.py：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">import</span> chardet</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">"http://fanyi.baidu.com"</span>)</span><br><span class="line">    html = response.read()</span><br><span class="line">    charset = chardet.detect(html)</span><br><span class="line">    print(charset)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;本节关键字&quot;&gt;&lt;a href=&quot;#本节关键字&quot; class=&quot;headerlink&quot; title=&quot;本节关键字&quot;&gt;&lt;/a&gt;本节关键字&lt;/h3&gt;&lt;p&gt;&lt;em&gt;urllib | chardet&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&quot;urllib-简介&quot;&gt;&lt;a href=&quot;#urllib-简介&quot; class=&quot;headerlink&quot; title=&quot;urllib 简介&quot;&gt;&lt;/a&gt;urllib 简介&lt;/h3&gt;&lt;p&gt;在Python3.x中，我们可以使用urlib这个组件抓取网页，urllib是一个URL处理包，这个包中集合了一些处理URL的模块，如下：&lt;br&gt;
    
    </summary>
    
      <category term="Spider" scheme="http://yoursite.com/categories/Spider/"/>
    
      <category term="Urllib" scheme="http://yoursite.com/categories/Spider/Urllib/"/>
    
    
      <category term="Python3" scheme="http://yoursite.com/tags/Python3/"/>
    
      <category term="Spider" scheme="http://yoursite.com/tags/Spider/"/>
    
      <category term="Urllib" scheme="http://yoursite.com/tags/Urllib/"/>
    
  </entry>
  
  <entry>
    <title>一种Git保留两个repo的commit信息进行合并的方法</title>
    <link href="http://yoursite.com/2018/02/27/%E4%B8%80%E7%A7%8DGit%E4%BF%9D%E7%95%99%E4%B8%A4%E4%B8%AArepo%E7%9A%84commit%E4%BF%A1%E6%81%AF%E8%BF%9B%E8%A1%8C%E5%90%88%E5%B9%B6%E7%9A%84%E6%96%B9%E6%B3%95/"/>
    <id>http://yoursite.com/2018/02/27/一种Git保留两个repo的commit信息进行合并的方法/</id>
    <published>2018-02-27T06:46:37.000Z</published>
    <updated>2018-02-27T07:19:42.036Z</updated>
    
    <content type="html"><![CDATA[<p>以往的合并时首先要删除repo的.git文件夹，然后重新add-commit-push。带来的问题是会丢失某一个仓库的提交信息，不利于时光倒退。经过摸索终于实现了保留两个仓库提交信息的合并方法。介绍如下：</p><a id="more"></a><p>比如要将DownloadPicsBySeleniumAndPhantomJS这个项目合并到Web-Spider中，终端中执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ git remote add other git@github.com:ByiProX/****.git</span><br><span class="line">$ git fetch other</span><br><span class="line">$ git checkout -b repo1 other/mster</span><br><span class="line">$ git checkout master</span><br><span class="line">$ git merge repo1 --allow-unrelated-histories</span><br></pre></td></tr></table></figure></p><p><img src="http://img.blog.csdn.net/20180213030000117" alt=""></p><p>接下来解决merge冲突即可（可以尝试使用mergetool），如有需要可以删除多余分支和远程连接</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git mergetool <span class="comment"># 解决merge冲突</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ git remote rm other <span class="comment"># 删除远程连接  </span></span><br><span class="line">$ git branch -d repo1 <span class="comment"># 删除分支操作</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;以往的合并时首先要删除repo的.git文件夹，然后重新add-commit-push。带来的问题是会丢失某一个仓库的提交信息，不利于时光倒退。经过摸索终于实现了保留两个仓库提交信息的合并方法。介绍如下：&lt;/p&gt;
    
    </summary>
    
      <category term="Git" scheme="http://yoursite.com/categories/Git/"/>
    
    
      <category term="Git" scheme="http://yoursite.com/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>Python3下使用Selenium&amp;PhantomJS爬火影忍者漫画</title>
    <link href="http://yoursite.com/2018/02/27/Python3%E4%B8%8B%E4%BD%BF%E7%94%A8Selenium-PhantomJS%E7%88%AC%E7%81%AB%E5%BD%B1%E5%BF%8D%E8%80%85%E6%BC%AB%E7%94%BB/"/>
    <id>http://yoursite.com/2018/02/27/Python3下使用Selenium-PhantomJS爬火影忍者漫画/</id>
    <published>2018-02-27T06:16:38.000Z</published>
    <updated>2018-02-27T17:03:57.896Z</updated>
    
    <content type="html"><![CDATA[<p>近期学习爬虫，发现懂的越多，不懂的知识点越多（所以当个傻子还是很幸福的）。好记性不如烂笔头，之前都是把看到的资料链接直接挂到一些平台，比如知乎、简书、Github等。今天有点时间，就好好码一下字，排排版，方便以后查阅。</p><p>Selenium用来模拟浏览器的行为，比如点击、最大化、滚动窗口等；PhantomJS是一种浏览器，不过这种浏览器没有UI界面，感觉就像是专门为爬虫设计，优点很明显，可以有效减小内存的使用。</p><a id="more"></a><h2 id="爬虫使用到的模块"><a href="#爬虫使用到的模块" class="headerlink" title="爬虫使用到的模块"></a>爬虫使用到的模块</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> myLogging <span class="keyword">import</span> MyLogging</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> re</span><br></pre></td></tr></table></figure><p>myLogging模块是自己配置的日志包，想要的可以点击<strong><em><a href="http://link.zhihu.com/?target=https%3A//github.com/ByiProX/DownloadPicsBySeleniumAndPhantomJS" target="_blank" rel="noopener">这里</a></em></strong>自己看</p><p>爬虫很关键的一点就是能够看懂网页的源代码，记得当初刚刚真正开始接触编程的时候，有很长的一段时间在看HTML、CSS、JS的一些知识，虽然忘得很多，但是印象还是有的，对于后面看网页源代码很有帮助。学习爬虫，除了会基本的python知识以外，还要会网页的一些知识。</p><h2 id="爬取图片思路："><a href="#爬取图片思路：" class="headerlink" title="爬取图片思路："></a>爬取图片思路：</h2><ul><li>已知连接，分析网页的代码结构，看所需的数据是否需要切换frame，并定位所需数据的位于哪个标签之下</li><li>采用不同的模块有不同的保存图片方式，如果采用request模块，保存图片方式是可以采用<strong>requests.get(comicUrl).content</strong>方法，使用该方法需要确定网页的地址。该项目中没有涉及request的使用，所以此后不再表述。对于selenium可以使用 <strong>get_screenshot_as_file()</strong> 方法，使用该方法强烈建议使用phantomjs，如果使用chrome浏览器，图片尺寸太大的话，会出现截图不完整，对比如下：<br><img src="http://upload-images.jianshu.io/upload_images/2952111-2323c462a546dcc3.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="对比"></li><li>找到下一张图片的连接位置并点击更新网页，一般来讲新网页与之前网页结构相同。</li><li>在新网页的基础上保存图片，设置循环如此反复。</li></ul><p>爬取网页的URL为：<a href="http://link.zhihu.com/?target=http%3A//comic.kukudm.com/comiclist/3/3/1.htm" target="_blank" rel="noopener">爬取火影漫画第一话</a></p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DownloadPics</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, url)</span>:</span></span><br><span class="line">        self.url = url</span><br><span class="line">        self.log = MyLogging()</span><br><span class="line">        self.browser = self.get_browser()</span><br><span class="line">        self.save_pics(self.browser)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_browser</span><span class="params">(self)</span>:</span></span><br><span class="line">        browser = webdriver.PhantomJS()</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            browser.get(self.url)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            MyLogging.error(<span class="string">'open the url %s failed'</span> % self.url)</span><br><span class="line">        browser.implicitly_wait(<span class="number">20</span>)</span><br><span class="line">        <span class="keyword">return</span> browser</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_pics</span><span class="params">(self, browser)</span>:</span></span><br><span class="line">        pics_title = browser.title.split(<span class="string">'_'</span>)[<span class="number">0</span>]</span><br><span class="line">        self.create_dir(pics_title)</span><br><span class="line">        os.chdir(pics_title)</span><br><span class="line">        sum_page = self.find_total_page_num(browser)</span><br><span class="line">        i = <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; sum_page:</span><br><span class="line">            image_name = str(i) + <span class="string">'.png'</span></span><br><span class="line">            browser.get_screenshot_as_file(image_name)  </span><br><span class="line">            <span class="comment"># 使用PhantomJS避免了截图的不完整，可以与Chrome比较</span></span><br><span class="line">            self.log.info(<span class="string">'saving image %s'</span> % image_name)</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">            css_selector = <span class="string">"a[href='/comiclist/3/3/%s.htm']"</span> % i  </span><br><span class="line">            <span class="comment"># 该方法感觉还不错呢，不过这个网站确实挺差劲的</span></span><br><span class="line">            next_page = browser.find_element_by_css_selector(css_selector)</span><br><span class="line">            next_page.click()</span><br><span class="line">            time.sleep(<span class="number">2</span>)</span><br><span class="line">            <span class="comment"># browser.implicitly_wait(20)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">find_total_page_num</span><span class="params">(self, browser)</span>:</span></span><br><span class="line">        page_element = browser.find_element_by_css_selector(<span class="string">"table[cellspacing='1']"</span>)</span><br><span class="line">        num = re.search(<span class="string">r'共\d+页'</span>, page_element.text).group()[<span class="number">1</span>:<span class="number">-1</span>]  </span><br><span class="line">        <span class="keyword">return</span> int(num)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_dir</span><span class="params">(self, dir_name)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> os.path.exists(dir_name):</span><br><span class="line">            self.log.error(<span class="string">'create directory %s failed cause a same directory exists'</span> % dir_name)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                os.makedirs(dir_name)</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                self.log.error(<span class="string">'create directory %s failed'</span> % dir_name)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.log.info(<span class="string">'create directory %s success'</span> % dir_name)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    start_url = <span class="string">'http://comic.kukudm.com/comiclist/3/3/1.htm'</span></span><br><span class="line">    DL = DownloadPics(start_url)</span><br></pre></td></tr></table></figure><h2 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h2><p><img src="http://upload-images.jianshu.io/upload_images/2952111-e7e2cf39116b5fea.gif?imageMogr2/auto-orient/strip" alt="gif"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;近期学习爬虫，发现懂的越多，不懂的知识点越多（所以当个傻子还是很幸福的）。好记性不如烂笔头，之前都是把看到的资料链接直接挂到一些平台，比如知乎、简书、Github等。今天有点时间，就好好码一下字，排排版，方便以后查阅。&lt;/p&gt;
&lt;p&gt;Selenium用来模拟浏览器的行为，比如点击、最大化、滚动窗口等；PhantomJS是一种浏览器，不过这种浏览器没有UI界面，感觉就像是专门为爬虫设计，优点很明显，可以有效减小内存的使用。&lt;/p&gt;
    
    </summary>
    
      <category term="Spider" scheme="http://yoursite.com/categories/Spider/"/>
    
      <category term="Selenium" scheme="http://yoursite.com/categories/Spider/Selenium/"/>
    
    
      <category term="Python3" scheme="http://yoursite.com/tags/Python3/"/>
    
      <category term="Spider" scheme="http://yoursite.com/tags/Spider/"/>
    
      <category term="Selenium" scheme="http://yoursite.com/tags/Selenium/"/>
    
      <category term="PhantomJS" scheme="http://yoursite.com/tags/PhantomJS/"/>
    
  </entry>
  
  <entry>
    <title>Django2.0.1搭建电影网站</title>
    <link href="http://yoursite.com/2018/02/27/Django2-0-1%E6%90%AD%E5%BB%BA%E7%94%B5%E5%BD%B1%E7%BD%91%E7%AB%99/"/>
    <id>http://yoursite.com/2018/02/27/Django2-0-1搭建电影网站/</id>
    <published>2018-02-27T05:48:44.000Z</published>
    <updated>2018-02-27T06:41:59.694Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://school.thoughtworks.cn/bbs/assets/uploads/files/1517985555955-fireshot-capture-8-cinema-http___127.0.0.1_8000_.png" alt="首页"></p><p>本项目已经部署到服务器，可以通过该IP查看<br><a href="http://59.110.221.56/" target="_blank" rel="noopener">http://59.110.221.56/</a><br><a href="https://github.com/ByiProX/ThoughtWorks-Cinema" target="_blank" rel="noopener">GitHub源代码</a><br><a id="more"></a></p><h2 id="技术栈"><a href="#技术栈" class="headerlink" title="技术栈"></a>技术栈</h2><blockquote><p>Bootstrap 3 + Django 2.0.1 + MySQL 5.7.17 + Nginx + locust</p></blockquote><h2 id="本地服务运行方法"><a href="#本地服务运行方法" class="headerlink" title="本地服务运行方法"></a>本地服务运行方法</h2><p>终端在venv文件夹路径下开启虚拟环境(根据自己的shell进行选择)：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">source ../venv/bin/activate.fish</span><br><span class="line">source ../venv/bin/activate</span><br><span class="line">source ../venv/bin/activate.csh</span><br></pre></td></tr></table></figure><p>然后执行：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 TWS_Cinema/manage.py runserver</span><br></pre></td></tr></table></figure></p><p><strong>如果报错</strong>，终端进入requirements.txt所在目录，运行命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install -r requirements.txt</span><br></pre></td></tr></table></figure></p><p>然后执行：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 TWS_Cinema/manage.py runserver</span><br></pre></td></tr></table></figure></p><h2 id="单元测试运行方法"><a href="#单元测试运行方法" class="headerlink" title="单元测试运行方法"></a>单元测试运行方法</h2><p>在manage.py路径下终端运行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 manage.py test</span><br></pre></td></tr></table></figure><h2 id="网站功能描述"><a href="#网站功能描述" class="headerlink" title="网站功能描述"></a>网站功能描述</h2><ul><li><p>实现导航栏搜索电影，支持按年份搜索和类型搜索<br>  – 显示分类列表<br>  – 点击分类显示符合分类要求的电影</p></li><li><p>实现搜索功能，支持按电影名称模糊搜索</p></li><li><p>实现电影详细信息查看功能<br>  – 显示电影详细信息<br>  – 显示豆瓣 Top 5 影评<br>  – 在电影详细页面显示相似电影推荐<br>  – 增加电影观看链接</p></li></ul><h2 id="API"><a href="#API" class="headerlink" title="API"></a>API</h2><ul><li><p>按电影id搜索 —— api/movie/id/        # 例如：api/movie/id/1291545</p></li><li><p>按电影名搜索 —— api/movie/title/     # 例如：api/movie/title/大鱼</p></li><li><p>按电影原始名搜索 —— api/movie/original_title/     # 例如：api/movie/original_title/Big Fish</p></li><li><p>按电影类型搜索 —— api/movie/genre/   # 例如：api/movie/genre/剧情</p></li><li><p>按电影年份搜索 —— api/movie/year/    # 例如：api/movie/year/2003</p></li></ul><h2 id="网站性能测试结果"><a href="#网站性能测试结果" class="headerlink" title="网站性能测试结果"></a>网站性能测试结果</h2><p>在文件locustfile.py路径下运行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">locust --host=http://59.110.221.56</span><br></pre></td></tr></table></figure></p><h3 id="压力测试"><a href="#压力测试" class="headerlink" title="压力测试"></a>压力测试</h3><ul><li>采取的框架：<strong>locust</strong></li><li>服务器性能：<ul><li>CPU：1核</li><li>内存：2 GB (I/O优化)</li><li>带宽：1Mbps</li></ul></li><li>测试结果：<ul><li>500人：100%正确</li><li>1000人：40%出错率</li></ul></li><li>测试截图</li></ul><p><img src="http://upload-images.jianshu.io/upload_images/2952111-4c41c64c40130ebe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Screen Shot 2018-02-08 at 16.15.49.png"></p><p><img src="http://upload-images.jianshu.io/upload_images/2952111-c2c542dbf0ce9e58.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Screen Shot 2018-02-08 at 16.14.10.png"></p><p><img src="http://upload-images.jianshu.io/upload_images/2952111-f5e4ace67f22ddac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Screen Shot 2018-02-08 at 16.13.49.png"></p><h2 id="电影网站的其他截图"><a href="#电影网站的其他截图" class="headerlink" title="电影网站的其他截图"></a>电影网站的其他截图</h2><p><img src="https://school.thoughtworks.cn/bbs/assets/uploads/files/1517985569003-fireshot-capture-9-%E9%A6%96%E9%A1%B5-http___127.0.0.1_8000_movie_display_.png" alt="list"><br><img src="https://school.thoughtworks.cn/bbs/assets/uploads/files/1517985586166-fireshot-capture-10-%E7%94%B5%E5%BD%B1%E8%AF%A6%E6%83%85-http___127.0.0.1_8000_movie_id_1291545_.png" alt="detail"></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://my.oschina.net/u/1447352/blog/1499428/" target="_blank" rel="noopener">Locust 简介以及使用</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://school.thoughtworks.cn/bbs/assets/uploads/files/1517985555955-fireshot-capture-8-cinema-http___127.0.0.1_8000_.png&quot; alt=&quot;首页&quot;&gt;&lt;/p&gt;
&lt;p&gt;本项目已经部署到服务器，可以通过该IP查看&lt;br&gt;&lt;a href=&quot;http://59.110.221.56/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://59.110.221.56/&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://github.com/ByiProX/ThoughtWorks-Cinema&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub源代码&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Django" scheme="http://yoursite.com/categories/Django/"/>
    
    
      <category term="Django" scheme="http://yoursite.com/tags/Django/"/>
    
      <category term="Python3" scheme="http://yoursite.com/tags/Python3/"/>
    
  </entry>
  
  <entry>
    <title>简谈爬虫攻与防</title>
    <link href="http://yoursite.com/2018/02/27/%E7%AE%80%E8%B0%88%E7%88%AC%E8%99%AB%E6%94%BB%E4%B8%8E%E9%98%B2/"/>
    <id>http://yoursite.com/2018/02/27/简谈爬虫攻与防/</id>
    <published>2018-02-27T04:44:38.000Z</published>
    <updated>2018-02-27T17:04:37.804Z</updated>
    
    <content type="html"><![CDATA[<h2 id="封锁间隔时间破解"><a href="#封锁间隔时间破解" class="headerlink" title="封锁间隔时间破解"></a>封锁间隔时间破解</h2><p>Scrapy在两次请求之间的时间设置是DOWNLOAD_DELAY。如果不考虑反爬虫的因素，该值当然越小越好。如果DOWNLOAD_DELAY设为0.001，也就是每1毫秒请求一次网页，这简直非人类干的事情。有些网站会检测一个ip的访问时间，异常情况下会封锁该ip。</p><a id="more"></a><h2 id="封锁Cookies"><a href="#封锁Cookies" class="headerlink" title="封锁Cookies"></a>封锁Cookies</h2><p>众所周知，网站是通过Cookie来确定用户身份的，Scrapy在爬取数据时使用同一个Cookies发起请求。该做法和把DOWNLOAD_DELAY设为0.001没有本质区别。在scrapy中，直接社禁用Cookies就可以了。在settings.py中设置<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Disable cookies (enabled by default)</span></span><br><span class="line">COOKIES_ENABLED = <span class="keyword">False</span></span><br></pre></td></tr></table></figure></p><h2 id="封锁user-agent和proxy破解"><a href="#封锁user-agent和proxy破解" class="headerlink" title="封锁user-agent和proxy破解"></a>封锁user-agent和proxy破解</h2><p>user-agent是浏览器的身份标识。网站通过UA来确定浏览器类型。很多浏览器拒绝不符合一定标准的UA请求网页。同一个UA高频率的访问网站会有被网站列入黑名单的危险。破解的方法很简单，可以准备一个<strong>UA池</strong>，每次请求时随机挑选一个进行请求。</p><p>在middlewares.py同级目录下创建UAResource.py,文件内容如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line">UserAgents = [</span><br><span class="line">  <span class="string">"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)"</span>,</span><br><span class="line">  <span class="string">"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)"</span>,</span><br><span class="line">  <span class="string">"Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)"</span>,</span><br><span class="line">  <span class="string">"Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)"</span>,</span><br><span class="line">  <span class="string">"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)"</span>,</span><br><span class="line">  <span class="string">"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)"</span>,</span><br><span class="line">  <span class="string">"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)"</span>,</span><br><span class="line">  <span class="string">"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)"</span>,</span><br><span class="line">  <span class="string">"Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6"</span>,</span><br><span class="line">  <span class="string">"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1"</span>,</span><br><span class="line">  <span class="string">"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0"</span>,</span><br><span class="line">  <span class="string">"Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5"</span>,</span><br><span class="line">  <span class="string">"Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6"</span>,</span><br><span class="line">  <span class="string">"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11"</span>,</span><br><span class="line">  <span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20"</span>,</span><br><span class="line">  <span class="string">"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52"</span>,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">Proxies = [</span><br><span class="line"><span class="string">'http://122.114.31.177:808'</span>,</span><br><span class="line"><span class="string">'http://1.2.3.4:80'</span>,</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>修改middlewares.py，添加内容为<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> .UAResource <span class="keyword">import</span> UserAgents</span><br><span class="line"><span class="keyword">from</span> .UAResource <span class="keyword">import</span> Proxies</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomProxy</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        proxy = random.choice(Proxies)</span><br><span class="line">        request.meta[<span class="string">'proxy'</span>] = proxy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomUserAgent</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""docstring for RandomUerAgent."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        ua = random.choice(UserAgents)</span><br><span class="line">        request.headers.setdefault(<span class="string">'User-Agent'</span>, ua)</span><br></pre></td></tr></table></figure></p><p>最后修改setting.py,将RandomUserAgent和RandomProxy添加到DOWNLOADER_MIDDLEWARES中，注意RandomProxy要放到RandomUserAgent之前，即将RandomProxy的值比RandomUserAgent后的值小<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">   <span class="comment"># 'meijutt.middlewares.MeijuttDownloaderMiddleware': 543,</span></span><br><span class="line">   <span class="string">'meijutt.middlewares.RandomProxy'</span>: <span class="number">10</span>,</span><br><span class="line">   <span class="string">'meijutt.middlewares.RandomUserAgent'</span>: <span class="number">30</span>,</span><br><span class="line"></span><br><span class="line">   <span class="comment"># 禁止内置的(在 DOWNLOADER_MIDDLEWARES_BASE 中设置并默认启用的)中间件</span></span><br><span class="line">   <span class="string">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span>: <span class="keyword">None</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>免费代理不够稳定，如果不想用proxy，设置RandomProxy为None,即禁止使用<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'meijutt.middlewares.RandomProxy'</span>: <span class="keyword">None</span>,</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;封锁间隔时间破解&quot;&gt;&lt;a href=&quot;#封锁间隔时间破解&quot; class=&quot;headerlink&quot; title=&quot;封锁间隔时间破解&quot;&gt;&lt;/a&gt;封锁间隔时间破解&lt;/h2&gt;&lt;p&gt;Scrapy在两次请求之间的时间设置是DOWNLOAD_DELAY。如果不考虑反爬虫的因素，该值当然越小越好。如果DOWNLOAD_DELAY设为0.001，也就是每1毫秒请求一次网页，这简直非人类干的事情。有些网站会检测一个ip的访问时间，异常情况下会封锁该ip。&lt;/p&gt;
    
    </summary>
    
      <category term="Spider" scheme="http://yoursite.com/categories/Spider/"/>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Spider/Scrapy/"/>
    
    
      <category term="Spider" scheme="http://yoursite.com/tags/Spider/"/>
    
      <category term="Scrapy" scheme="http://yoursite.com/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Github多分支管理Hexo-Blog项目</title>
    <link href="http://yoursite.com/2018/02/27/Github%E5%A4%9A%E5%88%86%E6%94%AF%E7%AE%A1%E7%90%86Hexo-Blog%E9%A1%B9%E7%9B%AE/"/>
    <id>http://yoursite.com/2018/02/27/Github多分支管理Hexo-Blog项目/</id>
    <published>2018-02-26T16:23:41.000Z</published>
    <updated>2018-02-27T06:40:56.801Z</updated>
    
    <content type="html"><![CDATA[<p>Hexo在部署之后在github的仓库中我们只能找到生成的静态文件。然而博客的源文件：主题、文章、配置等文件都还在本地，并没有备份。对于多台终端设备的用户不够友好，而且存在一定的风险，万一那天电脑坏了或者是出现一些其他问题，就得从头再来。为了解决上述问题，我们可以利用github的分支思想来备份我们的源文件。</p><a id="more"></a><p>备份之前，需要了解博客根目录下面的文件以及文件夹作用：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">.deploy_git/        网站静态文件(git)</span><br><span class="line">node_modules/       插件</span><br><span class="line">public/             网站静态文件</span><br><span class="line">scaffolds/          文章模板</span><br><span class="line">source/             博文等</span><br><span class="line">themes/             主题</span><br><span class="line">_config.yml         网站配置文件</span><br><span class="line">package.json        Hexo信息</span><br><span class="line">db.json             数据文件</span><br></pre></td></tr></table></figure></p><h2 id="备份的思路"><a href="#备份的思路" class="headerlink" title="备份的思路"></a>备份的思路</h2><p><code>master分支存放部署生成的静态文件，Hexo-Bog分支存放我们要备份项目源文件。</code>实际备份中，.deploy_git、public文件夹和我们的master分支内容重复，所以略过。因此，我们在根目录下面建一个<strong>.gitignore</strong>文件来建立“黑名单”，禁止备份。</p><h2 id="编辑-gitignore过滤文件"><a href="#编辑-gitignore过滤文件" class="headerlink" title="编辑.gitignore过滤文件"></a>编辑<strong>.gitignore</strong>过滤文件</h2><p>文件内容如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">.DS_Store</span><br><span class="line">public/</span><br><span class="line">.deploy*/</span><br></pre></td></tr></table></figure></p><h2 id="关于备份"><a href="#关于备份" class="headerlink" title="关于备份"></a>关于备份</h2><p>终端中在项目的根目录下执行，对于作者自己的项目，命令执行的路径为ByiProX/下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ git init</span><br><span class="line">$ git remote add origin git@github.com:username/username.github.io.git</span><br><span class="line"><span class="comment"># username为博客项目的名称，也就是git的用户名</span></span><br><span class="line">$ git add .</span><br><span class="line">$ git commit -m <span class="string">"ready for backup of the project"</span></span><br><span class="line">$ git push origin master:Hexo-Blog</span><br></pre></td></tr></table></figure></p><p>执行完毕后会发现github博客仓库已经有了一个新分支Hexo-Blog，于是备份工作完成。<br>以后，开始写博文时，即终端运行<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new [layout] &lt;title&gt;</span><br></pre></td></tr></table></figure></p><p>完成文章后,对编辑后的文章进行备份保存，即终端运行,为下面的部署做准备<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git add .</span><br><span class="line">$ git commit -m <span class="string">"add one article"</span></span><br><span class="line">$ git push origin master:Hexo-Blog</span><br></pre></td></tr></table></figure></p><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><p>运行一下命令进行仓库master分支静态文件部署<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean</span><br><span class="line">$ hexo generate</span><br><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure></p><p>以上完成项目源文件以及静态文件的Git管理</p><h2 id="参考文献及进阶"><a href="#参考文献及进阶" class="headerlink" title="参考文献及进阶"></a>参考文献及进阶</h2><p><a href="https://mrlrf.github.io/2017/05/05/Hexo-github%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/" target="_blank" rel="noopener">Hexo+github搭建个人博客并实现多终端管理</a><br><a href="https://blog.zaihua.me/post/blog_github_backup.html" target="_blank" rel="noopener">如何在github上面备份Hexo</a><br><a href="https://formulahendry.github.io/2016/12/04/hexo-ci/" target="_blank" rel="noopener">Hexo的版本控制与持续集成</a><br><a href="https://www.zhihu.com/question/21193762" target="_blank" rel="noopener">使用hexo，如果换了电脑怎么更新博客</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hexo在部署之后在github的仓库中我们只能找到生成的静态文件。然而博客的源文件：主题、文章、配置等文件都还在本地，并没有备份。对于多台终端设备的用户不够友好，而且存在一定的风险，万一那天电脑坏了或者是出现一些其他问题，就得从头再来。为了解决上述问题，我们可以利用github的分支思想来备份我们的源文件。&lt;/p&gt;
    
    </summary>
    
      <category term="Hexo" scheme="http://yoursite.com/categories/Hexo/"/>
    
    
      <category term="Hexo" scheme="http://yoursite.com/tags/Hexo/"/>
    
      <category term="Git" scheme="http://yoursite.com/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2018/02/26/hello-world/"/>
    <id>http://yoursite.com/2018/02/26/hello-world/</id>
    <published>2018-02-25T18:47:57.250Z</published>
    <updated>2018-02-27T06:43:43.774Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><p>非常好的一篇markdown参考手册</p><a id="more"></a><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;非常好的一篇markdown参考手册&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
