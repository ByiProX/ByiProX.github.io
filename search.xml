<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[python3 产生随机字符串]]></title>
    <url>%2F2018%2F04%2F19%2Fpython3-%E4%BA%A7%E7%94%9F%E9%9A%8F%E6%9C%BA%E5%AD%97%E7%AC%A6%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[123456import randomimport string# 长度为16的随机字符串rand_str = ''.join(random.SystemRandom().choice(string.ascii_uppercase + string.ascii_lowercase + string.digits) for _ in range(16))]]></content>
      <categories>
        <category>Python3 进阶</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTML里引入CSS的四种方式]]></title>
    <url>%2F2018%2F04%2F09%2FHTML%E9%87%8C%E5%BC%95%E5%85%A5CSS%E7%9A%84%E5%9B%9B%E7%A7%8D%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[行内式：也称内联式，在标记的style属性中设定CSS样式。这种方式没有体现出CSS的优势； 嵌入式：将CSS样式集中写在网页的标签对的标签对中； 链接式：跟第4个的导入式都称外部式或者外联式，使用link引用外部CSS文件； 导入式：使用@import引用外部CSS文件；]]></content>
      <categories>
        <category>Frontend</category>
      </categories>
      <tags>
        <tag>Frontend</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 解包和压包]]></title>
    <url>%2F2018%2F04%2F06%2FPython3-%E8%A7%A3%E5%8C%85%E5%92%8C%E5%8E%8B%E5%8C%85%2F</url>
    <content type="text"><![CDATA[Python中的解包可以这样理解：一个可迭代对象是一个整体，想把可迭代对象中每个元素当成一个个个体剥离出来，这个过程就是解包。下面将举例说明。 可迭代对象每个元素赋值给一个变量12345678910111213141516171819202122232425262728293031323334353637383940# 列表&gt;&gt;&gt; name, age, date = ['Bob', 20, '2018-1-1']&gt;&gt;&gt; name'Bob'&gt;&gt;&gt; age20&gt;&gt;&gt; date'2018-1-1'&gt;&gt;&gt; a, b, c = enumerate(['a', 'b', 'c'])&gt;&gt;&gt; a(0, 'a')# 元组&gt;&gt;&gt; a, b, c = ('a', 'b', 'c')&gt;&gt;&gt; a'a'# 字典&gt;&gt;&gt; a, b, c = &#123;'a':1, 'b':2, 'c':3&#125;&gt;&gt;&gt; a'a'&gt;&gt;&gt; a, b, c = &#123;'a':1, 'b':2, 'c':3&#125;.items()&gt;&gt;&gt; a('a', 1)&gt;&gt;&gt; a, b, c = &#123;'a':1, 'b':2, 'c':3&#125;.values()&gt;&gt;&gt; a1# 字符串&gt;&gt;&gt; a, b, c = 'abc'&gt;&gt;&gt; a'a'# 生成器&gt;&gt;&gt; a, b, c = (x + 1 for x in range(3))&gt;&gt;&gt; b2 星号”*”的使用—解包比如我们要计算平均分，去除最高分和最低分，除了用切片，还可以用解包的方式获得中间的数值123&gt;&gt;&gt; small, *new, big = sorted([93,12,33,55,99])&gt;&gt;&gt; new[33, 55, 93] 压包与解包123456789a = ['a', 'b', 'c']b = [1, 2, 3]for i, j in zip(a, b): print(i+j)# 输出135 分析以上代码： zip函数将a, b压包程一个可迭代对象 对可迭代对象的每一个元素（(‘a’, 1)）进行解包（i, j = (‘a’, 1)） 接下来可以分别调用i, j变量进行计算 再举一个例子：1234567891011l = [ ('Bob', '1991', '60'), ('Bill', '1992', '65'), ('Mike', '1993', '70')]for name, *args in l: print(name, args)Bob ['1991', '60']Bill ['1992', '65']Mike ['1993', '70'] “_”的用法当一些变量不用时，用_表示是更好的写法，可以让读代码的人知道这个元素是不要的123456&gt;&gt;&gt; p = ('Bob', 20, 50, (11,20,2000))&gt;&gt;&gt; name, *_, (*_, year) = p&gt;&gt;&gt; name'Bob'&gt;&gt;&gt; year2000 多变量同时赋值1234&gt;&gt;&gt; a, b = 1, 2&gt;&gt;&gt; a = 1, 2&gt;&gt;&gt; a(1, 2) 下面用法都会报错12*a = 1, 3a,b,c = 1,2 但是这种写法是可以的123&gt;&gt;&gt; *a, = 1,2&gt;&gt;&gt; a[1,2] “*”之可变参数函数定义时，我们使用*的可变参数，其实也是压包解包过程12345def func(*args): print(args)&gt;&gt;&gt; func(1,2,3,4)(1, 2, 3, 4) 参数用num表示，num变量就可以当成元组调用了。其实这个过程相当于num, = 1,2,5,6 “*”之关键字参数12345def func(**kw): print(kw)&gt;&gt;&gt; func(name = 'Bob', age = 10, weight = 60)&#123;'name' = 'Bob', 'age' = 10, 'weight' = 60&#125; 键值对传入**kw，kw就可以表示相应字典。 **的用法只在函数定义中使用，不能这样使用1a, **b = &#123;'name' = 'Bob', 'age' = 10, 'weight' = 60&#125; 可变参数与关键字参数的细节问题函数传入实参时，可变参数*之前的参数不能指定参数名123456789101112def func(a, *b): print(a) print(b)&gt;&gt;&gt; func(a=1, 2,3,4)File "&lt;ipython-input-17-978eea76866e&gt;", line 1 func(a=1, 2,3,4) ^SyntaxError: positional argument follows keyword argumentfunc(1,2,3,4)1(2,3,4) 函数传入实参时，可变参数*之后的参数必须指定参数名，否则就会被归到可变参数之中1234567891011121314def func(a, *b, c = None): print(a) print(b) print(c)&gt;&gt;&gt; func(1,2,3,4)1(2,3,4)None&gt;&gt;&gt; func(1,2,3,c=4)1(2,3)4 如果一个函数想要使用时必须明确指定参数名，可以将所有参数都放在可变参数之后，而可变参数不用管它就可以，也不用命名，如下:1234567def func(*, a, b): print(a) print(b)&gt;&gt;&gt; func(a = 1, b = 2)12 可变参数的这两条特性，可以用于将 只需要按照位置赋值的参数 和 需要明确指定参数名的参数区分开来 关键字参数都只能作为最后一个参数，前面的参数按照位置赋值还是名称赋值都可以1234567891011def func(a, *b, c, **d): print(a) print(b) print(c) print(d)&gt;&gt;&gt; func(1,2,3,c=4,m=5,n=6)1(2,3)4&#123;'m': 5, 'n':6&#125; 可变参数与关键词参数共同使用以表示任意参数下面是这一点在装饰器当中的使用 123456789101112131415def mydecorator(func): def wrapper(*args, **kw): print('i am using a decorator') return func(*args, **kw) return wrapper@mydecoratordef myfun(a, b): print(a) print(b)&gt;&gt;&gt; myfun(1, b = 2)i am using a decorator12 wrapper函数使用*args, **kw作为参数，则被修饰的myfun函数需要的参数无论是什么样的，传入wrapper都不会报错，这保证了装饰器可以修饰各种各样函数的灵活性。毕竟我们一般在函数中传入参数时，要么所有参数名都写，要么前面几个不写，后面的会写，这样使用*args, **kw完全没有问题。 解包作为参数传入函数中首先定义一个函数12def myfun(a, b): print(a + b) 列表|元组的解包123&gt;&gt;&gt; n = [1,2]&gt;&gt;&gt; myfun(*n)3 字典的解包12345&gt;&gt;&gt; mydict = &#123;'a':1, 'b':2&#125;&gt;&gt;&gt; myfun(**mydict)3&gt;&gt;&gt; myfun(*mydict)'ab' 一个简单的应用123&gt;&gt;&gt; Bob = &#123;'name': 'Bob', 'age':20&#125;&gt;&gt;&gt; "&#123;name&#125;'s age is &#123;age&#125;".format(**Bob)"Bob's age is 20" 多返回值函数下面过程也涉及到了解包12345678def myfun(a, b): print a+1, b+1&gt;&gt;&gt; m,n = myfun(1,2)&gt;&gt;&gt; m2&gt;&gt;&gt; n3 其本身是一个元组123&gt;&gt;&gt; p = myfun(1, 2)&gt;&gt;&gt; p(2, 3)]]></content>
      <categories>
        <category>Python3 进阶</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 对象操作的时间复杂度总结]]></title>
    <url>%2F2018%2F03%2F29%2FPython3-%E5%AF%B9%E8%B1%A1%E6%93%8D%E4%BD%9C%E7%9A%84%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[列表 list列表是以数组（Array）实现的。最大的开销发生在超过当前分配大小的增长，这种情况下所有元素都需要移动；或者是在起始位置附近插入或者删除元素，这种情况下所有在该位置后面的元素都需要移动，这种情况可以考虑使用双向队列来解决。 Operation Average Case Amortized Worst Case Copy O(n) O(n) Append[1] O(1) O(1) Pop last O(1) O(1) Pop intermediate O(k) O(k) Insert O(n) O(n) Get Item O(1) O(1) Set Item O(1) O(1) Delete Item O(n) O(n) Iteration O(n) O(n) Get Slice O(k) O(k) Del Slice O(n) O(n) Set Slice O(k+n) O(k+n) Extend[1] O(k) O(k) Sort O(n log n) O(n log n) Multiply O(nk) O(nk) x in s O(n) min(s), max(s) O(n) Get Length O(1) O(1) 双向队列 collections.dequedeque是以双向链表的形式实现的。双向队列的两端都是可达的，但从查找队列中间的元素较为缓慢，增删元素就更慢了。 操作 平均情况 最坏情况 复制 O(n) O(n) append O(1) O(1) appendleft O(1) O(1) pop O(1) O(1) popleft O(1) O(1) extend O(k) O(k) extendleft O(k) O(k) rotate O(k) O(k) remove O(n) O(n) 集合 set Operation Average case Worst Case notes x in s O(1) O(n) Union s\ t O(len(s)+len(t)) Intersection s&amp;t O(min(len(s), len(t)) O(len(s) * len(t)) replace “min” with “max” if t is not a set Multiple intersection s1&amp;s2&amp;..&amp;sn (n-1)*O(l) where l is max(len(s1),..,len(sn)) Difference s-t O(len(s)) s.difference_update(t) O(len(t)) Symmetric Difference s^t O(len(s)) O(len(s) * len(t)) s.symmetric_difference_update(t) O(len(t)) O(len(t) * len(s)) 字典 dict字典的平均情况基于以下假设： 对象的散列函数足够撸棒（robust），不会发生冲突。 字典的键是从所有可能的键的集合中随机选择的。 Operation Average Case Amortized Worst Case Copy[2] O(n) O(n) Get Item O(1) O(n) Set Item[1] O(1) O(n) Delete Item O(1) O(n) Iteration[2] O(n) O(n) 参考 https://wiki.python.org/moin/TimeComplexity]]></content>
      <categories>
        <category>Python3 进阶</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 for ... else ...陷阱]]></title>
    <url>%2F2018%2F03%2F29%2FPython3-for-else-%E9%99%B7%E9%98%B1%2F</url>
    <content type="text"><![CDATA[假设有如下代码：12345for i in range(10): if i == 5: print 'found it! i = %s' % ielse: print 'not found it ...' 我们期望的结果是，当找到5时打印出：1found it! i = 5 实际上打印出来的结果为：12found it! i = 5not found it ... 显然这不是我们期望的结果。 根据官方文档说法： When the items are exhausted (which is immediately when the sequence is empty), the suite in the else clause, if present, is executed, and the loop terminates. A break statement executed in the first suite terminates the loop without executing the else clause’s suite. A continue statement executed in the first suite skips the rest of the suite and continues with the next item, or with the else clause if there was no next item. https://docs.python.org/2/reference/compound_stmts.html#the-for-statement 大意是说当迭代的对象迭代完并为空时，位于else的子句将执行，而如果在for循环中含有break时则直接终止循环，并不会执行else子句。 所以正确的写法应该为：123456for i in range(10): if i == 5: print 'found it! i = %s' % i breakelse: print 'not found it ...' 当使用pylint检测代码时会提示:1Else clause on loop without a break statement (useless-else-on-loop) 所以养成使用pylint检测代码的习惯还是很有必要的，像这种逻辑错误不注意点还是很难发现的。 同样的原理适用于while ... else循环 参考 https://www.cnblogs.com/dspace/p/6622799.html]]></content>
      <categories>
        <category>Python3 进阶</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[What is the difference between “ is None ” and “ ==None ” --- Reprint]]></title>
    <url>%2F2018%2F03%2F29%2FWhat-is-the-difference-between-%E2%80%9C-is-None-%E2%80%9D-and-%E2%80%9C-None-%E2%80%9D-Reprint%2F</url>
    <content type="text"><![CDATA[Answer 1The answer is explained here. To quote: A class is free to implement comparison any way it chooses, and it can choose to make comparison against None mean something (which actually makes sense; if someone told you to implement the None object from scratch, how else would you get it to compare True against itself?). Practically-speaking, there is not much difference since custom comparison operators are rare. But you should use is None as a general rule. is None is a bit (~50%) faster than == None :) – Nas Banov Answer 2is always returns True if it compares the same object instance Whereas == is ultimately determined by the __eq__() method i.e. 12345678910class Foo: def __eq__(self,other): return Truefoo=Foo()print(foo==None)# Trueprint(foo is None)# False Answer 3In this case, they are the same. None is a singleton object (there only ever exists one None). is checks to see if the object is the same object, while == just checks if they are equivalent. For example:1234p = [1]q = [1]p is q # False because they are not the same actual objectp == q # True because they are equivalent But since there is only one None, they will always be the same, and is will return True. 123p = Noneq = Nonep is q # True because they are both pointing to the same "None" Answer 4(ob1 is ob2) equal to (id(ob1) == id(ob2)) … but (ob is ob2) is a LOT faster. Timeit says “(a is b)” is 0.0365 usec per loop and “(id(a)==id(b))” is 0.153 usec per loop. 4.2x faster! – AKX {} is {} is false and id({}) == id({}) can be (and is in CPython) true. Reference What is the difference between “ is None ” and “ ==None ” Is there any difference between “foo is None” and “foo == None”?]]></content>
      <categories>
        <category>Python3 进阶</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Differences between `==` and `is` in Python3?---(Reprint)]]></title>
    <url>%2F2018%2F03%2F29%2FDifferences-between-and-is-in-Python3---(Reprint)%2F</url>
    <content type="text"><![CDATA[Source QuestionMy Google-fu has failed me. In Python, are the following two tests for equality equivalent? 12345678n = 5# Test one.if n == 5: print 'Yay!'# Test two.if n is 5: print 'Yay!' Does this hold true for objects where you would be comparing instances (a list say)? Okay, so this kind of answers my question: 123456789L = []L.append(1)if L == [1]: print 'Yay!'# Holds true, but...if L is [1]: print 'Yay!'# Doesn't. So == tests value where is tests to see if they are the same object? Answers 1is will return True if two variables point to the same object, == if the objects referred to by the variables are equal. 1234567891011&gt;&gt;&gt; a = [1, 2, 3]&gt;&gt;&gt; b = a&gt;&gt;&gt; b is aTrue&gt;&gt;&gt; b == aTrue&gt;&gt;&gt; b = a[:]&gt;&gt;&gt; b is aFalse&gt;&gt;&gt; b == aTrue In your case, the second test only works because Python caches small integer objects, which is an implementation detail. For larger integers, this does not work: 1234&gt;&gt;&gt; 1000 is 10**3False&gt;&gt;&gt; 1000 == 10**3True The same holds true for string literals: 123456789&gt;&gt;&gt; "a" is "a"True&gt;&gt;&gt; "aa" is "a" * 2True&gt;&gt;&gt; x = "a"&gt;&gt;&gt; "aa" is x * 2False&gt;&gt;&gt; "aa" is intern(x*2)True Please see this question as well. Answers 2There is a simple rule of thumb to tell you when to use == or is. == is for value equality. Use it when you would like to know if two objects have the same value. is is for reference equality. Use it when you would like to know if two references refer to the same object. In general, when you are comparing something to a simple type, you are usually checking for value equality, so you should use ==. For example, the intention of your example is probably to check whether x has a value equal to 2 (==), not whether x is literally referring to the same object as 2. Something else to note: because of the way the CPython reference implementation works, you’ll get unexpected and inconsistent results if you mistakenly use is to compare for reference equality on integers: 123456&gt;&gt;&gt; a = 500&gt;&gt;&gt; b = 500&gt;&gt;&gt; a == bTrue&gt;&gt;&gt; a is bFalse That’s pretty much what we expected: a and b have the same value, but are distinct entities. But what about this? 123456&gt;&gt;&gt; c = 200&gt;&gt;&gt; d = 200&gt;&gt;&gt; c == dTrue&gt;&gt;&gt; c is dTrue This is inconsistent with the earlier result. What’s going on here? It turns out the reference implementation of Python caches integer objects in the range -5..256 as singleton instances for performance reasons. Here’s an example demonstrating this: 123456789101112&gt;&gt;&gt; for i in range(250, 260): a = i; print "%i: %s" % (i, a is int(str(i)));...250: True251: True252: True253: True254: True255: True256: True257: False258: False259: False This is another obvious reason not to use is: the behavior is left up to implementations when you’re erroneously using it for value equality. References Is there a difference between == and is in Python?]]></content>
      <categories>
        <category>Python3 进阶</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[原码、反码、补码]]></title>
    <url>%2F2018%2F03%2F27%2F%E5%8E%9F%E7%A0%81%E3%80%81%E5%8F%8D%E7%A0%81%E3%80%81%E8%A1%A5%E7%A0%81%2F</url>
    <content type="text"><![CDATA[机器数和真值在学习原码, 反码和补码之前, 需要先了解机器数和真值的概念 机器数一个数在计算机中的二进制表示形式, 叫做这个数的机器数。机器数是带符号的，在计算机用一个数的最高位存放符号, 正数为0, 负数为1. 比如，十进制中的数 +3 ，假设计算机字长为8位，转换成二进制就是 00000011。如果是 -3 ，就是 10000011 。那么，这里的 &gt;00000011 和 10000011 就是机器数。 真值因为第一位是符号位，所以机器数的形式值就不等于真正的数值。例如上面的有符号数 10000011，其最高位 1 代表负，其真正数值是 -3 而不是形式值131（10000011 转换成十进制等于 131）。所以，为区别起见，将带符号位的机器数对应的真正数值称为机器数的真值。 例：0000 0001 的真值 = +000 0001 = +1，1000 0001 的真值 = –000 0001 = –1 在探求为何机器要使用补码之前, 让我们先了解原码, 反码和补码的概念 原码原码就是符号位加上真值的绝对值, 即用第一位表示符号, 其余位表示值. 比如如果是 8 位二进制: [+1]原 = 0000 0001 [-1]原 = 1000 0001 因为第一位是符号位, 所以 8 位二进制数的取值范围就是: [1111 1111 , 0111 1111] 即 [-127 , 127] 原码是人脑最容易理解和计算的表示方式. 反码反码的表示方法是: 正数的反码是其本身，负数的反码是在其原码的基础上, 符号位不变，其余各个位取反。 [+1] = [00000001]原 = [00000001]反 [-1] = [10000001]原 = [11111110]反 可见如果一个反码表示的是负数, 人脑无法直观的看出来它的数值. 通常要将其转换成原码再计算. 补码补码的表示方法是: 正数的补码就是其本身, 负数的补码是在其原码的基础上, 符号位不变, 其余各位取反, 最后 +1. (即在反码的基础上 +1) [+1] = [00000001]原 = [00000001]反 = [00000001]补 [-1] = [10000001]原 = [11111110]反 = [11111111]补 对于负数, 补码表示方式也是人脑无法直观看出其数值的. 通常也需要转换成原码在计算其数值. 为何要使用原码, 反码和补码现在我们知道了计算机可以有三种编码方式表示一个数. 对于正数三种编码方式的结果都相同: [+1] = [00000001]原 = [00000001]反 = [00000001]补 是对于负数: [-1] = [10000001]原 = [11111110]反 = [11111111]补 可见原码, 反码和补码是完全不同的. 既然原码才是被人脑直接识别并用于计算表示方式, 为何还会有反码和补码呢? 首先, 因为人脑可以知道第一位是符号位, 在计算的时候我们会根据符号位, 选择对真值区域的加减. (真值的概念在本文最开头). 但是对于计算机, 加减乘数已经是最基础的运算, 要设计的尽量简单. 计算机辨别”符号位”显然会让计算机的基础电路设计变得十分复杂! 于是人们想出了将符号位也参与运算的方法. 我们知道, 根据运算法则减去一个正数等于加上一个负数, 即: 1-1 = 1 + (-1) = 0 , 所以机器可以只有加法而没有减法, 这样计算机运算的设计就更简单了. 于是人们开始探索 将符号位参与运算, 并且只保留加法的方法. 首先来看原码: // 计算十进制的表达式: 1 - 1 = 0 1 - 1 = 1 + (-1) = [00000001]原 + [10000001]原 = [10000010]原 = -2 如果用原码表示, 让符号位也参与计算, 显然对于减法来说, 结果是不正确的.这也就是为何计算机内部不使用原码表示一个数。 为了解决原码做减法的问题, 出现了反码: // 计算十进制的表达式: 1 - 1 = 0 1 - 1 = 1 + (-1) = [0000 0001]原 + [1000 0001]原= [0000 0001]反 + [1111 1110]反 = [1111 1111]反 = [1000 0000]原 = -0 发现用反码计算减法, 结果的真值部分是正确的. 而唯一的问题其实就出现在”0”这个特殊的数值上. 虽然人们理解上 + 0和 -0 是一样的, 但是 0 带符号是没有任何意义的. 而且会有 [0000 0000]原 和 [1000 0000]原 两个编码表示 0. 于是补码的出现, 解决了0的符号以及两个编码的问题: 1 - 1 = 1 + (-1) = [0000 0001]原 + [1000 0001]原 = [0000 0001]补 + [1111 1111]补 = [0000 0000]补=[0000 0000]原 这样 0 用 [0000 0000] 表示, 而以前出现问题的 -0 则不存在了.而且可以用[1000 0000]表示 -128: (-1) + (-127) = [1000 0001]原 + [1111 1111]原 = [1111 1111]补 + [1000 0001]补 = [1000 0000]补 -1-127 的结果应该是 -128, 在用补码运算的结果中, [1000 0000]补 就是 -128. 但是注意因为实际上是使用以前的-0的补码来表示 -128, 所以 -128 并没有原码和反码表示 (对 -128 的补码表示 [1000 0000]补 算出来的原码是[0000 0000]原 , 这是不正确的) 使用补码, 不仅仅修复了 0 的符号以及存在两个编码的问题, 而且还能够多表示一个最低数. 这就是为什么 8 位二进制, 使用原码或反码表示的范围为 [-127, +127], 而使用补码表示的范围为 [-128, 127]. 故机器的存储是使用补码, 所以对于编程中常用到的 32 位 int 类型, 可以表示范围是: [-2^31, 2^31-1] 因为第一位表示的是符号位.而使用补码表示时又可以多保存一个最小值。 参考 https://www.jianshu.com/p/279d9eba0985]]></content>
      <tags>
        <tag>计算机基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正向代理和反向代理]]></title>
    <url>%2F2018%2F03%2F26%2F%E6%AD%A3%E5%90%91%E4%BB%A3%E7%90%86%E5%92%8C%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[正向代理（forward proxy）代理客户端，隐藏真实客户端，反向代理（reverse proxy）代理服务器，隐藏真实服务端。 反向代理举例:用户想访问：”http://ooxx.me/readme&quot;，但ooxx.me上并不存在readme页面，他是偷偷从另外一台服务器上取回来,然后作为自己的内容吐给用户，但用户并不知情，这很正常,用户一般都很笨。这里所提到的ooxx.me 这个域名对应的服务器就设置了反向代理功能。 结论：对于客户端而言它就像是原始服务器，并且客户端不需要进行任何特别的设置。客户端向反向代理的命名空间(name-space)中的内容发送普通请求，接着反向代理将判断向何处(原始服务器)转交请求，并将获得的内容返回给客户端，就像这些内容原本就是它自己的一样。 在计算机世界里，由于单个服务器的处理客户端（用户）请求能力有一个极限，当用户的接入请求蜂拥而入时，会造成服务器忙不过来的局面，可以使用多个服务器来共同分担成千上万的用户请求，这些服务器提供相同的服务，对于用户来说，根本感觉不到任何差别。 反向代理用途 保证内网的安全，隐藏和保护原始服务器。可以使用反向代理提供WAF功能，阻止web攻击。大型网站，通常将反向代理作为公网访问地址，Web服务器是内网。 负载均衡，通过反向代理服务器来优化网站的负载 反向代理的实现 需要有一个负载均衡设备来分发用户请求，将用户请求分发到空闲的服务器上 服务器返回自己的服务到负载均衡设备 负载均衡将服务器的服务返回用户 以上的潜台词是：用户和负载均衡设备直接通信，也意味着用户做服务器域名解析时，解析得到的IP其实是负载均衡的IP，而不是服务器的IP，这样有一个好处是，当新加入/移走服务器时，仅仅需要修改负载均衡的服务器列表，而不会影响现有的服务。 正向代理正向代理,也就是传说中的代理,他的工作原理就像一个跳板,简单的说,我是一个用户,我访问不了某网站,但是我能访问一个代理服务器，这个代理服务器呢,他能访问那个我不能访问的网站，于是我先连上代理服务器,告诉他我需要那个无法访问网站的内容，代理服务器去取回来,然后返回给我。从网站的角度,只在代理服务器来取内容的时候有一次记录，有时候并不知道是用户的请求,也隐藏了用户的资料,这取决于代理告不告诉网站。 正向代理是一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。客户端必须要进行一些特别的设置才能使用正向代理。 正向代理的用途 访问原来无法访问的资源，如google 可以做缓存，加速访问资源 对客户端访问授权，上网进行认证 代理可以记录用户访问记录（上网行为管理），对外隐藏用户信息 为了便于理解，摘自阿笠博士的回答,正向代理中，proxy和client同属一个LAN，对server透明；反向代理中，proxy和server同属一个LAN，对client透明。实际上proxy在两种代理中做的事都是代为收发请求和响应，不过从结构上来看正好左右互换了下，所以把后出现的那种代理方式叫成了反向代理。 正向代理和反向代理的区别从用途上来讲：正向代理的典型用途是为在防火墙内的局域网客户端提供访问Internet的途径。正向代理还可以使用缓冲特性减少网络使用率。反向代理的典型用途是将防火墙后面的服务器提供给Internet用户访问。反向代理还可以为后端的多台服务器提供负载平衡，或为后端较慢的服务器提供缓冲服务。另外，反向代理还可以启用高级URL策略和管理技术，从而使处于不同web服务器系统的web页面同时存在于同一个URL空间下。 从安全性来讲：正向代理允许客户端通过它访问任意网站并且隐藏客户端自身，因此你必须采取安全措施以确保仅为经过授权的客户端提供服务。反向代理对外都是透明的，访问者并不知道自己访问的是一个代理。 打个比方，a,b,c三个人，正向代理是a通过b向C借钱，a知道c的存在 。反向代理是a向b借钱，b又向C借，a不知道c的存在。 参考 https://www.zhihu.com/question/24723688/answer/160252724 https://blog.csdn.net/andyzhaojianhui/article/details/48247969 https://www.cnblogs.com/Anker/p/6056540.html]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一次完整的HTTP请求流程]]></title>
    <url>%2F2018%2F03%2F20%2F%E4%B8%80%E6%AC%A1%E5%AE%8C%E6%95%B4%E7%9A%84HTTP%E8%AF%B7%E6%B1%82%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[HTTP通信机制是在一次完整的HTTP通信过程中，Web浏览器与Web服务器之间将完成下列7个步骤： 1. 建立TCP连接：在HTTP工作开始之前，Web浏览器首先要通过网络与Web服务器建立连接，该连接是通过TCP来完成的，该协议与IP协议共同构建Internet， 即著名的TCP/IP协议族，因此Internet又被称作是TCP/IP网络。HTTP是比TCP更高层次的应用层协议，根据规则，只有低层协议建立之后才能进行更高层协议的连接， 因此，首先要建立TCP连接，一般TCP连接的端口号是80。 2. Web浏览器向Web服务器发送请求命令：一旦建立了TCP连接，Web浏览器就会向Web服务器发送请求命令。例如：GET/sample/hello.jsp HTTP/1.1。 3. Web浏览器发送请求头信息 ：浏览器发送其请求命令之后，还要以头信息的形式向Web服务器发送一些别的信息，之后浏览器发送了一空白行来通知服务器，它已经结束了该头信息的发送。 4. Web服务器应答 ：客户机向服务器发出请求后，服务器会客户机回送应答， HTTP/1.1 200 OK ，应答的第一部分是协议的版本号和应答状态码。 5. Web服务器发送应答头信息：正如客户端会随同请求发送关于自身的信息一样，服务器也会随同应答向用户发送关于它自己的数据及被请求的文档。 6. Web服务器向浏览器发送数据：Web服务器向浏览器发送头信息后，它会发送一个空白行来表示头信息的发送到此为结束，接着，它就以Content-Type应答头信息所描述的格式发送用户所请求的实际数据。 7. Web服务器关闭TCP连接 ：一般情况下，一旦Web服务器向浏览器发送了请求数据，它就要关闭TCP连接，然后如果浏览器或者服务器在其头信息加入了这行代码：Connection:keep-alive； TCP连接在发送后将仍然保持打开状态，于是，浏览器可以继续通过相同的连接发送请求。保持连接节省了为每个请求建立新连接所需的时间，还节约了网络带宽。]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三次握手 | 四次挥手]]></title>
    <url>%2F2018%2F03%2F19%2F%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%20%7C%20%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%2F</url>
    <content type="text"><![CDATA[三次握手第一次握手 建立连接时，客户端发送syn包（syn=j）到服务器，并进入SYN_SENT状态，等待服务器确认；SYN即同步序列编号(synchronize sequence numbers)。 第二次握手 服务器收到syn包，必须确认客户的SYN（ack = j+1）,同时自己也发送一个SYN包（syn=k），即发送SYN和ACK包，此时服务器进入SYN_RECV状态。 第三次握手 客户端收到服务器的SYN+ACK包，向服务器发送确认包ACK（ack=k+1），此包发送完毕，客户端和服务器进入ESTABLISHED状态，TCP连接成功，完成三次握手。 完成上述三次握手后，客户端和服务器开始传送数据。 四次挥手对于一个已经建立的连接，TCP使用改进的三次握手来释放连接（使用一个带有FIN附加标记的字段）。TCP关闭连接的步骤如下 当主机A的应用程序通知TCP数据已经发送完毕时，TCP向主机B发送一个带有FIN附加标记的报文段（FIN=finish） 当主机B收到这个FIN报文段之后，并不立即用FIN报文回复主机A，而是想主机A发送一个确认序号ACK，同时通知自己的应用程序：对方要求关闭连接。【先发送ACK的目的是为了防止在这段时间内，对方重传FIN报文】 主机B的应用程序告诉TCP：我要彻底关闭连接，TCP向主机A发送一个FIN报文段。 主机A收到这个FIN报文段后，向主机B发送一个ACK表示连接彻底释放。]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 回文字符串的判断]]></title>
    <url>%2F2018%2F03%2F19%2FPython3-%E5%9B%9E%E6%96%87%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E5%88%A4%E6%96%AD%2F</url>
    <content type="text"><![CDATA[1234567def isPalindrome(str_): if len(str_) &lt; 2: return True if str_[0] != str_[-1]: return False return isPalindrome(str_[1:-1])]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 单例模式]]></title>
    <url>%2F2018%2F03%2F19%2FPython3-%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[单例模式（Singleton Pattern）是最简单的设计模式之一。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。这种模式涉及到一个单一的类，该类负责创建自己的对象，同时确保只有单个对象被创建。这个类提供了一种访问其唯一的对象的方式，可以直接访问，不需要实例化该类的对象。 注意： 1、单例类只能有一个实例。 2、单例类必须自己创建自己的唯一实例。 3、单例类必须给所有其他对象提供这一实例。 单例模式介绍意图： 保证一个类仅有一个实例，并提供一个访问它的全局访问点。 主要解决： 一个全局使用的类频繁地创建与销毁。 何时使用： 当您想控制实例数目，节省系统资源的时候。 如何解决： 判断系统是否已经有这个单例，如果有则返回，如果没有则创建。 关键代码： 构造函数是私有的。 应用实例： 1、一个党只能有一个主席。 2、Windows 是多进程多线程的，在操作一个文件的时候，就不可避免地出现多个进程或线程同时操作一个文件的现象，所以所有文件的处理必须通过唯一的实例来进行。 3、一些设备管理器常常设计为单例模式，比如一个电脑有两台打印机，在输出的时候就要处理不能两台打印机打印同一个文件。 优点： 1、在内存里只有一个实例，减少了内存的开销，尤其是频繁的创建和销毁实例（比如管理学院首页页面缓存）。 2、避免对资源的多重占用（比如写文件操作）。 缺点： 没有接口，不能继承，与单一职责原则冲突，一个类应该只关心内部逻辑，而不关心外面怎么样来实例化。 使用场景： 1、要求生产唯一序列号。 2、WEB 中的计数器，不用每次刷新都在数据库里加一次，用单例先缓存起来。 3、创建的一个对象需要消耗的资源过多，比如 I/O 与数据库的连接等。 注意事项： getInstance() 方法中需要使用同步锁 synchronized (Singleton.class) 防止多线程同时进入造成 instance 被多次实例化。 单例模式的实现在 Python 中，我们可以用多种方法来实现单例模式： 使用基类 __new__ 使用模块 使用装饰器（decorator） 使用元类（metaclass） 使用基类 __new____new__ 是真正创建实例对象的方法，所以重写基类的__new__方法，以此来保证创建对象的时候只生成一个实例 12345678class Singleton(object): def __new__(cls, *args, **kwargs): if not hasattr(cls, '_instance'): cls._instance = super(Singleton, cls).__new__(cls, *args, **kwargs) return cls._instance class MyClass(Singleton): a = 1 在上面的代码中，我们将类的实例和一个类变量 _instance 关联起来，如果 cls._instance 为 None 则创建实例，否则直接返回 cls._instance。执行结果如下：12345678&gt;&gt;&gt; one = MyClass()&gt;&gt;&gt; two = MyClass()&gt;&gt;&gt; one == twoTrue&gt;&gt;&gt; one is twoTrue&gt;&gt;&gt; id(one), id(two)(4303862608, 4303862608) 使用元类元类（参考：深刻理解Python中的元类）是用于创建类对象的类，类对象创建实例对象时一定会调用__call__方法，因此在调用__call__时候保证始终只创建一个实例即可，type是python中的一个元类。 元类（metaclass）可以控制类的创建过程，它主要做三件事： 拦截类的创建 修改类的定义 返回修改后的类 使用元类实现单例模式的代码如下： 123456789101112131415161718class Singleton(type): def __call__(cls, *args, **kwargs): if not hasattr(cls, '_instance'): cls._instance = super(Singleton, cls).__call__(*args, **kwargs) return cls._instanceclass MyClass(metaclass=Singleton): a = 1## 执行结果如下&gt;&gt;&gt; one = MyClass()&gt;&gt;&gt; two = MyClass()&gt;&gt;&gt; one == twoTrue&gt;&gt;&gt; one is twoTrue&gt;&gt;&gt; id(one), id(two)(4303862608, 4303862608) 使用装饰器装饰器（decorator）可以动态地修改一个类或函数的功能。这里，我们也可以使用装饰器来装饰某个类，使其只能生成一个实例，代码如下： 1234567891011def singleton(cls): instances = &#123;&#125; def wrapper(*args, **kwargs): if cls not in instances: instances[cls] = cls(*args, **kwargs) return instances[cls] return wrapper@singletonclass Foo(object): pass 将装饰器写成类形式12345678910111213class single03(object): def __init__(self, cls): self._cls = cls self._instances = None; def __call__(self, *args): if not self._instances: self._instances = self._cls(*args) return self._instances @single03 class A(object): def __init__(self, name): self.name = name 在上面，我们定义了一个装饰器 singleton，它返回了一个内部函数 warpper，该函数会判断某个类是否在字典 instances 中，如果不存在，则会将 cls 作为 key，cls(*args, **kw) 作为 value 存到 instances 中，否则，直接返回 instances[cls] 使用模块Python 的模块就是天然的单例模式，因为模块在第一次导入时，会生成 .pyc 文件，当第二次导入时，就会直接加载 .pyc 文件，而不会再次执行模块代码。因此，我们只需把相关的函数和数据定义在一个模块中，就可以获得一个单例对象了。如果我们真的想要一个单例类，可以考虑这样做：123456# mysingleton.pyclass My_Singleton(object): def foo(self): passmy_singleton = My_Singleton() 将上面的代码保存在文件 mysingleton.py 中，然后这样使用:123from mysingleton import my_singletonmy_singleton.foo() 参考 http://python.jobbole.com/87294/ http://python.jobbole.com/87791/?utm_source=blog.jobbole.com&amp;utm_medium=relatedPosts]]></content>
      <categories>
        <category>Python3 进阶</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 基于链表技术实现栈]]></title>
    <url>%2F2018%2F03%2F17%2FPython3-%E5%9F%BA%E4%BA%8E%E9%93%BE%E8%A1%A8%E6%8A%80%E6%9C%AF%E5%AE%9E%E7%8E%B0%E6%A0%88%2F</url>
    <content type="text"><![CDATA[定义异常类实现栈结构之前，先考虑为操作失败的处理定义一个异常类。在这里通过继承已有的异常类定义自己的异常类。由于栈操作(如空栈弹出)时不满足需要可以看做参数值错误，采用下面的定义12class StackUnderflow(ValueError): # 栈下溢，空栈访问 pass 定义一个表结点类1234class LNode: def __init__(self, elem, next_=None): self.elem = elem self.next = next_ 方法的第二个参数用名字next_，是为了避免与Python标准函数next重名 栈的链接表实现由于所有栈的操作都在线性表的一端进行，采用链接表技术，自然应该用表头一端作为栈顶，表尾作为栈底，是操作实现方便，效率也高。按照这种安排，容易定义出一个链接栈类。123456789101112131415161718192021class LStack: def __init__(self): self._top = None def is_empty(self): return self._top is None def top(self): if self._top is None: raise StackUnderflow('in LStack.top()') return self._top.elem def push(self, elem): self._top = LNode(elem, self._top) def pop(self): if self._top is None: raise StackUnderflow('in LStack.pop()') p = self._top self._top = p.next return p.elem 参考 《数据结构与算法Python语言描述》 — 裘宗燕]]></content>
      <categories>
        <category>Data Structures</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Algorithm</tag>
        <tag>Data Structures</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 基于顺序表技术实现栈类]]></title>
    <url>%2F2018%2F03%2F17%2FPython3-%E5%9F%BA%E4%BA%8E%E9%A1%BA%E5%BA%8F%E8%A1%A8%E6%8A%80%E6%9C%AF%E5%AE%9E%E7%8E%B0%E6%A0%88%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[定义异常类实现栈结构之前，先考虑为操作失败的处理定义一个异常类。在这里通过继承已有的异常类定义自己的异常类。由于栈操作(如空栈弹出)时不满足需要可以看做参数值错误，采用下面的定义12class StackUnderflow(ValueError): # 栈下溢，空栈访问 pass 上面把异常StackUnderflow定义为ValueError子类，只简单定义了一个类名，类体部分只有一个pass语句，未定义任何新属性，因为不准备提供ValueError之外的新功能，只是想与其他ValueError异常有所区分，程序出错时能产生不同的错误信息。必要时可以定义专门的异常处理操作。自定义异常与python内置异常类似，同样通过except进行捕捉和处理，但只能通过raise语句引发。 栈类定义把list当做栈使用时，完全可以满足应用需要。但是，这样建立的对象实际上还是list，提供了list类型的所有操作。特别是提供了一大批栈结构原本不应该支持的操作，威胁栈的使用安全性(例如，栈要求未经弹出的元素应该存在，但表运行任意删除)。另外，这样的“栈”不是一个独立的类型，因此没有独立类型的所有重要性质。 为了概念更清晰，实现更安全，操作名也更容易理解，，可以考虑使用顺序表定义一个栈类，使之成为一个独立的类型，把Python的list隐藏在类内部，作为其实现基础。 123456789101112131415161718192021222324"""基于顺序表实现栈类用list对象 _elems存储栈中的元素所有的栈操作都映射到list操作"""class SStack(object): def __init__(self): self._elems = [] def is_empty(self): return self._elems == [] def top(self): if self._elems == []: raise StackUnderflow('in SStack.top()') return self._elems[-1] def push(self, elem): self._elems.append(elem) def pop(self): if self._elems == []: raise StackUnderflow('in SStack.pop()') return self._elems.pop() 参考 《数据结构与算法Python语言描述》 — 裘宗燕]]></content>
      <categories>
        <category>Data Structures</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Algorithm</tag>
        <tag>Data Structures</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 实现二叉树前、中、后序遍历及按层遍历]]></title>
    <url>%2F2018%2F03%2F15%2FPython3-%E5%AE%9E%E7%8E%B0%E4%BA%8C%E5%8F%89%E6%A0%91%E5%89%8D%E3%80%81%E4%B8%AD%E3%80%81%E5%90%8E%E5%BA%8F%E9%81%8D%E5%8E%86%E5%8F%8A%E6%8C%89%E5%B1%82%E9%81%8D%E5%8E%86%2F</url>
    <content type="text"><![CDATA[假设有这么一个二叉树如下： 前序遍历结果：1, 2, 4, 5, 8, 9, 11, 3, 6, 7, 10中序遍历结果：4, 2, 8, 5, 11, 9, 1, 6, 3, 10, 7后序遍历结果：4, 8, 11, 9, 5, 2, 6, 10, 7, 3, 1 二叉树的类实现123456789101112class Node(object): def __init__(self, value=None, left=None, right=None): self.value = value self.left = left self.right = rightif __name__ == "__main__": tree = Node(1, Node(2, Node(4), Node(5, Node(8), Node(9, left=Node(11)))), Node(3, Node(6), Node(7, left=Node(10)))) 深度优先遍历递归法1234567# 前序遍历（递归）def pre_deep_func(root): if root is None: return print(root.value, end = ' ') # print 放到下一行 就是中序遍历，放到最后 就是后序遍历 pre_deep_func(root.left) pre_deep_func(root.right) 非递归法前序遍历根据已有的认识，此函数需要一个栈，保存树尚未访问过的部分信息。对于前序遍历也会有不同的实现方法，下面考虑一种方法，即： 由于采取先序遍历，遇到结点就应该访问，下一步就应该沿着树的坐分支下行 但结点的右分支（右子树）还没有访问，因此需要记录，将右子结点入栈。 遇到空树时回溯，取出栈中保存的一个右分支，像一颗二叉树一样遍历它。 1234567891011121314151617181920212223242526272829303132## 方法一 常规打印def preorder_nonrec(root): s = [] while s or root: while root: # 沿左分支下行 print(root.value, end = ' ') # 先处理根数据 s.append(root.right) # 右分支入栈 root = root.left root = s.pop() # 遇到空树，回溯## 方法二 通过生成器函数遍历def preorder_elements(root): s = [] while s or root: while root: s.append(root.right) yield root.value root = root.left root = s.pop()## 方法三# 前序遍历（根左右）:模拟压栈过程# 入栈之前读（根、左），这样出栈时再读右（也是右结点子节点们的根）def pre_deep_func2(root): a = [] while a or root: while root: print(root.value, end = ' ') a.append(root) ## 根入栈 root = root.left h = a.pop() root = h.right 非递归算法的一个价值是把算法过程完整的暴露出来，便于进行细致的分析。时间复杂度：在非递归的算法中，因为在执行的过程中访问每个结点一次，一部分子树(所有右子树，方法一、二)被压入和弹出各一次(栈操作是O(1)时间)，所以整个遍历过程需要的时间复杂度为O(n)。空间复杂度：这里的关键因素是遍历中栈可能达到的最大深度（栈中元素的最大深度个数），而栈的最大深度由被遍历的二叉树的高度决定。由于二叉树的高度可能达到O(n)，所以在最坏情况下，算法的空间复杂度为O(n)，n个结点的二叉树的平均高度为O(log n)，所以非递归前序遍历的平均空间复杂度为O(log n)。在一些情况下，修改实现方法也可能降低空间的开销。对于上面函数，修改其定义，只把非空的右子树进栈，在很多情况下能减小一些空间开销。 其他非递归的遍历算法，包括中序遍历和后续遍历算法以及层次遍历算法，都可以直接了当的修改成迭代器。但是递归算法不可以。 中序遍历1234567891011# 中序遍历（左根右）:模拟压栈过程# 出栈之后读（左、根），这样出栈后指针变更再读右def mid_deep_func2(root): a = [] while a or root: while root: a.append(root) root = root.left h = a.pop() print h.value root = h.right 后序遍历1234567891011121314151617181920212223242526# 后序遍历（左右根）:模拟逆序(根右左)存入数组b，然后再数组b逆序输出# (根右左)与(根左右)类似，入栈a前读（根、右），出栈后指针变更再读左## 方法 1def after_deep_func2(root): a = [] b = [] while a or root: while root: b.append(root.value) a.append(root.left) root = root.right root = a.pop() print(b[::-1])## 方法2def after_deep_func2(root): a = [] b = [] while a or root: while root: b.append(root.value) a.append(root) root = root.right h = a.pop() root = h.left print b[::-1] 广度优先遍历12345678910def level_func(root): a = [] a.append(root) while a: head = a.pop(0) print head.value if head.left: a.append(head.left) if head.right: a.append(head.right) 参考 http://blog.csdn.net/su92chen/article/details/70242822 《数据结构与算法Python语言描述》 — 裘宗燕]]></content>
      <categories>
        <category>Data Structures</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Algorithm</tag>
        <tag>Data Structures</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是Socket(转)]]></title>
    <url>%2F2018%2F03%2F13%2F%E4%BB%80%E4%B9%88%E6%98%AFSocket-%E8%BD%AC%2F</url>
    <content type="text"><![CDATA[对TCP/IP、UDP、Socket编程这些词你不会很陌生吧？随着网络技术的发展，这些词充斥着我们的耳朵。那么我想问： 什么是TCP/IP、UDP？ Socket在哪里呢？ Socket是什么呢？ 你会使用它们吗？ 什么是TCP/IP、UDPTCP/IP（Transmission Control Protocol/Internet Protocol）即传输控制协议/网间协议，是一个工业标准的协议集，它是为广域网（WANs）设计的。UDP（User Data Protocol，用户数据报协议）是与TCP相对应的协议。它是属于TCP/IP协议族中的一种。 TCP/IP协议族包括运输层、网络层、链路层。由上图可以知道TCP/IP与UDP的关系。 Socket在哪里 Socket是什么Socket是应用层与TCP/IP协议族通信的中间软件抽象层，它是一组接口。在设计模式中，Socket其实就是一个门面模式，它把复杂的TCP/IP协议族隐藏在Socket接口后面，对用户来说，一组简单的接口就是全部，让Socket去组织数据，以符合指定的协议。 如何使用前人已经给我们做了好多的事了，网络间的通信也就简单了许多，但毕竟还是有挺多工作要做的。以前听到Socket编程，觉得它是比较高深的编程知识，但是只要弄清Socket编程的工作原理，神秘的面纱也就揭开了。一个生活中的场景。你要打电话给一个朋友，先拨号，朋友听到电话铃声后提起电话，这时你和你的朋友就建立起了连接，就可以讲话了。等交流结束，挂断电话结束此次交谈。生活中的场景就解释了这工作原理，也许TCP/IP协议族就是诞生于生活中，这也不一定。 先从服务器端说起。服务器端先初始化Socket，然后与端口绑定(bind)，对端口进行监听(listen)，调用accept阻塞，等待客户端连接。在这时如果有个客户端初始化一个Socket，然后连接服务器(connect)，如果连接成功，这时客户端与服务器端的连接就建立了。客户端发送数据请求，服务器端接收请求并处理请求，然后把回应数据发送给客户端，客户端读取数据，最后关闭连接，一次交互结束。 在这里我就举个简单的例子，我们走的是TCP协议这条路（见图2）。例子用MFC编写，运行的界面如下： 在客户端输入服务器端的IP地址和发送的数据，然后按发送按钮，服务器端接收到数据，然后回应客户端。客户端读取回应的数据，显示在界面上。 客户端就一个函数完成了一次通信。在这里IP地址为何用127.0.0.1呢？使用这个IP地址，服务器端和客户端就能运行在同一台机器上，这样调试方便多了。当然你可以在你朋友的机器上运行Server程序(本人在局域网中测试过)，在自己的机器上运行Client程序，当然输入的IP地址就该是你朋友机器的IP地址了。 参考 http://www.360doc.com/content/11/0609/15/5482098_122692444.shtml]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 实现遍历目录与子目录，并抓取.py文件]]></title>
    <url>%2F2018%2F03%2F12%2FPython3-%E5%AE%9E%E7%8E%B0%E9%81%8D%E5%8E%86%E7%9B%AE%E5%BD%95%E4%B8%8E%E5%AD%90%E7%9B%AE%E5%BD%95%EF%BC%8C%E5%B9%B6%E6%8A%93%E5%8F%96-py%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[12345678910tree test.├── subtest1│ ├── subsubdir│ │ └── sub.py│ └── test1.py├── subtest2│ └── test2.py└── subtest3 └── test3.py 1234567891011121314151617181920212223# 1. for-in dir/subdir to get the filesname # 2. splitext filename to filter import os def getFiles(dir, suffix): res = [] for root, directory, files in os.walk(dir): for filename in files: name, suf = os.path.splitext(filename) if suf == suffix: res.append(os.path.join(root, filename)) return res for file in getFiles("./", '.py'): print(file)# output:# ./walkdir.py# ./subtest2/test2.py# ./subtest3/test3.py# ./subtest1/test1.py# ./subtest1/subsubdir/sub.py]]></content>
      <categories>
        <category>Python3 进阶</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 os和sys模块的作用，以及常用的模块方法]]></title>
    <url>%2F2018%2F03%2F12%2FPython3-os%E5%92%8Csys%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BD%9C%E7%94%A8%EF%BC%8C%E4%BB%A5%E5%8F%8A%E5%B8%B8%E7%94%A8%E7%9A%84%E6%A8%A1%E5%9D%97%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[官方解释： os： This module provides a portable way of using operating system dependent functionality. 翻译：提供一种方便的使用操作系统函数的方法。 sys：This module provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter. 翻译：提供访问由解释器使用或维护的变量和在与解释器交互使用到的函数。 os 常用方法1234567891011121314151617181920os.remove() # 删除文件 os.rename() # 重命名文件 os.walk() # 生成目录树下的所有文件名 os.chdir() # 改变目录 os.mkdir/makedirs # 创建目录/多层目录 os.rmdir/removedirs # 删除目录/多层目录 os.listdir() # 列出指定目录的文件 os.getcwd() # 取得当前工作目录 os.chmod() # 改变目录权限 os.path.basename() # 去掉目录路径，返回文件名 os.path.dirname() # 去掉文件名，返回目录路径 os.path.join() # 将分离的各部分组合成一个路径名 os.path.split() # 返回（dirname(),basename())元组 os.path.splitext() # 返回(filename,extension)元组 os.path.getatime|ctime|mtime # 分别返回最近访问、创建、修改时间 os.path.getsize() # 返回文件大小 os.path.exists() # 是否存在 os.path.isabs() # 是否为绝对路径 os.path.isdir() # 是否为目录 os.path.isfile() # 是否为文件 sys 常用方法1234567891011121314151617181920sys.argv # 命令行参数List，第一个元素是程序本身路径 sys.modules.keys() # 返回所有已经导入的模块列表 sys.exc_info() # 获取当前正在处理的异常类,exc_type、exc_value、exc_traceback当前处理的异常详细信息 sys.exit(n) # 退出程序，正常退出时exit(0) sys.hexversion # 获取Python解释程序的版本值，16进制格式如：0x020403F0 sys.version # 获取Python解释程序的版本信息 sys.maxint # 最大的Int值 sys.maxunicode # 最大的Unicode值 sys.modules # 返回系统导入的模块字段，key是模块名，value是模块 sys.path # 返回模块的搜索路径，初始化时使用PYTHONPATH环境变量的值 sys.platform # 返回操作系统平台名称 sys.stdout # 标准输出 sys.stdin # 标准输入 sys.stderr # 错误输出 sys.exc_clear() # 用来清除当前线程所出现的当前的或最近的错误信息 sys.exec_prefix # 返回平台独立的python文件安装的位置 sys.byteorder # 本地字节规则的指示器，big-endian平台的值是'big',little-endian平台的值是'little' sys.copyright # 记录python版权相关的东西 sys.api_version # 解释器的C的API版本 sys.version_info]]></content>
      <categories>
        <category>Python3 进阶</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tesseract的使用]]></title>
    <url>%2F2018%2F03%2F12%2FTesseract%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Running Tesseract with command-lineTesseract is a command-line program, so first open a terminal or command prompt. The command is used like this:1tesseract imagename outputbase [-l lang] [-psm pagesegmode] [configfile...] So basic usage to do OCR on an image called ‘myscan.png’ and save the result to ‘out.txt’ would be:1tesseract myscan.png out Or to do the same with German:1tesseract myscan.png out -l deu It can even be used with multiple languages traineddata at a time eg. English and German:1tesseract myscan.png out -l eng+deu Tesseract also includes a hOCR mode, which produces a special HTML file with the coordinates of each word. This can be used to create a searchable pdf, using a tool such as Hocr2PDF. To use it, use the ‘hocr’ config option, like this:1tesseract myscan.png out hocr You can also create a searchable pdf directly from tesseract ( versions &gt;=3.03):1tesseract myscan.png out pdf More information about the various options is available in the Tesseract manpage. Other LanguagesTesseract has been trained for many languages, check for your language in the Tessdata repository. For example, if we want Tesseract support Chinese language, just put chi_sim.traineddata into the path /usr/local/Cellar/tesseract/3.05.01/share/tessdata/。 It can also be trained to support other languages and scripts; for more details see TrainingTesseract. Running Tesseract with PythonPython-tesseract is an optical character recognition (OCR) tool for python. That is, it will recognize and “read” the text embedded in images. Usage Quick start 123456789101112131415161718192021try: import Imageexcept ImportError: from PIL import Imageimport pytesseractpytesseract.pytesseract.tesseract_cmd = '&lt;full_path_to_your_tesseract_executable&gt;'# Include the above line, if you don't have tesseract executable in your PATH# Example tesseract_cmd: 'C:\\Program Files (x86)\\Tesseract-OCR\\tesseract'# Simple image to stringprint(pytesseract.image_to_string(Image.open('test.png')))# French text image to stringprint(pytesseract.image_to_string(Image.open('test-european.jpg'), lang='fra'))# Get bounding box estimatesprint(pytesseract.image_to_boxes(Image.open('test.png')))# Get verbose data including boxes, confidences, line and page numbersprint(pytesseract.image_to_data(Image.open('test.png'))) Support for OpenCV image/NumPy array objects123456import cv2img = cv2.imread('/**path_to_image**/digits.png')print(pytesseract.image_to_string(img))# OR explicit beforehand convertingprint(pytesseract.image_to_string(Image.fromarray(img)) Add the following config, if you have tessdata error like: “Error opening data file…”12345tessdata_dir_config = '--tessdata-dir "&lt;replace_with_your_tessdata_dir_path&gt;"'# Example config: '--tessdata-dir "C:\\Program Files (x86)\\Tesseract-OCR\\tessdata"'# It's important to add double quotes around the dir path.pytesseract.image_to_string(image, lang='chi_sim', config=tessdata_dir_config) Functions image_to_string Returns the result of a Tesseract OCR run on the image to string image_to_boxes Returns result containing recognized characters and their box boundaries image_to_data Returns result containing box boundaries, confidences, and other information. Requires Tesseract 3.05+. For more information, please check the Tesseract TSV documentation Parameters image_to_data(image, lang=None, config=&#39;&#39;, nice=0, output_type=Output.STRING) image Object, PIL Image/NumPy array of the image to be processed by Tesseract lang String, Tesseract language code string config String, Any additional configurations as a string, ex: config=&#39;--psm 6&#39; nice Integer, modifies the processor priority for the Tesseract run. Not supported on Windows. Nice adjusts the niceness of unix-like processes. output_type Class attribute, specifies the type of the output, defaults to string. For the full list of all supported types, please check the definition of pytesseract.Output class.]]></content>
      <categories>
        <category>Tesseract</category>
      </categories>
      <tags>
        <tag>Tesseract</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 将两个排好序的列表合并成一个有序列表]]></title>
    <url>%2F2018%2F03%2F12%2FPython3-%E5%B0%86%E4%B8%A4%E4%B8%AA%E6%8E%92%E5%A5%BD%E5%BA%8F%E7%9A%84%E5%88%97%E8%A1%A8%E5%90%88%E5%B9%B6%E6%88%90%E4%B8%80%E4%B8%AA%E6%9C%89%E5%BA%8F%E5%88%97%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425def merge_list(a, b): if not a: return b if not b: return a a_index = b_index = 0 ret = [] while a_index &lt; len(a) and b_index &lt; len(b): if a[a_index] &lt;= b[b_index]: ret.append(a[a_index]) a_index += 1 else: ret.append(b[b_index]) b_index += 1 if a_index &lt; len(a): ret.extend(a[a_index:]) if b_index &lt; len(b): ret.extend(b[b_index:]) return retif __name__ == '__main__': a = [1, 2, 3, 5, 7, 9] b = [1, 2, 2, 4, 5, 6, 8, 10] print(merge_list(a, b))]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 实现常见的各种排序方法]]></title>
    <url>%2F2018%2F03%2F09%2FPython3-%E5%AE%9E%E7%8E%B0%E5%B8%B8%E8%A7%81%E7%9A%84%E5%90%84%E7%A7%8D%E6%8E%92%E5%BA%8F%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[快速排序快速排序的思路： 先从数列中取出一个数作为基准数。 分区过程，将比这个数大的数全放到它的右边，小于或等于它的数全放到它的左边(假定从小到大排序)。 再对左右区间重复第二步，直到各区间只有一个数 1234567891011def quicksort(array): """快速排序""" if len(array) &lt; 2: # 基线条件：为空或者只有一个元素的数组是有序的 return array else: pivot = array[0] # 递归条件,基准值 less = [i for i in array[1:] if i &lt;= pivot] # 所有小于基准值的元素组成的子数组 greater = [i for i in array[1:] if i &gt; pivot] # 所有大于基准值的元素组成的子数组 return quicksort(less) + [pivot] + quicksort(greater)print(quicksort([1,2,7,4,2,9,3])) 冒泡排序冒泡排序原理即：从数组下标为0的位置开始，比较下标位置为0和1的数据，如果0号位置的大，则交换位置，如果1号位置大，则什么也不做，然后右移一个位置，比较1号和2号的数据，和刚才的一样，如果1号的大，则交换位置，以此类推直至最后一个位置结束，到此数组中最大的元素就被排到了最后，之后再根据之前的步骤开始排前面的数据，直至全部数据都排序完成。 就是传说中的大的沉到底原则，适用于小量数据 冒泡排序思路: 每一趟只能将一个数归位, 如果有n个数进行排序,只需将n-1个数归位, 也就是说要进行n-1趟操作(已经归位的数不用再比较) 缺点: 冒泡排序解决了桶排序浪费空间的问题, 但是冒泡排序的效率特别低 12345678910def bubbleSort(relist): """冒泡排序""" len_ = len(relist) for i in range(len_ - 1): # 这个循环负责设置冒泡排序进行的次数 for j in range(0,len_-i-1): if relist[j] &gt; relist[j+1]: relist[j+1], relist[j] = relist[j], relist[j+1] return relistprint(bubbleSort([1,5,2,6,9,3])) 由上图看出最大的数一直沉到底部 选择排序选择排序基本原理： 第1趟，在待排序记录r1 ~ r[n]中选出最小的记录，将它与r1交换； 第2趟，在待排序记录r2 ~ r[n]中选出最小的记录，将它与r2交换； 以此类推，第i趟在待排序记录r[i] ~ r[n]中选出最小的记录，将它与r[i]交换，使有序序列不断增长直到全部排序完毕。 12345678910111213141516171819202122232425262728293031# 方法一def selectSort(relist): len_ = len(relist) for i in range(len_): min_index = i for j in range(i+1,len_): # 这个循环会找到值比第i个索引所代表值小的索引 if relist[j] &lt; relist[min_index]: min_index = j relist[i] ,relist[min_index] = relist[min_index], relist[i] # 互换两个索引位置 return relistprint selectSort([1,5,2,6,9,3])# 方法二def findSmallest(arr): smallest = arr[0] smallest_index = 0 for i in range(1, len(arr)): if arr[i] &lt; smallest: smallest = arr[i] smallest_index = i return smallest_indexdef selectionSort(arr): newArr = [] for i in range(len(arr)): smallest = findSmallest(arr) newArr.append(arr.pop(smallest)) return newArrprint(selectionSort([5,3,7,2,1,8])) 归并排序所谓归并是指将若干个已排好序的部分合并成一个有序的部分。 假设我们有一个没有排好序的序列(14,12,15,13,11,16)，那么首先我们使用分割的办法将这个序列分割成一个个已经排好序的子序列。然后再利用归并的方法将一个个的子序列合并成排序好的序列。分割和归并的过程可以看下面的图例。这样通过先递归的分解数列，再合并数列就完成了归并排序。 123456789101112131415161718192021import mathdef merge(left, right): result = [] while left and right: result.append(left.pop(0) if left[0] &lt;= right[0] else right.pop(0)) while left: result.append(left.pop(0)) while right: result.append(right.pop(0)) return resultdef mergeSort(relist): if len(relist) &lt;= 1: return relist mid_index = math.floor(len(relist)/2) left = mergeSort(relist[:mid_index]) # 递归拆解的过程 right = mergeSort(relist[mid_index:]) return merge(left, right) # 合并的过程print(mergeSort([1,5,2,6,9,3])) 堆排序 创建最大堆:将堆所有数据重新排序，使其成为最大堆 最大堆调整:作用是保持最大堆的性质，是创建最大堆的核心子程序 堆排序:移除位在第一个数据的根节点，并做最大堆调整的递归运算 12345678910111213141516171819202122232425262728293031323334353637# code from -http://blog.csdn.net/minxihou/article/details/51850001import randomdef MAX_Heapify(heap,HeapSize,root):#在堆中做结构调整使得父节点的值大于子节点 left = 2*root + 1 right = left + 1 larger = root if left &lt; HeapSize and heap[larger] &lt; heap[left]: larger = left if right &lt; HeapSize and heap[larger] &lt; heap[right]: larger = right if larger != root:#如果做了堆调整则larger的值等于左节点或者右节点的，这个时候做对调值操作 heap[larger],heap[root] = heap[root],heap[larger] MAX_Heapify(heap, HeapSize, larger)def Build_MAX_Heap(heap):#构造一个堆，将堆中所有数据重新排序 HeapSize = len(heap)#将堆的长度当独拿出来方便 for i in range((HeapSize -2)//2,-1,-1):#从后往前出数 MAX_Heapify(heap,HeapSize,i)def HeapSort(heap):#将根节点取出与最后一位做对调，对前面len-1个节点继续进行对调整过程。 Build_MAX_Heap(heap) for i in range(len(heap)-1,-1,-1): heap[0],heap[i] = heap[i],heap[0] MAX_Heapify(heap, i, 0) return heapif __name__ == '__main__': a = [30,50,57,77,62,78,94,80,84] print(a) HeapSort(a) print(a) b = [random.randint(1,1000) for i in range(1000)] #print b HeapSort(b) print(b) 复杂度 排序法 最差时间 平均时间复杂度 稳定度 空间复杂度 冒泡排序 O(n^2) O(n^2) 稳定 O(1) 快速排序 O(n^2) O(n*log n) 不稳定 O(log2n)~O(n) 选择排序 O(n^2) O(n^2) 不稳定 O(1) 二叉树排序 O(n^2) O(n*log n) 不一定 O(n) 插入排序 O(n^2) O(n^2) 稳定 O(1) 堆排序 O(n*log n) O(n*log n) 不稳定 O(1) 希尔排序 O O 不稳定 O(1) 归并排序 O(n*log n) O(n*log n) 稳定 ? 参考 http://blog.csdn.net/mrlevo520/article/details/77829204 http://blog.csdn.net/minxihou/article/details/51850001]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Algorithm</tag>
        <tag>Data Structures</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 实现二分查找]]></title>
    <url>%2F2018%2F03%2F09%2FPython3-%E5%AE%9E%E7%8E%B0%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE%2F</url>
    <content type="text"><![CDATA[二分查找,给出一个已经排好序的列表,注意是已经排好序的,查找指定元素在列表中的位置123456789101112131415161718192021import mathdef binary_search(my_list, item): """从list中查找item""" low = 0 high = len(my_list) - 1 while low &lt;= high: mid = math.floor((low + high)/2) guess = my_list[mid] if guess &gt; item: high = mid - 1 elif guess &lt; item: low = mid + 1 else: return mid return Nonemy_list = [1,3,5,7,9]item = 1print(binary_search(my_list, item))]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 向上取整ceil|向下取整floor|四舍五入round]]></title>
    <url>%2F2018%2F03%2F09%2FPython3-%E5%90%91%E4%B8%8A%E5%8F%96%E6%95%B4ceil-%E5%90%91%E4%B8%8B%E5%8F%96%E6%95%B4floor-%E5%9B%9B%E8%88%8D%E4%BA%94%E5%85%A5round%2F</url>
    <content type="text"><![CDATA[12345678910111213141516import math#向上取整print("math.ceil---向上取整") print("math.ceil(2.3) =&gt; ", math.ceil(2.3)) # 3print("math.ceil(2.6) =&gt; ", math.ceil(2.6)) # 3#向下取整print("\nmath.floor---向下取整")print("math.floor(2.3) =&gt; ", math.floor(2.3)) # 2print("math.floor(2.6) =&gt; ", math.floor(2.6)) # 2#四舍五入print("\nround---四舍五入")print("round(2.3) =&gt; ", round(2.3)) # 2print("round(2.6) =&gt; ", round(2.6)) # 3 1234567891011math.ceil---向上取整math.ceil(2.3) =&gt; 3math.ceil(2.6) =&gt; 3math.floor---向下取整math.floor(2.3) =&gt; 2math.floor(2.6) =&gt; 2round---四舍五入round(2.3) =&gt; 2round(2.6) =&gt; 3]]></content>
      <categories>
        <category>Python3 进阶</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache和Nginx的对比]]></title>
    <url>%2F2018%2F03%2F08%2FApache%E5%92%8CNginx%E7%9A%84%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[Apache apache 的 rewrite 比 nginx 强大，在 rewrite 频繁的情况下，用 apache apache 发展到现在，模块超多，基本想到的都可以找到 apache 更为成熟，少 bug ，nginx 的 bug 相对较多 apache 超稳定 apache 对 PHP 支持比较简单，nginx 需要配合其他后端用 apache 在处理动态请求有优势，nginx 在这方面是鸡肋，一般动态请求要 apache 去做，nginx 适合静态和反向。 apache 仍然是目前的主流，拥有丰富的特性，成熟的技术和开发社区 Nginx 轻量级，采用 C 进行编写，同样的 web 服务，会占用更少的内存及资源 抗并发，nginx 以 epoll and kqueue 作为开发模型，处理请求是异步非阻塞的，负载能力比 apache 高很多，而 apache 则是阻塞型的。在高并发下 nginx 能保持低资源低消耗高性能 ，而 apache 在 PHP 处理慢或者前端压力很大的情况下，很容易出现进程数飙升，从而拒绝服务的现象。 nginx 处理静态文件好，静态处理性能比 apache 高三倍以上 nginx 的设计高度模块化，编写模块相对简单 nginx 配置简洁，正则配置让很多事情变得简单，而且改完配置能使用 -t 测试配置有没有问题，apache 配置复杂 ，重启的时候发现配置出错了，会很崩溃 nginx 作为负载均衡服务器，支持 7 层负载均衡 nginx 本身就是一个反向代理服务器，而且可以作为非常优秀的邮件代理服务器 启动特别容易, 并且几乎可以做到 7*24 不间断运行，即使运行数个月也不需要重新启动，还能够不间断服务的情况下进行软件版本的升级 社区活跃，各种高性能模块出品迅速 总结nginx 相对 apache 的优点： 轻量级，同样起web 服务，比apache 占用更少的内存及资源 抗并发，nginx 处理请求是异步非阻塞的，支持更多的并发连接，而apache 则是阻塞型的，在高并发下nginx 能保持低资源低消耗高性能 配置简洁 高度模块化的设计，编写模块相对简单 社区活跃 apache 相对nginx 的优点： rewrite ，比nginx 的rewrite 强大 模块超多，基本想到的都可以找到 少bug ，nginx 的bug 相对较多 超稳定 两者最核心的区别在于 apache 是同步多进程模型，一个连接对应一个进程，而 nginx 是异步的，多个连接（万级别）可以对应一个进程 一般来说，需要性能的 web 服务，用 nginx 。如果不需要性能只求稳定，更考虑 apache ，后者的各种功能模块实现得比前者，例如 ssl 的模块就比前者好，可配置项多。epoll(freebsd 上是 kqueue ) 网络 IO 模型是 nginx 处理性能高的根本理由，但并不是所有的情况下都是 epoll 大获全胜的，如果本身提供静态服务的就只有寥寥几个文件，apache 的 select 模型或许比 epoll 更高性能。当然，这只是根据网络 IO 模型的原理作的一个假设，真正的应用还是需要实测了再说的。 更为通用的方案是，前端 nginx 抗并发，后端 apache 集群，配合起来会更好。]]></content>
      <tags>
        <tag>Web服务器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用户密码保存的方式有哪些]]></title>
    <url>%2F2018%2F03%2F08%2F%E7%94%A8%E6%88%B7%E5%AF%86%E7%A0%81%E4%BF%9D%E5%AD%98%E7%9A%84%E6%96%B9%E5%BC%8F%E6%9C%89%E5%93%AA%E4%BA%9B%2F</url>
    <content type="text"><![CDATA[明文保存 明文hash后保存,如md5 MD5+Salt方式,这个salt可以随机 知乎使用了Bcrypy(好像)加密]]></content>
      <tags>
        <tag>加密</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程间的通信方式(IPC)]]></title>
    <url>%2F2018%2F03%2F07%2F%E8%BF%9B%E7%A8%8B%E9%97%B4%E7%9A%84%E9%80%9A%E4%BF%A1%E6%96%B9%E5%BC%8F-IPC%2F</url>
    <content type="text"><![CDATA[管道（Pipe）：管道可用于具有亲缘关系进程间的通信，允许一个进程和另一个与它有共同祖先的进程之间进行通信。 命名管道（named pipe）：命名管道克服了管道没有名字的限制，因此，除具有管道所具有的功能外，它还允许无亲缘关系进程间的通信。命名管道在文件系统中有对应的文件名。命名管道通过命令mkfifo或系统调用mkfifo来创建。 信号（Signal）：信号是比较复杂的通信方式，用于通知接受进程有某种事件发生，除了用于进程间通信外，进程还可以发送信号给进程本身；linux除了支持Unix早期信号语义函数sigal外，还支持语义符合Posix.1标准的信号函数sigaction（实际上，该函数是基于BSD的，BSD为了实现可靠信号机制，又能够统一对外接口，用sigaction函数重新实现了signal函数）。 消息（Message）队列：消息队列是消息的链接表，包括Posix消息队列system V消息队列。有足够权限的进程可以向队列中添加消息，被赋予读权限的进程则可以读走队列中的消息。消息队列克服了信号承载信息量少，管道只能承载无格式字节流以及缓冲区大小受限等缺 共享内存：使得多个进程可以访问同一块内存空间，是最快的可用IPC形式。是针对其他通信机制运行效率较低而设计的。往往与其它通信机制，如信号量结合使用，来达到进程间的同步及互斥。 内存映射（mapped memory）：内存映射允许任何多个进程间通信，每一个使用该机制的进程通过把一个共享的文件映射到自己的进程地址空间来实现它。 信号量（semaphore）：主要作为进程间以及同一进程不同线程之间的同步手段。 套接口（Socket）：更为一般的进程间通信机制，可用于不同机器之间的进程间通信。起初是由Unix系统的BSD分支开发出来的，但现在一般可以移植到其它类Unix系统上：Linux和System V的变种都支持套接字。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django运行方式以及处理流程简介(转载)]]></title>
    <url>%2F2018%2F03%2F07%2FDjango%E8%BF%90%E8%A1%8C%E6%96%B9%E5%BC%8F%E4%BB%A5%E5%8F%8A%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B%E7%AE%80%E4%BB%8B-%E8%BD%AC%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[Django的运行方式运行Django项目的方法很多，这里主要介绍一下常用的方法。一种是在开发和调试中经常用到runserver方法，使用Django自己的web server(WSGI Server)；另外一种就是使用fastcgi，uWSGIt等协议运行Django项目。 runserver方法runserver方法是调试Django时经常用到的运行方式，它使用Django自带的WSGI Server运行，主要在测试和开发中使用，使用方法如下：1234Usage: manage.py runserver [options] [optional port number, or ipaddr:port]# python manager.py runserver # default port is 8000# python manager.py runserver 8080# python manager.py runserver 127.0.0.1:9090 看一下manager.py的源码，你会发现上面的命令其实是通过Django的execute_from_command_line方法执行了内部实现的runserver命令，那么现在看一下runserver具体做了什么。。 看了源码之后，可以发现runserver命令主要做了两件事情： 1). 解析参数，并通过django.core.servers.basehttp.get_internal_wsgi_application方法获取wsgi handler; 2). 根据ip_address和port生成一个WSGIServer对象，接受用户请求 get_internal_wsgi_application的源码如下：123456789101112131415161718192021222324def get_internal_wsgi_application(): """ Loads and returns the WSGI application as configured by the user in ``settings.WSGI_APPLICATION``. With the default ``startproject`` layout, this will be the ``application`` object in ``projectname/wsgi.py``. This function, and the ``WSGI_APPLICATION`` setting itself, are only useful for Django's internal servers (runserver, runfcgi); external WSGI servers should just be configured to point to the correct application object directly. If settings.WSGI_APPLICATION is not set (is ``None``), we just return whatever ``django.core.wsgi.get_wsgi_application`` returns. """ from django.conf import settings app_path = getattr(settings, 'WSGI_APPLICATION') if app_path is None: return get_wsgi_application() return import_by_path( app_path, error_prefix="WSGI application '%s' could not be loaded; " % app_path ) 通过上面的代码我们可以知道，Django会先根据settings中的WSGI_APPLICATION来获取handler；在创建project的时候，Django会默认创建一个wsgi.py文件，而settings中的WSGI_APPLICATION配置也会默认指向这个文件。看一下这个wsgi.py文件，其实它也和上面的逻辑一样，最终调用get_wsgi_application实现。 uWSGI方法uWSGI + Nginx的方法是现在最常见的在生产环境中运行Django的方法，要了解这种方法，首先要了解一下WSGI和uWSGI协议。 WSGI，全称Web Server Gateway Interface，或者Python Web Server Gateway Interface，是为Python语言定义的Web服务器和Web应用程序或框架之间的一种简单而通用的接口，基于现存的CGI标准而设计的。WSGI其实就是一个网关(Gateway)，其作用就是在协议之间进行转换。(PS: 这里只对WSGI做简单介绍，想要了解更多的内容可自行搜索) uWSGI是一个Web服务器，它实现了WSGI协议、uwsgi、http等协议。注意!uwsgi是一种通信协议，而uWSGI是实现uwsgi协议和WSGI协议的Web服务器。uWSGI具有超快的性能、低内存占用和多app管理等优点。以我的博客为例，uWSGI的xml配置如下： 1234567891011121314151617181920&lt;uwsgi&gt; &lt;!-- 端口 --&gt; &lt;socket&gt;:7600&lt;/socket&gt; &lt;stats&gt;:40000&lt;/stats&gt; &lt;!-- 系统环境变量 --&gt; &lt;env&gt;DJANGO_SETTINGS_MODULE=geek_blog.settings&lt;/env&gt; &lt;!-- 指定的python WSGI模块 --&gt; &lt;module&gt;django.core.handlers.wsgi:WSGIHandler()&lt;/module&gt; &lt;processes&gt;6&lt;/processes&gt; &lt;master /&gt; &lt;master-as-root /&gt; &lt;!-- 超时设置 --&gt; &lt;harakiri&gt;60&lt;/harakiri&gt; &lt;harakiri-verbose/&gt; &lt;daemonize&gt;/var/app/log/blog/uwsgi.log&lt;/daemonize&gt; &lt;!-- socket的监听队列大小 --&gt; &lt;listen&gt;32768&lt;/listen&gt; &lt;!-- 内部超时时间 --&gt; &lt;socket-timeout&gt;60&lt;/socket-timeout&gt;&lt;/uwsgi&gt; uWSGI和Nginx一起使用的配置方法就不在这里说明了，网上教程很多，需要的可以自行搜索。 HTTP请求处理流程Django和其他Web框架一样，HTTP的处理流程基本类似：接受request，返回response内容。Django的具体处理流程大致如下图所示： 1. 加载project settings 在通过django-admin.py创建project的时候，Django会自动生成默认的settings文件和manager.py等文件，在创建WSGIServer之前会执行下面的引用：1from django.conf import settings 上面引用在执行时，会读取os.environ中的DJANGO_SETTINGS_MODULE配置，加载项目配置文件，生成settings对象。所以，在manager.py文件中你可以看到，在获取WSGIServer之前，会先将project的settings路径加到os路径中。 2. 创建WSGIServer 不管是使用runserver还是uWSGI运行Django项目，在启动时都会调用django.core.servers.basehttp中的run()方法，创建一个django.core.servers.basehttp.WSGIServer类的实例，之后调用其serve_forever()方法启动HTTP服务。run方法的源码如下：12345678910def run(addr, port, wsgi_handler, ipv6=False, threading=False): server_address = (addr, port) if threading: httpd_cls = type(str('WSGIServer'), (socketserver.ThreadingMixIn, WSGIServer), &#123;&#125;) else: httpd_cls = WSGIServer httpd = httpd_cls(server_address, WSGIRequestHandler, ipv6=ipv6) # Sets the callable application as the WSGI application that will receive requests httpd.set_app(wsgi_handler) httpd.serve_forever() 如上，我们可以看到：在创建WSGIServer实例的时候会指定HTTP请求的Handler，上述代码使用WSGIRequestHandler。当用户的HTTP请求到达服务器时，WSGIServer会创建WSGIRequestHandler实例，使用其handler方法来处理HTTP请求(其实最终是调用wsgiref.handlers.BaseHandler中的run方法处理)。WSGIServer通过set_app方法设置一个可调用(callable)的对象作为application，上面提到的handler方法最终会调用设置的application处理request，并返回response。 其中，WSGIServer继承自wsgiref.simple_server.WSGIServer，而WSGIRequestHandler继承自wsgiref.simple_server.WSGIRequestHandler，wsgiref是Python标准库给出的WSGI的参考实现。其源码可自行到wsgiref参看，这里不再细说. 3. 处理Request 第二步中说到的application，在Django中一般是django.core.handlers.wsgi.WSGIHandler对象，WSGIHandler继承自django.core.handlers.base.BaseHandler，这个是Django处理request的核心逻辑，它会创建一个WSGIRequest实例，而WSGIRequest是从http.HttpRequest继承而来 4. 返回Response 上面提到的BaseHandler中有个get_response方法，该方法会先加载Django项目的ROOT_URLCONF，然后根据url规则找到对应的view方法(类)，view逻辑会根据request实例生成并返回具体的response。 在Django返回结果之后，第二步中提到wsgiref.handlers.BaseHandler.run方法会调用finish_response结束请求，并将内容返回给用户。 Django处理Request的详细流程上述的第三步和第四步逻辑只是大致说了一下处理过程，Django在处理request的时候其实做了很多事情，下面我们详细的过一下。首先给大家分享两个网上看到的Django流程图： 上面的两张流程图可以大致描述Django处理request的流程，按照流程图2的标注，可以分为以下几个步骤： 1. 用户通过浏览器请求一个页面 2. 请求到达Request Middlewares，中间件对request做一些预处理或者直接response请求 3. URLConf通过urls.py文件和请求的URL找到相应的View 4. View Middlewares被访问，它同样可以对request做一些处理或者直接返回response 5. 调用View中的函数 6. View中的方法可以选择性的通过Models访问底层的数据 7. 所有的Model-to-DB的交互都是通过manager完成的 8. 如果需要，Views可以使用一个特殊的Context 9. Context被传给Template用来生成页面 a. Template使用Filters和Tags去渲染输出 b. 输出被返回到View c. HTTPResponse被发送到Response Middlewares d. 任何Response Middlewares都可以丰富response或者返回一个完全不同的response e. Response返回到浏览器，呈现给用户 上述流程中最主要的几个部分分别是：Middleware(中间件，包括request, view, exception, response)，URLConf(url映射关系)，Template(模板系统)，下面一一介绍一下。 1. Middleware(中间件) Middleware并不是Django所独有的东西，在其他的Web框架中也有这种概念。在Django中，Middleware可以渗入处理流程的四个阶段：request，view，response和exception，相应的，在每个Middleware类中都有rocess_request，process_view， process_response 和 process_exception这四个方法。你可以定义其中任意一个或多个方法，这取决于你希望该Middleware作用于哪个处理阶段。每个方法都可以直接返回response对象。 Middleware是在Django BaseHandler的load_middleware方法执行时加载的，加载之后会建立四个列表作为处理器的实例变量：1234567_request_middleware：process_request方法的列表_view_middleware：process_view方法的列表_response_middleware：process_response方法的列表_exception_middleware：process_exception方法的列表 Django的中间件是在其配置文件(settings.py)的MIDDLEWARE_CLASSES元组中定义的。在MIDDLEWARE_CLASSES中，中间件组件用字符串表示：指向中间件类名的完整Python路径。例如GeekBlog项目的配置： 1234567891011MIDDLEWARE_CLASSES = ( 'django.middleware.cache.UpdateCacheMiddleware', 'django.middleware.common.CommonMiddleware', 'django.middleware.cache.FetchFromCacheMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.locale.LocaleMiddleware', 'geek_blog.middlewares.MobileDetectionMiddleware', # 自定义的Middleware) Django项目的安装并不强制要求任何中间件，如果你愿意，MIDDLEWARE_CLASSES可以为空。中间件出现的顺序非常重要：在request和view的处理阶段，Django按照MIDDLEWARE_CLASSES中出现的顺序来应用中间件，而在response和exception异常处理阶段，Django则按逆序来调用它们。也就是说，Django将MIDDLEWARE_CLASSES视为view函数外层的顺序包装子：在request阶段按顺序从上到下穿过，而在response则反过来。 以下两张图可以更好地帮助你理解： 2. URLConf(URL映射) 如果处理request的中间件都没有直接返回response，那么Django会去解析用户请求的URL。URLconf就是Django所支撑网站的目录。它的本质是URL模式以及要为该URL模式调用的视图函数之间的映射表。通过这种方式可以告诉Django，对于这个URL调用这段代码，对于那个URL调用那段代码。具体的，在Django项目的配置文件中有ROOT_URLCONF常量，这个常量加上根目录”/“，作为参数来创建django.core.urlresolvers.RegexURLResolver的实例，然后通过它的resolve方法解析用户请求的URL，找到第一个匹配的view。 其他有关URLConf的内容，这里不再具体介绍，大家可以看DjangoBook了解。 3. Template(模板) 大部分web框架都有自己的Template(模板)系统，Django也是。但是，Django模板不同于Mako模板和jinja2模板，在Django模板不能直接写Python代码，只能通过额外的定义filter和template tag实现。由于本文主要介绍Django流程，模板内容就不过多介绍。 参考文章： uWSGI Web服务器介绍 wsgiref源码分析 用Python写一个简单的Web框架 Django 结构及处理流程分析 Django运行方式及处理流程总结 PS: 以上代码和内容都是基于Django 1.6.5版本，其他版本可能与其不同，请参考阅读。]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三种多路复用IO实现方式：select，poll，epoll]]></title>
    <url>%2F2018%2F03%2F07%2F%E4%B8%89%E7%A7%8D%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8IO%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F%EF%BC%9Aselect%EF%BC%8Cpoll%EF%BC%8Cepoll%2F</url>
    <content type="text"><![CDATA[select，poll，epoll都是IO多路复用的机制。I/O多路复用就通过一种机制，可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。 在linux 没有实现epoll事件驱动机制之前，我们一般选择用select或者poll等IO多路复用的方法来实现并发服务程序。在大数据、高并发、集群等一些名词唱得火热之年代，select和poll的用武之地越来越有限，风头已经被epoll占尽。 简单来讲select有3个缺点: 连接数受限 查找配对速度慢 数据由内核拷贝到用户态 poll改善了第一个缺点epoll改了三个缺点. 使用场景IO多路复用是指内核一旦发现进程指定的一个或者多个IO条件准备读取，它就通知该进程。IO多路复用适用如下场合： 当客户处理多个描述符时（一般是交互式输入和网络套接口），必须使用I/O复用。 当一个客户同时处理多个套接口时，这种情况是可能的，但很少出现。 如果一个TCP服务器既要处理监听套接口，又要处理已连接套接口，一般也要用到I/O复用。 如果一个服务器即要处理TCP，又要处理UDP，一般要使用I/O复用。 如果一个服务器要处理多个服务或多个协议，一般要使用I/O复用。 select、poll、epoll简介select基本原理：select 函数监视的文件描述符分3类，分别是writefds、readfds、和exceptfds。调用后select函数会阻塞，直到有描述符就绪（有数据 可读、可写、或者有except），或者超时（timeout指定等待时间，如果立即返回设为null即可），函数返回。当select函数返回后，可以通过遍历fdset，来找到就绪的描述符。 基本流程，如图所示： select目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点。select的一个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在Linux上一般为1024，可以通过修改宏定义甚至重新编译内核的方式提升这一限制，但是这样也会造成效率的降低。 select本质上是通过设置或者检查存放fd标志位的数据结构来进行下一步处理。这样所带来的缺点是： 1、select最大的缺陷就是单个进程所打开的FD是有一定限制的，它由FD_SETSIZE设置，默认值是1024。 一般来说这个数目和系统内存关系很大，具体数目可以cat /proc/sys/fs/file-max察看。32位机默认是1024个。64位机默认是2048. 2、对socket进行扫描时是线性扫描，即采用轮询的方法，效率较低。 当套接字比较多的时候，每次select()都要通过遍历FD_SETSIZE个Socket来完成调度，不管哪个Socket是活跃的，都遍历一遍。这会浪费很多CPU时间。如果能给套接字注册某个回调函数，当他们活跃时，自动完成相关操作，那就避免了轮询，这正是epoll与kqueue做的。 3、需要维护一个用来存放大量fd的数据结构，这样会使得用户空间和内核空间在传递该结构时复制开销大。 poll基本原理：poll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态，如果设备就绪则在设备等待队列中加入一项并继续遍历，如果遍历完所有fd后没有发现就绪设备，则挂起当前进程，直到设备就绪或者主动超时，被唤醒后它又要再次遍历fd。这个过程经历了多次无谓的遍历。 它没有最大连接数的限制，原因是它是基于链表来存储的，但是同样有一个缺点： 1）大量的fd的数组被整体复制于用户态和内核地址空间之间，而不管这样的复制是不是有意义。 2）poll还有一个特点是“水平触发”，如果报告了fd后，没有被处理，那么下次poll时会再次报告该fd。 注意：从上面看，select和poll都需要在返回后，通过遍历文件描述符来获取已经就绪的socket。事实上，同时连接的大量客户端在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降。 epoll epoll是在2.6内核中提出的，是之前的select和poll的增强版本。相对于select和poll来说，epoll更加灵活，没有描述符限制。epoll使用一个文件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次。 基本原理：epoll支持水平触发和边缘触发，最大的特点在于边缘触发，它只告诉进程哪些fd刚刚变为就绪态，并且只会通知一次。还有一个特点是，epoll使用“事件”的就绪通知方式，通过epoll_ctl注册fd，一旦该fd就绪，内核就会采用类似callback的回调机制来激活该fd，epoll_wait便可以收到通知。 epoll的优点： 1、没有最大并发连接的限制，能打开的FD的上限远大于1024（1G的内存上能监听约10万个端口）。 2、效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。 只有活跃可用的FD才会调用callback函数；即Epoll最大的优点就在于它只管你“活跃”的连接，而跟连接总数无关，因此在实际的网络环境中，Epoll的效率就会远远高于select和poll。 3、内存拷贝，利用mmap()文件映射内存加速与内核空间的消息传递；即epoll使用mmap减少复制开销。 epoll对文件描述符的操作有两种模式：LT（level trigger）和ET（edge trigger）。LT模式是默认模式，LT模式与ET模式的区别如下： LT模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用epoll_wait时，会再次响应应用程序并通知此事件。 ET模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次响应应用程序并通知此事件。 select、poll、epoll对比1、支持一个进程所能打开的最大连接数 2、FD剧增后带来的IO效率问题 3、消息传递方式 综上，在选择select，poll，epoll时要根据具体的使用场合以及这三种方式的自身特点： 1、表面上看epoll的性能最好，但是在连接数少并且连接都十分活跃的情况下，select和poll的性能可能比epoll好，毕竟epoll的通知机制需要很多函数回调。 2、select低效是因为每次它都需要轮询。但低效也是相对的，视情况而定，也可通过良好的设计改善。 参考 https://www.cnblogs.com/jeakeven/p/5435916.html http://blog.csdn.net/davidsguo008/article/details/73556811 https://www.cnblogs.com/wangyufu/p/6593515.html]]></content>
      <categories>
        <category>操作系统</category>
        <category>多任务处理</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈TCP和UDP的区别以及应用]]></title>
    <url>%2F2018%2F03%2F07%2F%E6%B5%85%E8%B0%88TCP%E5%92%8CUDP%E7%9A%84%E5%8C%BA%E5%88%AB%E4%BB%A5%E5%8F%8A%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[TCP和UDP区别TCP(Transmission Control Protocol)—传输控制协议,提供的是面向连接、可靠的字节流服务。当客户和服务器彼此交换数据前，必须先在双方之间建立一个TCP连接，之后才能传输数据。TCP提供超时重发，丢弃重复数据，检验数据，流量控制等功能，保证数据能从一端传到另一端。UDP(User Data Protocol)—用户数据报协议，是一个简单的面向数据报的运输层协议。在网络中它与TCP协议一样用于处理数据包，是一种无连接的协议。UDP不提供可靠性，它只是把应用程序传给IP层的数据报发送出去，但是并不能保证它们能到达目的地。由于UDP在传输数据报前不用在客户和服务器之间建立一个连接，且没有超时重发等机制，故而传输速度很快。 应用 HTTP协议在运输层采用的就是TCP协议，在浏览器中输入IP地址后，与服务器建立连接，采用的就是TCP协议，是一种面向连接、可靠的字节流服务。 当强调传输性能而不是传输的完整性时，如：音频、多媒体应用和视频会议时，UDP是最好的选择。另外，腾讯QQ采用也是UDP协议。]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈Session和Cookie的区别与联系]]></title>
    <url>%2F2018%2F03%2F07%2F%E6%B5%85%E8%B0%88Session%E5%92%8CCookie%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E8%81%94%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[Cookie Session 储存位置 客户端 服务器端 目的 跟踪会话，也可以保存用户偏好设置或者保存用户名密码等 跟踪会话 安全性 不安全 安全 session技术是要使用到cookie的，之所以出现session技术，主要是为了安全。 Session的概念Session 是存放在服务器端的，类似于Session结构来存放用户数据，当浏览器 第一次发送请求时，服务器自动生成了一个Session和一个Session ID用来唯一标识这个Session，并将其通过响应发送到浏览器。当浏览器第二次发送请求，会将前一次服务器响应中的Session ID放在请求中一并发送到服务器上，服务器从请求中提取出Session ID，并和保存的所有Session ID进行对比，找到这个用户对应的Session。 一般情况下，服务器会在一定时间内（默认30分钟）保存这个 Session，过了时间限制，就会销毁这个Session。在销毁之前，程序员可以将用户的一些数据以Key和Value的形式暂时存放在这个 Session中。当然，也有使用数据库将这个Session序列化后保存起来的，这样的好处是没了时间的限制，坏处是随着时间的增加，这个数据 库会急速膨胀，特别是访问量增加的时候。一般还是采取前一种方式，以减轻服务器压力。 cookie的概念Cookie意为“甜饼”，是由W3C组织提出，最早由Netscape社区发展的一种机制。目前Cookie已经成为标准，所有的主流浏览器如IE、Netscape、Firefox、Opera等都支持Cookie。 由于HTTP是一种无状态的协议，服务器单从网络连接上无从知道客户身份。怎么办呢？就给客户端们颁发一个通行证吧，每人一个，无论谁访问都必须携带自己通行证。这样服务器就能从通行证上确认客户身份了。这就是Cookie的工作原理。 Cookie实际上是一小段的文本信息。客户端请求服务器，如果服务器需要记录该用户状态，就使用response向客户端浏览器颁发一个Cookie。客户端浏览器会把Cookie保存起来。当浏览器再请求该网站时，浏览器把请求的网址连同该Cookie一同提交给服务器。服务器检查该Cookie，以此来辨认用户状态。服务器还可以根据需要修改Cookie的内容。 Session的客户端实现形式（即Session ID的保存方法）一般浏览器提供了两种方式来保存，还有一种是程序员使用html隐藏域的方式自定义实现： 使用Cookie来保存，这是最常见的方法，本文“记住我的登录状态”功能的实现正式基于这种方式的。服务器通过设置Cookie的方式将Session ID发送到浏览器。如果我们不设置这个过期时间，那么这个Cookie将不存放在硬盘上，当浏览器关闭的时候，Cookie就消失了，这个Session ID就丢失了。如果我们设置这个时间为若干天之后，那么这个Cookie会保存在客户端硬盘中，即使浏览器关闭，这个值仍然存在，下次访问相应网站时，同 样会发送到服务器上。 使用URL附加信息的方式，也就是像我们经常看到JSP网站会有aaa.jsp?JSESSIONID=*一样的。这种方式和第一种方式里面不设置Cookie过期时间是一样的。 第三种方式是在页面表单里面增加隐藏域，这种方式实际上和第二种方式一样，只不过前者通过GET方式发送数据，后者使用POST方式发送数据。但是明显后者比较麻烦。 cookie与session的区别cookie数据保存在客户端，session数据保存在服务器端。 简 单的说，当你登录一个网站的时候，如果web服务器端使用的是session,那么所有的数据都保存在服务器上面，客户端每次请求服务器的时候会发送 当前会话的sessionid，服务器根据当前sessionid判断相应的用户数据标志，以确定用户是否登录，或具有某种权限。由于数据是存储在服务器 上面，所以你不能伪造，但是如果你能够获取某个登录用户的sessionid，用特殊的浏览器伪造该用户的请求也是能够成功的。sessionid是服务 器和客户端链接时候随机分配的，一般来说是不会有重复，但如果有大量的并发请求，也不是没有重复的可能性，我曾经就遇到过一次。登录某个网站，开始显示的 是自己的信息，等一段时间超时了，一刷新，居然显示了别人的信息。 如果浏览器使用的是 cookie，那么所有的数据都保存在浏览器端，比如你登录以后，服务器设置了 cookie用户名(username),那么，当你再次请求服务器的时候，浏览器会将username一块发送给服务器，这些变量有一定的特殊标记。服 务器会解释为 cookie变量。所以只要不关闭浏览器，那么 cookie变量便一直是有效的，所以能够保证长时间不掉线。如果你能够截获某个用户的 cookie变量，然后伪造一个数据包发送过去，那么服务器还是认为你是合法的。所以，使用 cookie被攻击的可能性比较大。如果设置了的有效时间，那么它会将 cookie保存在客户端的硬盘上，下次再访问该网站的时候，浏览器先检查有没有 cookie，如果有的话，就读取该 cookie，然后发送给服务器。如果你在机器上面保存了某个论坛 cookie，有效期是一年，如果有人入侵你的机器，将你的 cookie拷走，然后放在他的浏览器的目录下面，那么他登录该网站的时候就是用你的的身份登录的。所以 cookie是可以伪造的。当然，伪造的时候需要主意，直接copy cookie文件到 cookie目录，浏览器是不认的，他有一个index.dat文件，存储了 cookie文件的建立时间，以及是否有修改，所以你必须先要有该网站的 cookie文件，并且要从保证时间上骗过浏览器，曾经在学校的vbb论坛上面做过试验，copy别人的 cookie登录，冒用了别人的名义发帖子，完全没有问题。 Session是由应用服务器维持的一个服务器端的存储空间，用户在连接服务器时，会由服务器生成一个唯一的SessionID,用该SessionID 为标识符来存取服务器端的Session存储空间。而SessionID这一数据则是保存到客户端，用Cookie保存的，用户提交页面时，会将这一 SessionID提交到服务器端，来存取Session数据。这一过程，是不用开发人员干预的。所以一旦客户端禁用Cookie，那么Session也会失效。 服务器也可以通过URL重写的方式来传递SessionID的值，因此不是完全依赖Cookie。如果客户端Cookie禁用，则服务器可以自动通过重写URL的方式来保存Session的值，并且这个过程对程序员透明。 可以试一下，即使不写Cookie，在使用request.getCookies();取出的Cookie数组的长度也是1，而这个Cookie的名字就是JSESSIONID，还有一个很长的二进制的字符串，是SessionID的值。 Session与Cookie的应用场景Cookies是属于Session对象的一种。但有不同，Cookies不会占服务器资源，是存在客服端内存或者一个cookie的文本文件中；而“Session”则会占用服务器资源。所以，尽量不要使用Session，而使用Cookies。但是我们一般认为cookie是不可靠的，session是可靠地，但是目前很多著名的站点也都以来cookie。有时候为了解决禁用cookie后的页面处理，通常采用url重写技术，调用session中大量有用的方法从session中获取数据后置入页面。 Cookies与Session的应用场景：Cookies的安全性能一直是倍受争议的。虽然Cookies是保存在本机上的，但是其信息的完全可见性且易于本地编辑性，往往可以引起很多的安全问题。所以Cookies到底该不该用，到底该怎样用，就有了一个需要给定的底线。 先来看看，网站的敏感数据有哪些。 登陆验证信息。一般采用Session(“Logon”)＝true or false的形式。用户的各种私人信息，比如姓名等，某种情况下，需要保存在Session里需要在页面间传递的内容信息，比如调查工作需要分好几步。每一步的信息都保存在Session里，最后在统一更新到数据库。 当然还会有很多，这里列举一些比较典型的假如，一个人孤僻到不想碰Session，因为他认为，如果用户万一不小心关闭了浏览器，那么之前保存的数据就全部丢失了。所以，他出于好意，决定把这些用Session的地方，都改成用Cookies来存储，这完全是可行的，且基本操作和用Session一模一样。那么，下面就针对以上的3个典型例子，做一个分析很显然，只要某个有意非法入侵者，知道该网站验证登陆信息的Session变量是什么，那么他就可以事先编辑好该Cookies，放入到Cookies目录中，这样就可以顺利通过验证了。这是不是很可怕？Cookies完全是可见的，即使程序员设定了Cookies的生存周期（比如只在用户会话有效期内有效），它也是不安全的。假设，用户忘了关浏览器 或者一个恶意者硬性把用户给打晕，那用户的损失将是巨大的。这点如上点一样，很容易被它人窃取重要的私人信息。但，其还有一个问题所在是，可能这些数据信息量太大，而使得Cookies的文件大小剧增。这可不是用户希望所看到的。 显然，Cookies并不是那么一块好啃的小甜饼。但，Cookies的存在，当然有其原因。它给予程序员更多发挥编程才能的空间。所以，使用Cookies该有个底线。这个底线一般来说，遵循以下原则。不要保存私人信息。任何重要数据，最好通过加密形式来保存数据（最简单的可以用URLEncode，当然也可以用完善的可逆加密方式，遗憾的是，最好不要用md5来加密）。是否保存登陆信息，需有用户自行选择。长于10K的数据，不要用到Cookies。也不要用Cookies来玩点让客户惊喜的小游戏。 cookie最典型的应用是： 判断用户是否登陆过网站，以便下次登录时能够直接登录。如果我们删除cookie，则每次登录必须从新填写登录的相关信息。 另一个重要的应用是“购物车”中类的处理和设计。用户可能在一段时间内在同一家网站的不同页面选择不同的商品，可以将这些信息都写入cookie，在最后付款时从cookie中提取这些信息，当然这里面有了安全和性能问题需要我们考虑了。 参考 https://www.cnblogs.com/andy-zhou/p/5360107.html#_caption_1 http://blog.csdn.net/duan1078774504/article/details/51912868]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何理解操作系统中的同步和异步、阻塞和非阻塞]]></title>
    <url>%2F2018%2F03%2F07%2F%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E5%90%8C%E6%AD%A5%E5%92%8C%E5%BC%82%E6%AD%A5%E3%80%81%E9%98%BB%E5%A1%9E%E5%92%8C%E9%9D%9E%E9%98%BB%E5%A1%9E%2F</url>
    <content type="text"><![CDATA[同步所谓同步，就是在发出一个功能调用时，在没有得到结果之前，该调用就不返回。按照这个定义，其实绝大多数函数都是同步调用（例如sin, isdigit等）。但是一般而言，我们在说同步、异步的时候，特指那些需要其他部件协作或者需要一定时间完成的任务。 同步，可以理解为在执行完一个函数或方法之后，一直等待系统返回值或消息，这时程序是出于阻塞的，只有接收到返回的值或消息后才往下执行其他的命令。 最常见的例子就是 SendMessage。该函数发送一个消息给某个窗口，在对方处理完消息之前，这个函数不返回。当对方处理完毕以后，该函数才把消息处理函数所返回的 LRESULT值返回给调用者。 异步异步的概念和同步相对。当一个异步过程调用发出后，调用者不能立刻得到结果。由另外的并行程序执行这段代码，处理完这个调用的部件在完成后，通过状态、通知和回调来通知调用者。以CAsycSocket类为例（注意，CSocket从CAsyncSocket派生，但是起功能已经由异步转化为同步），当一个客户端通过调用 Connect函数发出一个连接请求后，调用者线程立刻可以朝下运行。当连接真正建立起来以后，socket底层会发送一个消息通知该对象。 这里提到执行部件和调用者通过三种途径返回结果：状态、通知和回调。可以使用哪一种依赖于执行部件的实现，除非执行部件提供多种选择，否则不受调用者控制。如果执行部件用状态来通知，那么调用者就需要每隔一定时间检查一次，效率就很低（有些初学多线程编程的人，总喜欢用一个循环去检查某个变量的值，这其实是一种很严重的错误）。如果是使用通知的方式，效率则很高，因为执行部件几乎不需要做额外的操作。至于回调函数，其实和通知没太多区别。 异步，执行完函数或方法后，不必阻塞性地等待返回值或消息，只需要向系统委托一个异步过程，那么当系统接收到返回值或消息时，系统会自动触发委托的异步过程，从而完成一个完整的流程。 同步和异步类比同步，就是实时处理，比如服务器一接收客户端请求，马上响应，这样客户端可以在最短的时间内得到结果，但是如果多个客户端，或者一个客户端发出的请求很频繁，服务器无法同步处理，就会造成涌塞。同步如打电话，通信双方不能断（我们是同时进行，同步），你一句我一句，这样的好处是，对方想表达的信息我马上能收到，但是，我在打着电话，我无法做别的事情。 异步，就是分时处理，服务器接收到客户端请求后并不是立即处理，而是等待服务器比较空闲的时候加以处理，可以避免涌塞。异步如收发收短信，对比打电话，打电话我一定要在电话的旁边听着，保证双方都在线，而收发短信，对方不用保证此刻我一定在手机旁，同时，我也不用时刻留意手机有没有来短信。这样的话，我看着视频，然后来了短信，我就处理短信（也可以不处理），接着再看视频。 对于写程序，同步往往会阻塞，没有数据过来，我就等着，异步则不会阻塞，没数据来我干别的事，有数据来去处理这些数据。 同步在一定程度上可以看做是单线程，这个线程请求一个方法后就待这个方法给他回复，否则他不往下执行（死心眼）。异步在一定程度上可以看做是多线程的（废话，一个线程怎么叫异步），请求一个方法后，就不管了，继续执行其他的方法。 阻塞阻塞调用是指调用结果返回之前，当前线程会被挂起。函数只有在得到结果之后才会返回。 有人也许会把阻塞调用和同步调用等同起来，实际上他是不同的。对于同步调用来说，很多时候当前线程还是激活的，只是从逻辑上当前函数没有返回而已。例如，我们在CSocket中调用Receive函数，如果缓冲区中没有数据，这个函数就会一直等待，直到有数据才返回。而此时，当前线程还会继续处理各种各样的消息。如果主窗口和调用函数在同一个线程中，除非你在特殊的界面操作函数中调用，其实主界面还是应该可以刷新。 socket接收数据的另外一个函数recv则是一个阻塞调用的例子。当socket工作在阻塞模式的时候，如果没有数据的情况下调用该函数，则当前线程就会被挂起，直到有数据为止。 非阻塞非阻塞和阻塞的概念相对应，指在不能立刻得到结果之前，该函数不会阻塞当前线程，而会立刻返回。 对象的阻塞模式和阻塞函数调用对象是否处于阻塞模式和函数是不是阻塞调用有很强的相关性，但是并不是一一对应的。阻塞对象上可以有非阻塞的调用方式，我们可以通过一定的API去轮询状态，在适当的时候调用阻塞函数，就可以避免阻塞。而对于非阻塞对象，调用特殊的函数也可以进入阻塞调用。函数select就是这样的一个例子。]]></content>
      <categories>
        <category>操作系统</category>
        <category>多任务处理</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 nonlocal声明]]></title>
    <url>%2F2018%2F03%2F06%2FPython3-nonlocal%E5%A3%B0%E6%98%8E%2F</url>
    <content type="text"><![CDATA[前一节讲闭包时用到以下代码：12345678910# 示例2def make_averager(): series = [] def averager(new_value): series.append(new_value) total = sum(series) return total/len(series) return averager 以上实现make_averager函数的方法效率不高。在如下示例中,我们把所有值存储在历史数列中，然后在每次调用averager时使用sum求和。更好的实现方式是，只存储目前的总和以及元素个数，然后使用这两个值计算平均数。以下是实现方式，这种实现方式存在缺陷，只是为了引出nonlocal声明。123456789def make_averager(): count = 0 total = 0 def averager(new_value): count += 1 total += new_value return total/count return averager 尝试使用以上定义的函数，得到如下结果：123456789101112131415&gt;&gt;&gt; avg = make_averager()&gt;&gt;&gt; avg(10)---------------------------------------------------------------------------UnboundLocalError Traceback (most recent call last)&lt;ipython-input-42-ace390caaa2e&gt; in &lt;module&gt;()----&gt; 1 avg(10)&lt;ipython-input-38-371a27b41829&gt; in averager(new_value) 3 total = 0 4 def averager(new_value):----&gt; 5 count += 1 6 total += new_value 7 return total/countUnboundLocalError: local variable 'count' referenced before assignment 问题是，当count是数字或任何不可变类型时，count += 1语句的作用其实与count = count + 1一样，因此我们在averager的定义体中为count赋值了，这样会把count变量变为局部变量，而不是自由变量。total变量也会受到这样的影响。 示例2 中没有遇到这样的问题是因为我们没有给series赋值，我们只是调用series.append，并把它传给sum和len。也就是说，我们利用了列表是可变的对象这一事实。 但是对数字和字符串、元组等不可变类型来说，只能读取，不能更新。如果尝试重新绑定，例如count += 1,其实会隐式的创建局部变量count。这样count就不是自由变量了，也就不会保存在闭包中。 为了解决这个问题，python3引入了nonlocal声明。他的作用是把变量标记为自由变量，即使在函数中变量赋予新值，也会变成自由变量。如果为nonlocal声明的变量赋予新值，闭包中保存的绑定会更新。正确版的make_averager的正确实现如下：12345678910def make_averager(): count = 0 total = 0 def averager(new_value): nonlocal count, total count += 1 total += new_value return total/count return averager 这样一来，上面的错误就没有了。]]></content>
      <categories>
        <category>Python3 进阶</category>
        <category>Python3 闭包</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 闭包]]></title>
    <url>%2F2018%2F03%2F06%2FPython3-%E9%97%AD%E5%8C%85%2F</url>
    <content type="text"><![CDATA[闭包是指延伸了作用域的函数，其中包含函数定义体中引用、但是不在定义体中定义的非全局变量。闭包(closure)是函数式编程的重要的语法结构。闭包也是一种组织代码的结构，它同样提高了代码的可重复使用性。 当一个内嵌函数引用其外部作用域的变量,我们就会得到一个闭包. 总结一下,创建一个闭包必须满足以下几点: 必须有一个内嵌函数 内嵌函数必须引用外部函数中的变量 外部函数的返回值必须是内嵌函数 闭包是一种函数，它会保留定义函数时存在的自由变量的绑定，这样调用函数时虽然定义作用域不可用了，但仍能使用那些绑定。 闭包的概念难以掌握，最好通过示例理解。 假如有个名为avg的函数，他的作用是计算不断增加的系列值得平均数。起初，avg是这样使用的：123456&gt;&gt;&gt; avg(10)10.0&gt;&gt;&gt; avg(11)10.5&gt;&gt;&gt; avg(12)11.0 初学者可能会用类来实现，如示例112345678910# 示例1class Averager(object): """docstring for Average.""" def __init__(self): self.series = [] def __call__(self, new_value): self.series.append(new_value) total = sum(self.series) return total/len(self.series) Average的实例是可调用对象：1234567&gt;&gt;&gt; avg = Averager()&gt;&gt;&gt; avg(10)10.0&gt;&gt;&gt; avg(11)10.5&gt;&gt;&gt; avg(12)11.0 下面使用函数式实现，如示例2：12345678910# 示例2def make_averager(): series = [] def averager(new_value): series.append(new_value) total = sum(series) return total/len(series) return averager 调用make_averager时，返回一个averager函数对象。每次调用averager时，他会把参数添加到系列值中，然后计算当前平均值，如下所示：1234567&gt;&gt;&gt; avg = make_averager()&gt;&gt;&gt; avg(10)10.0&gt;&gt;&gt; avg(11)10.5&gt;&gt;&gt; avg(12)11.0 注意，以上两个示例有共通之处：调用Averager()或make_averager()得到一个可调用对象avg，他会更新历史值，然后计算当前均值。示例1中，avg是Averager的实例；实例2中是内部函数averager。不管怎样，我们都只需要调用avg(n),把n放入系列值中，然后重新计算均值。 Averager()类的实例avg在哪里存储历史值很明显：self.series实例属性；但是第二个示例中的avg函数在哪里寻找series呢？ 注意，series是make_averager函数的局部变量，因为那个函数的定义体中初始化了series：series = []。可是，调用avg(10)时，make_averager函数已经返回了，而他的本地作用域也一去不复返了。 在averager函数中，series是自由变量。这是一个技术术语，指未在本地作用域中绑定的变量，如图： averager的闭包延伸到那个函数的作用域之外，包含自由变量series的绑定 我们可以审查返回的averager对象，发现Python在__code__属性（表示编译后的函数定义体）中保存局部变量和自由变量的名称，如下所示 12345# 审查make_averager创建的函数&gt;&gt;&gt; avg.__code__.co_varnames('new_value', 'total')&gt;&gt;&gt; avg.__code__.co_freevars('series',) series绑定在返回的avg函数的__closure__属性中。avg.__closure__中各个元素对应于avg.__code__.co_freevars中的一个名称。这些元素是cell对象，有个cell_content属性，保存着真正的值。这些属性的值如示例所示：123456&gt;&gt;&gt; avg.__code__.co_freevars('series',)&gt;&gt;&gt; avg.__closure__(&lt;cell at 0x108b89828: list object at 0x108ae96c8&gt;,)&gt;&gt;&gt; avg.__closure__[0].cell_contents[10,11,12] 综上，闭包是一种函数，它会保留定义函数时存在的自由变量的绑定，这样调用函数时虽然定义作用域不可用了，但仍能使用那些绑定。 注意，只有嵌套在其他函数中的函数才可能需要处理不在全局作用域中的外部变量。]]></content>
      <categories>
        <category>Python3 进阶</category>
        <category>Python3 闭包</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 让字典保持有序]]></title>
    <url>%2F2018%2F03%2F06%2FPython3-%E8%AE%A9%E5%AD%97%E5%85%B8%E4%BF%9D%E6%8C%81%E6%9C%89%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[背景想创建一个字典，同时当对字典做迭代或序列化操作时，也能控制其中元素的顺序。 解决方案要控制字典中元素的顺序，可以使用collections模块中的OrderDict类。当对字典做迭代时，他会严格按照元素初始添加的顺序执行。例如：12345678910111213141516from collections import OrderedDictd = OrderedDict()d['foo'] = 1d['bar'] = 2d['spam'] = 3d['qrok'] = 4for key in d: print(key, d[key])# output:# foo 1# bar 2# spam 3# qrok 4 当想创建一个映射结构以便稍后对其做序列化或编码成另一种格式时，OrderedDict就显得特别有用。例如，如果想在进行json编码时精确控制各个字段的顺序，那么只要首先在OrderedDict中构建数据就可以了。 1234&gt;&gt;&gt; import json&gt;&gt;&gt; json.dumps(d)'&#123;"foo":1,"bar":2,"spam":3,"qrok":4&#125;'&gt;&gt;&gt; 讨论OrderedDict内部维护了一个双向链表，他会根据元素加入的顺序来排列键的位置。第一个加入的元素被放置的链表的末尾。接下来对已存在的键做重新赋值不会改变键的顺序。 注意OrderedDict的大小是普通字典的两倍多，这是由于它额外创建的链表所致。因此，如果打算构建一个涉及大量OrderedDict实例的数据结构（例如从CSV文件中读取100000行内容到OrderedDict列表中），那么需要认真对应用做需求分析，从而判断使用OrderedDict所带来的好处是否能够超越额外的内存开销带来的缺点。 a]]></content>
      <categories>
        <category>Python3 进阶</category>
        <category>Python3 字典</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python2.7.x与Python3.x差异]]></title>
    <url>%2F2018%2F03%2F06%2FPython2-7-x%E4%B8%8EPython3-x%E5%B7%AE%E5%BC%82%2F</url>
    <content type="text"><![CDATA[Contents __future__模块 print函数 整除 Unicode xrange模块 Python3中的range对象的__contains__方法 Raising exceptions Handling exceptions next()函数 and .next()方法 For循环变量和全局命名空间泄漏 比较不可排序类型 通过input()解析用户的输入 返回可迭代对象，而不是列表 更多的关于 Python 2 和 Python 3 的文章 参考： Python2.x与Python3.x差异 Key differences between Python 2.7.x and Python 3.x]]></content>
      <categories>
        <category>Python3 进阶</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 垃圾回收机制]]></title>
    <url>%2F2018%2F03%2F05%2FPython3-%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[Python中的垃圾回收GC(Garbage collection)是以引用计数为主，分代收集为辅。引用计数的缺陷是循环引用的问题。分代收集在一定程度上解决了循环引用的问题（可能描述不准确）。 引用计数Python语言默认采用的垃圾收集机制是『引用计数法 Reference Counting』，该算法最早George E. Collins在1960的时候首次提出，50年后的今天，该算法依然被很多编程语言使用，『引用计数法』的原理是：每个对象维护一个ob_ref字段，用来记录该对象当前被引用的次数，每当新的引用指向该对象时，它的引用计数ob_ref加1，每当该对象的引用失效时计数ob_ref减1，一旦对象的引用计数为0，该对象立即被回收，对象占用的内存空间将被释放。它的缺点是需要额外的空间维护引用计数，这个问题是其次的，不过最主要的问题是它不能解决对象的“循环引用”，因此，有很多语言比如Java并没有采用该算法做来垃圾的收集机制。 导致引用计数+1的情况 对象被创建，例如a=23 对象被引用，例如b=a 对象被作为参数，传入到一个函数中，例如func(a) 对象作为一个元素，存储在容器中，例如list1=[a,a]导致引用计数-1的情况 对象的别名被显式销毁，例如del a 对象的别名被赋予新的对象，例如a=24 一个对象离开它的作用域，例如f函数执行完毕时，func函数中的局部变量（全局变量不会） 对象所在的容器被销毁，或从容器中删除对象 标记清除『标记清除（Mark—Sweep）』算法是一种基于追踪回收（tracing GC）技术实现的垃圾回收算法。它分为两个阶段：第一阶段是标记阶段，GC会把所有的『活动对象』打上标记，第二阶段是把那些没有标记的对象『非活动对象』进行回收。那么GC又是如何判断哪些是活动对象哪些是非活动对象的呢？ 对象之间通过引用（指针）连在一起，构成一个有向图，对象构成这个有向图的节点，而引用关系构成这个有向图的边。从根对象（root object）出发，沿着有向边遍历对象，可达的（reachable）对象标记为活动对象，不可达的对象就是要被清除的非活动对象。根对象就是全局变量、调用栈、寄存器。 在上图中，我们把小黑圈视为全局变量，也就是把它作为root object，从小黑圈出发，对象1可直达，那么它将被标记，对象2、3可间接到达也会被标记，而4和5不可达，那么1、2、3就是活动对象，4和5是非活动对象会被GC回收。 标记清除算法作为Python的辅助垃圾收集技术主要处理的是一些容器对象，比如list、dict、tuple，instance等，因为对于字符串、数值对象是不可能造成循环引用问题。Python使用一个双向链表将这些容器对象组织起来。不过，这种简单粗暴的标记清除算法也有明显的缺点：清除非活动的对象前它必须顺序扫描整个堆内存，哪怕只剩下小部分活动对象也要扫描所有对象。 分代回收分代回收是一种以空间换时间的操作方式，Python将内存根据对象的存活时间划分为不同的集合，每个集合称为一个代，Python将内存分为了3“代”，分别为年轻代（第0代）、中年代（第1代）、老年代（第2代），他们对应的是3个链表，它们的垃圾收集频率与对象的存活时间的增大而减小。新创建的对象都会分配在年轻代，年轻代链表的总数达到上限时，Python垃圾收集机制就会被触发，把那些可以被回收的对象回收掉(包括循环引用的对象)，而那些不会回收的对象就会被移到中年代去，依此类推，老年代中的对象是存活时间最久的对象，甚至是存活于整个系统的生命周期内。同时，分代回收是建立在标记清除技术基础之上。分代回收同样作为Python的辅助垃圾收集技术处理那些容器对象 Python中的GC阈值随着你的程序运行，Python解释器保持对新创建的对象，以及因为引用计数为零而被释放掉的对象的追踪。从理论上说，这两个值应该保持一致，因为程序新建的每个对象都应该最终被释放掉。 当然，事实并非如此。因为循环引用的原因，并且因为你的程序使用了一些比其他对象存在时间更长的对象，从而被分配对象的计数值与被释放对象的计数值之间的差异在逐渐增长。一旦这个差异累计超过某个阈值，则Python的收集机制就启动了，并且触发上边所说到的零代算法，释放“浮动的垃圾”，并且将剩下的对象移动到一代列表。 随着时间的推移，程序所使用的对象逐渐从零代列表移动到一代列表。而Python对于一代列表中对象的处理遵循同样的方法，一旦被分配计数值与被释放计数值累计到达一定阈值，Python会将剩下的活跃对象移动到二代列表。 通过这种方法，你的代码所长期使用的对象，那些你的代码持续访问的活跃对象，会从零代链表转移到一代再转移到二代。通过不同的阈值设置，Python可以在不同的时间间隔处理这些对象。Python处理零代最为频繁，其次是一代然后才是二代。 gc模块Garbage Collector interfacegc模块提供一个接口给开发者设置垃圾回收的选项。上面说到，采用引用计数的方法管理内存的一个缺陷是循环引用，而gc模块的一个主要功能就是解决循环引用的问题。 常用函数： gc.set_debug(flags)设置gc的debug日志，一般设置为gc.DEBUG_LEAK gc.collect([generation])显式进行垃圾回收，可以输入参数，0代表只检查第一代的对象，1代表检查一，二代的对象，2代表检查一，二，三代的对象，如果不传参数，执行一个full collection，也就是等于传2。返回不可达（unreachable objects）对象的数目 gc.set_threshold(threshold0[, threshold1[, threshold2])设置自动执行垃圾回收的频率。 gc.get_count()获取当前自动执行垃圾回收的计数器，返回一个长度为3的列表 gc模块的自动垃圾回收机制必须要import gc模块，并且is_enable()=True才会启动自动垃圾回收。这个机制的主要作用就是发现并处理不可达的垃圾对象。 垃圾回收=垃圾检查+垃圾回收 在Python中，采用分代收集的方法。把对象分为三代，一开始，对象在创建的时候，放在一代中，如果在一次一代的垃圾检查中，该对象存活下来，就会被放到二代中，同理在一次二代的垃圾检查中，该对象存活下来，就会被放到三代中。 gc模块里面会有一个长度为3的列表的计数器，可以通过gc.get_count()获取。例如(488,3,0)，其中488是指距离上一次0代垃圾检查，Python分配内存的数目减去释放内存的数目，注意是内存分配，而不是引用计数的增加。例如：12345print gc.get_count() # (590, 8, 0)a = ClassA()print gc.get_count() # (591, 8, 0)del aprint gc.get_count() # (590, 8, 0) 3是指距离上一次1代垃圾检查，0代垃圾检查的次数，同理，0是指距离上一次2代垃圾检查，1代垃圾检查的次数。 gc模快有一个自动垃圾回收的阀值，即通过gc.get_threshold函数获取到的长度为3的元组，例如(700,10,10)每一次计数器的增加，gc模块就会检查增加后的计数是否达到阀值的数目，如果是，就会执行对应的代数的垃圾检查，然后重置计数器例如，假设阀值是(700,10,10)： 当计数器从(699,3,0)增加到(700,3,0)，gc模块就会执行gc.collect(0),即检查0代对象的垃圾，并重置计数器为(0,4,0) 当计数器从(699,9,0)增加到(700,9,0)，gc模块就会执行gc.collect(1),即检查1、2代对象的垃圾，并重置计数器为(0,0,1) 当计数器从(699,9,9)增加到(700,9,9)，gc模块就会执行gc.collect(2),即检查0、1、2代对象的垃圾，并重置计数器为(0,0,0) 应用 项目中避免循环引用 引入gc模块，启动gc模块的自动清理循环引用的对象机制 由于分代收集，所以把需要长期使用的变量集中管理，并尽快移到二代以后，减少GC检查时的消耗 gc模块唯一处理不了的是循环引用的类都有__del__方法，所以项目中要避免定义__del__方法，如果一定要使用该方法，同时导致了循环引用，需要代码显式调用gc.garbage里面的对象的__del__来打破僵局 参考 http://python.jobbole.com/87843/ https://www.cnblogs.com/pinganzi/p/6646742.html http://python.jobbole.com/87064/?utm_source=blog.jobbole.com&amp;utm_medium=relatedPosts]]></content>
      <categories>
        <category>Python3 进阶</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IO密集型任务、计算密集型任务，以及多线程、多进程]]></title>
    <url>%2F2018%2F03%2F05%2FIO%E5%AF%86%E9%9B%86%E5%9E%8B%E4%BB%BB%E5%8A%A1%E3%80%81%E8%AE%A1%E7%AE%97%E5%AF%86%E9%9B%86%E5%9E%8B%E4%BB%BB%E5%8A%A1%EF%BC%8C%E4%BB%A5%E5%8F%8A%E5%A4%9A%E7%BA%BF%E7%A8%8B%E3%80%81%E5%A4%9A%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[IO密集型任务 vs 计算密集型任务 所谓IO密集型任务，是指磁盘IO、网络IO占主要的任务，计算量很小。比如请求网页、读写文件等。当然我们在Python中可以利用sleep达到IO密集型任务的目的。 所谓计算密集型任务，是指CPU计算占主要的任务，CPU一直处于满负荷状态。比如在一个很大的列表中查找元素（当然这不合理），复杂的加减乘除等。 计算密集型任务的特点是要进行大量的计算，消耗CPU资源，比如计算圆周率、对视频进行高清解码等等，全靠CPU的运算能力。这种计算密集型任务虽然也可以用多任务完成，但是任务越多，花在任务切换的时间就越多，CPU执行任务的效率就越低，所以，要最高效地利用CPU，计算密集型任务同时进行的数量应当等于CPU的核心数。 计算密集型任务由于主要消耗CPU资源，因此，代码运行效率至关重要。Python这样的脚本语言运行效率很低，完全不适合计算密集型任务。对于计算密集型任务，最好用C语言编写。 第二种任务的类型是IO密集型，涉及到网络、磁盘IO的任务都是IO密集型任务，这类任务的特点是CPU消耗很少，任务的大部分时间都在等待IO操作完成（因为IO的速度远远低于CPU和内存的速度）。对于IO密集型任务，任务越多，CPU效率越高，但也有一个限度。常见的大部分任务都是IO密集型任务，比如Web应用。 IO密集型任务执行期间，99%的时间都花在IO上，花在CPU上的时间很少，因此，用运行速度极快的C语言替换用Python这样运行速度极低的脚本语言，完全无法提升运行效率。对于IO密集型任务，最合适的语言就是开发效率最高（代码量最少）的语言，脚本语言是首选，C语言最差。 多线程 VS 多进程多线程多线程即在一个进程中启动多个线程执行任务。一般来说使用多线程可以达到并行的目的，但由于Python中使用了全局解释锁GIL的概念，导致Python中的多线程并不是并行执行，而是“交替执行”。类似于下图：（图片转自网络，侵删） 所以Python中的多线程适合IO密集型任务，而不适合计算密集型任务。 Python提供两组多线程接口，一是thread模块_thread，提供低等级接口。二是threading模块，提供更容易使用的基于对象的接口，可以继承Thread对象来实现线程，此外其还提供了其它线程相关的对象，例如Timer，Lock等。 多线程模式通常比多进程快一点，但是也快不到哪去，而且，多线程模式致命的缺点就是任何一个线程挂掉都可能直接造成整个进程崩溃，因为所有线程共享进程的内存。在Windows上，如果一个线程执行的代码出了问题，你经常可以看到这样的提示：“该程序执行了非法操作，即将关闭”，其实往往是某个线程出了问题，但是操作系统会强制结束整个进程。 在Windows下，多线程的效率比多进程要高，所以微软的IIS服务器默认采用多线程模式。由于多线程存在稳定性的问题，IIS的稳定性就不如Apache。为了缓解这个问题，IIS和Apache现在又有多进程+多线程的混合模式，真是把问题越搞越复杂。 多进程由于Python中GIL的原因，对于计算密集型任务，Python下比较好的并行方式是使用多进程，这样可以非常有效的使用CPU资源。当然同一时间执行的进程数量取决你电脑的CPU核心数。 Python中的进程模块为mutliprocess模块，提供了很多容易使用的基于对象的接口。另外它提供了封装好的管道和队列，可以方便的在进程间传递消息。Python还提供了进程池Pool对象，可以方便的管理和控制线程。 多进程模式最大的优点就是稳定性高，因为一个子进程崩溃了，不会影响主进程和其他子进程。（当然主进程挂了所有进程就全挂了，但是Master进程只负责分配任务，挂掉的概率低）著名的Apache最早就是采用多进程模式。 多进程模式的缺点是创建进程的代价大，在Unix/Linux系统下，用fork调用还行，在Windows下创建进程开销巨大。另外，操作系统能同时运行的进程数也是有限的，在内存和CPU的限制下，如果有几千个进程同时运行，操作系统连调度都会成问题。 举个栗子实例讲解Python中的多线程、多进程如何应对IO密集型任务、计算密集型任务 这里通过一个实例，说明多线程适合IO密集型任务，多进程适合计算密集型任务。首先定义一个队列，并定义初始化队列的函数： 12345678910111213import multiprocessing# 定义全局变量Queueg_queue = multiprocessing.Queue()def init_queue(): print("init g_queue start") while not g_queue.empty(): print(g_queue.get()) for _index in range(10): g_queue.put(_index) print("init g_queue end") return 定义IO密集型任务和计算密集型任务，分别从队列中获取任务数据 1234567891011121314151617181920212223242526272829# 定义一个IO密集型任务：利用time.sleep()def task_io(task_id): print("IOTask[%s] start" % task_id) while not g_queue.empty(): time.sleep(1) try: data = g_queue.get(block=True, timeout=1) print("IOTask[%s] get data: %s" % (task_id, data)) except Exception as excep: print("IOTask[%s] error: %s" % (task_id, str(excep))) print("IOTask[%s] end" % task_id) returng_search_list = list(range(10000))# 定义一个计算密集型任务：利用一些复杂加减乘除、列表查找等def task_cpu(task_id): print("CPUTask[%s] start" % task_id) while not g_queue.empty(): count = 0 for i in range(10000): count += pow(3*2, 3*2) if i in g_search_list else 0 try: data = g_queue.get(block=True, timeout=1) print("CPUTask[%s] get data: %s" % (task_id, data)) except Exception as excep: print("CPUTask[%s] error: %s" % (task_id, str(excep))) print("CPUTask[%s] end" % task_id) return task_id 准备完上述代码之后，进行试验：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758if __name__ == '__main__': print("cpu count:", multiprocessing.cpu_count(), "\n") print("========== 直接执行IO密集型任务 ==========") init_queue() time_0 = time.time() task_io(0) print("结束：", time.time() - time_0, "\n") print("========== 多线程执行IO密集型任务 ==========") init_queue() time_0 = time.time() thread_list = [threading.Thread(target=task_io, args=(i,)) for i in range(5)] for t in thread_list: t.start() for t in thread_list: if t.is_alive(): t.join() print("结束：", time.time() - time_0, "\n") print("========== 多进程执行IO密集型任务 ==========") init_queue() time_0 = time.time() process_list = [multiprocessing.Process(target=task_io, args=(i,)) for i in range(multiprocessing.cpu_count())] for p in process_list: p.start() for p in process_list: if p.is_alive(): p.join() print("结束：", time.time() - time_0, "\n") print("========== 直接执行CPU密集型任务 ==========") init_queue() time_0 = time.time() task_cpu(0) print("结束：", time.time() - time_0, "\n") print("========== 多线程执行CPU密集型任务 ==========") init_queue() time_0 = time.time() thread_list = [threading.Thread(target=task_cpu, args=(i,)) for i in range(5)] for t in thread_list: t.start() for t in thread_list: if t.is_alive(): t.join() print("结束：", time.time() - time_0, "\n") print("========== 多进程执行cpu密集型任务 ==========") init_queue() time_0 = time.time() process_list = [multiprocessing.Process(target=task_cpu, args=(i,)) for i in range(multiprocessing.cpu_count())] for p in process_list: p.start() for p in process_list: if p.is_alive(): p.join() print("结束：", time.time() - time_0, "\n") 结果说明： 对于IO密集型任务： 直接执行用时：10.0333秒 多线程执行用时：4.0156秒 多进程执行用时：5.0182秒 说明多线程适合IO密集型任务。 对于计算密集型任务 直接执行用时：10.0273秒 多线程执行用时：13.247秒 多进程执行用时：6.8377秒 说明多进程适合计算密集型任务 参考 https://zhuanlan.zhihu.com/p/24283040]]></content>
      <categories>
        <category>操作系统</category>
        <category>多任务处理</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程、线程、协程]]></title>
    <url>%2F2018%2F03%2F04%2F%E8%BF%9B%E7%A8%8B%E3%80%81%E7%BA%BF%E7%A8%8B%E3%80%81%E5%8D%8F%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[进程电脑里运行的应用程序，都是进程，假设我们用的电脑是单核的，CPU同时只能执行一个进程。当程序处于I/O阻塞的时候，CPU如果和程序一起等待，那就太浪费了，CPU会去执行其他的程序，此时就涉及到切换，切换前要保存上一个程序运行的状态，才能恢复，所以就需要有个东西来记录这个东西，就可以引出进程的概念了。 进程就是一个程序在一个数据集上的一次动态执行过程。进程是一个动态概念，是竟争计算机系统资源的基本单位。进程由程序，数据集，进程控制块三部分组成。程序用来描述进程哪些功能以及如何完成；数据集是程序执行过程中所使用的资源；进程控制块用来保存程序运行的状态. 进程是系统进行资源分配和调度的一个独立单位。每个进程都有自己的独立内存空间，不同进程通过进程间通信来通信。由于进程比较重量，占据独立的内存，所以上下文进程间的切换开销（栈、寄存器、虚拟内存、文件句柄等）比较大，但相对比较稳定安全。 线程在网络或多用户环境下，一个服务器通常需要接收大量且不确定数量用户的并发请求，为每一个请求都创建一个进程显然是行不通的，——无论是从系统资源开销方面或是响应用户请求的效率方面来看。因此，操作系统中线程的概念便被引进了。线程，是进程的一部分，一个没有线程的进程可以被看作是单线程的。线程有时又被称为轻权进程或轻量级进程，是一个基本的cpu执行单元，也是程序执行过程中的最小单元。一个进程最少也会有一个主线程，在主线程中通过threading模块，再开子线程。 线程自己基本上不拥有系统资源,只拥有一点在运行中必不可少的资源(如程序计数器,一组寄存器和栈),但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源。线程间通信主要通过共享内存，上下文切换很快，资源开销较少，但相比进程不够稳定容易丢失数据。 进程拥有一个完整的虚拟地址空间，不依赖于线程而独立存在；反之，线程是进程的一部分，没有自己的地址空间，与进程内的其他线程一起共享分配给该进程的所有资源。 进程的状态有就绪，运行，等待三个状态；线程的状态有新建-就绪-（阻塞）-运行–死亡四个基本状态 线程全局锁GIL(Global Interpreter Lock),即Python为了保证线程安全而采取的独立线程运行的限制,说白了就是一个核只能在同一时间运行一个线程.对于io密集型任务，python的多线程起到作用，但对于cpu密集型任务，python的多线程几乎占不到任何优势，还有可能因为争夺资源而变慢。 进程、线程的关系 一个线程只能属于一个进程，而一个进程可以有多个线程，但至少有一个线程 资源分配给进程，进程是程序的主体，同一进程的所有线程共享该进程的所有资源 CPU分配给线程，即真正在CPU上运行的是线程 线程是最小的执行单元，进程是最小的资源管理单元 协程Coroutine协程是一种用户态的轻量级线程，协程的调度完全由用户控制。协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈，直接操作栈则基本没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快。 进程拥有自己独立的堆和栈，既不共享堆，亦不共享栈，进程由操作系统调度。线程拥有自己独立的栈和共享的堆，共享堆，不共享栈，线程亦由操作系统调度(标准线程是的)。协程和线程一样共享堆，不共享栈，协程由程序员在协程的代码里显示调度。进程和其他两个的区别还是很明显的。协程和线程的区别是：协程避免了无意义的调度，由此可以提高性能，但也因此，程序员必须自己承担调度的责任，同时，协程也失去了标准线程使用多CPU的能力。 举个例子：假设有一个操作系统，是单核的，系统上没有其他的程序需要运行，有两个线程 A 和 B ，A 和 B 在单独运行时都需要 10 秒来完成自己的任务，而且任务都是运算操作，A B 之间也没有竞争和共享数据的问题。现在 A B 两个线程并行，操作系统会不停的在 A B 两个线程之间切换，达到一种伪并行的效果，假设切换的频率是每秒一次，切换的成本是 0.1 秒(主要是栈切换)，总共需要 20 + 19 0.1 = 21.9 秒。如果使用协程的方式，可以先运行协程 A ，A 结束的时候让位给协程 B ，只发生一次切换，总时间是 20 + 1 0.1 = 20.1 秒。如果系统是双核的，而且线程是标准线程，那么 A B 两个线程就可以真并行，总时间只需要 10 秒，而协程的方案仍然需要 20.1 秒。 人们通常将协程和子程序（函数）比较着理解。子程序调用总是一个入口，一次返回，一旦退出即完成了子程序的执行。协程的起始处是第一个入口点，在协程里，返回点之后是接下来的入口点。在python中，协程可以通过yield来调用其它协程。通过yield方式转移执行权的协程之间不是调用者与被调用者的关系，而是彼此对称、平等的，通过相互协作共同完成任务。其运行的大致流程如下： 第一步，协程A开始执行。 第二步，协程A执行到一半，进入暂停，通过yield命令将执行权转移到协程B。 第三步，（一段时间后）协程B交还执行权。 第四步，协程A恢复执行。 协程的特点在于是一个线程执行，与多线程相比，其优势体现在： 协程的执行效率非常高。因为子程序切换不是线程切换，而是由程序自身控制，因此，没有线程切换的开销，和多线程比，线程数量越多，协程的性能优势就越明显。协程不需要多线程的锁机制。在协程中控制共享资源不加锁，只需要判断状态就好了。Tips:利用多核CPU最简单的方法是多进程+协程，既充分利用多核，又充分发挥协程的高效率，可获得极高的性能。 总结进程与线程比较线程是指进程内的一个执行单元,也是进程内的可调度实体。线程与进程的区别: 地址空间:线程是进程内的一个执行单元，进程内至少有一个线程，它们共享进程的地址空间，而进程有自己独立的地址空间 资源拥有:进程是资源分配和拥有的单位,同一个进程内的线程共享进程的资源 线程是处理器调度的基本单位,但进程不是 二者均可并发执行 每个独立的线程有一个程序运行的入口、顺序执行序列和程序的出口，但是线程不能够独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制 协程与线程进行比较 一个线程可以多个协程，一个进程也可以单独拥有多个协程，这样python中则能使用多核CPU。 线程进程都是同步机制，而协程则是异步 协程能保留上一次调用时的状态，每次过程重入时，就相当于进入上一次调用的状态]]></content>
      <categories>
        <category>操作系统</category>
        <category>多任务处理</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac OS 命令之文件(夹)删除]]></title>
    <url>%2F2018%2F03%2F03%2FMac-OS-%E5%91%BD%E4%BB%A4%E4%B9%8B%E6%96%87%E4%BB%B6-%E5%A4%B9-%E5%88%A0%E9%99%A4%2F</url>
    <content type="text"><![CDATA[rmdir删除空目录，不过一旦目录非空会提示Directiry not empty 使用rm既可以删除文件又可以删除文件夹删除文件夹（无论文件夹是否为空），使用 -rf 命令即可。即：1$ rm -rf 目录名字 -r 就是向下递归，不管有多少级目录，一并删除-f 就是直接强行删除，不作任何提示的意思删除文件夹实例：1$ rm -rf /User/Dhyana/desktop 将会删除 /User/Dhyana/desktop目录以及其下所有文件、文件夹 删除文件使用实例：1$ rm -f /User/Dhyana/desktop/test.py 将会强制删除/User/Dhyana/desktop/test.py这个文件 值得注意的是：使用这个rm -rf的时候一定要格外小心，linux没有回收站的，删除之后再想找回就很难了。有一个非常好笑的笑话就是命令行中输入12$ rm -rf /.*# 千万不要输入此命令，否则清空整个操作系统，后果自负]]></content>
      <categories>
        <category>Mac OS 命令行</category>
      </categories>
      <tags>
        <tag>Mac OS 命令行</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 使用virtualenv搭建虚拟环境]]></title>
    <url>%2F2018%2F03%2F03%2FPython3-%E4%BD%BF%E7%94%A8virtualenv%E6%90%AD%E5%BB%BA%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[在使用Python进行多个项目开发时，每个项目可能会需要安装不同的组件。把这些组件安装在同一台计算机下可能会导致组件之间的相互冲突，比如项目A使用Django 1.10，而项目B使用Django 1.8，那么同时安装两个版本可能在具体使用时产生冲突。使用虚拟环境可以有效避免这样的问题。 Python虚拟环境是一套由Ian Bicking编写的管理独立Python运行环境的系统。这样，开发者可以让每个项目运行在独立的虚拟环境中，从而避免了不同项目之间组件配置的冲突。 1.虚拟环境安装在终端中执行命令：1$ pip install virtualenv 2.虚拟环境使用假定我们要开发一个新的项目，需要一套独立的Python运行环境，或者为已有的项目建立虚拟环境，终端执行如下命令：12$ cd [项目所在目录]$ virtualenv venv 该命令执行后，将在当前目录下建立一个venv目录，该目录拷贝一份完整的当前系统的Python环境； 我么也可以执行1$ virtualenv --no-site-packages venv 这样，已经安装到系统Python环境中的所有第三方包都不会复制过来，这样，我们就得到了一个不带任何第三方包的“干净”的Python运行环境。新建的Python环境被放到当前目录下的venv目录。 有了venv这个Python环境，可以用source进入该环境（注意是在cd之后的目录）：1$ source venv/bin/activate[.fish|.zsh] 注意到命令提示符变了，有个(venv)前缀，表示当前环境是一个名为venv的Python环境。在venv环境下，用pip安装的包都被安装到venv/lib目录，而不会影响系统的Python环境。 退出当前的venv环境，使用deactivate命令：1$ deactivate 此时回到了正常的环境，现在pip或python均是在系统Python环境下执行。 3.注意为保证项目之间的独立性，建议所有使用pip安装的组件都在项目虚拟环境中进行，避免不同版本的冲突。 最后附上Virtualenv的官方文档]]></content>
      <categories>
        <category>Python3 进阶</category>
        <category>Python3 virtualenv</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 真假值对照表]]></title>
    <url>%2F2018%2F03%2F03%2FPython3-%E7%9C%9F%E5%81%87%E5%80%BC%E5%AF%B9%E7%85%A7%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[类型 False True 布尔 False(与0等价) True(与1等价) 数值 0, &nbsp;&nbsp;0.0 非零的数值 字符串 ‘’,&nbsp;&nbsp;””(空字符串) 非空字符串 容器 [],&nbsp;&nbsp;(),&nbsp;&nbsp;{},&nbsp;&nbsp;set() 至少有一个元素的容器对象 None None 非None对象]]></content>
      <categories>
        <category>Python3 进阶</category>
        <category>Python3 True or False</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown小技巧之空格输入]]></title>
    <url>%2F2018%2F03%2F03%2FMarkdown%E5%B0%8F%E6%8A%80%E5%B7%A7%E4%B9%8B%E7%A9%BA%E6%A0%BC%E8%BE%93%E5%85%A5%2F</url>
    <content type="text"><![CDATA[在使用Markdown的时候，有时候会与到这样的需求——为Markdown添加空格。跟word等其他编辑器不同，Markdown只能识别一个空格（在半角输入状态下）。通过搜寻资料找到了两个解决方案。 手动输入空格 （&amp;nbsp；）。注意！此时的分号为英文分号，但是不推荐使用此方法，太麻烦！ 使用全角空格。即：在全角输入状态下直接使用空格键就ok了]]></content>
      <categories>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 is和==的区别]]></title>
    <url>%2F2018%2F03%2F03%2FPython3-is%E5%92%8C-%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[Python 对象三要素要理解Python中is和==的区别，首先要理解Python对象的三个要素: 要素 说明 获取方式 id 身份标识，基本就是内存地址，用来唯一标识一个对象 id(obj) type 数据类型 type(obj) value 值 :—–: is和==区别 标识 名称 判断方法 is 同一性运算符 id == 比较运算符 value 程序举例例1： 123456a = &#123;"a":1, "b":2&#125;b = a.copy()a == b # True value一样a is b # False id不一样 例2： 1234567891011121314&gt;&gt;&gt; x = y = [4,5,6]&gt;&gt;&gt; z = [4,5,6]&gt;&gt;&gt; x == yTrue&gt;&gt;&gt; x == zTrue&gt;&gt;&gt; x is yTrue&gt;&gt;&gt; x is zFalse&gt;&gt;&gt;&gt;&gt;&gt; print id(x)&gt;&gt;&gt; print id(y)&gt;&gt;&gt; print id(z) 123456789101112131415161718192021222324252627282930313233343536&gt;&gt;&gt; a = 1 #a和b为数值类型&gt;&gt;&gt; b = 1&gt;&gt;&gt; a is bTrue&gt;&gt;&gt; id(a)&gt;&gt;&gt; id(b)&gt;&gt;&gt; a = 'cheesezh' #a和b为字符串类型&gt;&gt;&gt; b = 'cheesezh'&gt;&gt;&gt; a is bTrue&gt;&gt;&gt; id(a)&gt;&gt;&gt; id(b)&gt;&gt;&gt; a = (1,2,3) #a和b为元组类型&gt;&gt;&gt; b = (1,2,3)&gt;&gt;&gt; a is bFalse&gt;&gt;&gt; id(a)&gt;&gt;&gt; id(b)&gt;&gt;&gt; a = [1,2,3] #a和b为list类型&gt;&gt;&gt; b = [1,2,3]&gt;&gt;&gt; a is bFalse&gt;&gt;&gt; id(a)&gt;&gt;&gt; id(b)&gt;&gt;&gt; a = &#123;'cheese':1,'zh':2&#125; #a和b为dict类型&gt;&gt;&gt; b = &#123;'cheese':1,'zh':2&#125;&gt;&gt;&gt; a is bFalse&gt;&gt;&gt; id(a)&gt;&gt;&gt; id(b)&gt;&gt;&gt; a = set([1,2,3])#a和b为set类型&gt;&gt;&gt; b = set([1,2,3])&gt;&gt;&gt; a is bFalse&gt;&gt;&gt; id(a)&gt;&gt;&gt; id(b)]]></content>
      <categories>
        <category>Python3 进阶</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 拷贝对象(深拷贝deepcopy和浅拷贝copy)]]></title>
    <url>%2F2018%2F03%2F03%2FPython3-%E6%8B%B7%E8%B4%9D%E5%AF%B9%E8%B1%A1-%E6%B7%B1%E6%8B%B7%E8%B4%9Ddeepcopy%E5%92%8C%E6%B5%85%E6%8B%B7%E8%B4%9Dcopy%2F</url>
    <content type="text"><![CDATA[copy.copy 浅拷贝 只拷贝父对象，不会拷贝对象的内部的子对象。 copy.deepcopy 深拷贝 拷贝对象及其子对象 1234567891011121314151617181920# -*-coding:utf-8 -*-import copya = [1, 2, 3, 4, ['a', 'b']] #原始对象b = a #赋值，传对象的引用c = copy.copy(a) #对象拷贝，浅拷贝d = copy.deepcopy(a) #对象拷贝，深拷贝a.append(5) #修改对象aa[4].append('c') #修改对象a中的['a', 'b']数组对象print 'a = ', aprint 'b = ', bprint 'c = ', cprint 'd = ', d输出结果：a = [1, 2, 3, 4, ['a', 'b', 'c'], 5]b = [1, 2, 3, 4, ['a', 'b', 'c'], 5]c = [1, 2, 3, 4, ['a', 'b', 'c']]d = [1, 2, 3, 4, ['a', 'b']]]]></content>
      <categories>
        <category>Python3 进阶</category>
        <category>Python3 拷贝对象</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 作用域]]></title>
    <url>%2F2018%2F03%2F03%2FPython3-%E4%BD%9C%E7%94%A8%E5%9F%9F%2F</url>
    <content type="text"><![CDATA[Python 中，一个变量的作用域总是由在代码中被赋值的地方所决定的。 Python 获取变量中的值的搜索顺序为： 本地作用域（Local）→ 当前作用域被嵌入的本地作用域（Enclosing locals）→ 全局/模块作用域（Global）→内置作用域（Built-in）]]></content>
      <categories>
        <category>Python3 进阶</category>
        <category>Python3 作用域</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 函数重载]]></title>
    <url>%2F2018%2F03%2F03%2FPython3-%E5%87%BD%E6%95%B0%E9%87%8D%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[函数重载的目的动态语言中，有鸭子类型，如果走起路来像鸭子，叫起来也像鸭子，那么它就是鸭子。一个对象的特征不是由它的类型决定，而是通过对象中的方法决定，所以函数重载在动态语言中就显得没有意义了，因为函数可以通过鸭子类型来处理不同类型的对象，鸭子类型也是多态性的一种表现。 在Python中实现函数重载：123456789101112131415from io import StringIOclass Writer: @staticmethod def write(output, content): # output对象只要实现了write方法就行 output.write(content)# stringIO类型output = StringIO()Writer.write(output, 'hello world')# file 类型output = open('out.txt', 'w')Writer.write(output, 'hello world') 在静态语言中，方法重载是希望类可以以统一的方式处理不同类型的数据提供了可能。多个同名函数同时存在，具有不同的参数个数/类型，重载是一个类中多态性的一种表现。 在Java中实现函数重载：1234567891011class Writer&#123; public static void write(StringIO output, String content)&#123; output.write(content); return null; &#125; public static void write(File output, String content)&#123; output.write(content); return null; &#125; 参考自知乎用户刘志军：https://www.zhihu.com/question/20053359 函数重载主要是为了解决两个问题 可变参数类型。 可变参数个数。 另外，一个基本的设计原则是，仅仅当两个函数除了参数类型和参数个数不同以外，其功能是完全相同的，此时才使用函数重载，如果两个函数的功能其实不同，那么不应当使用重载，而应当使用一个名字不同的函数。 那么对于情况 1 ，函数功能相同，但是参数类型不同，python 如何处理？答案是根本不需要处理，因为 python 可以接受任何类型的参数，如果函数的功能相同，那么不同的参数类型在 python 中很可能是相同的代码，没有必要做成两个不同函数。 那么对于情况 2 ，函数功能相同，但参数个数不同，python 如何处理？大家知道，答案就是缺省参数。对那些缺少的参数设定为缺省参数即可解决问题。因为你假设函数功能相同，那么那些缺少的参数终归是需要用的。好了，鉴于情况 1 跟 情况 2 都有了解决方案，python 自然就不需要函数重载了。 参考自知乎用户pansz：https://www.zhihu.com/question/20053359]]></content>
      <categories>
        <category>Python3 进阶</category>
        <category>Python3 重载</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 子类的查看与类的对象判断]]></title>
    <url>%2F2018%2F03%2F03%2FPython3-%E5%AD%90%E7%B1%BB%E7%9A%84%E6%9F%A5%E7%9C%8B%E4%B8%8E%E7%B1%BB%E7%9A%84%E5%AF%B9%E8%B1%A1%E5%88%A4%E6%96%AD%2F</url>
    <content type="text"><![CDATA[如果想要查看一个类是不是另一个类的子类，可以使用內建的 issubclass 函数或者使用它的特殊特性__base__； 如果想要检查一个对象是不是一个类的实例，可以使用內建的 isinstance 函数或者使用它的特殊特性__class__; 12345678910111213141516171819202122232425262728# -*- coding: utf-8 -*- __metaclass__ = type #确定使新式类 class father(): def init(self): print("father()已经创建") class son(father): def init(self): print("son()已经创建") #下面测试issubclass()函数 print(issubclass(father,son)) # output: Falseprint(issubclass(son,father)) # output: True#下面使用__bases__ print("father.__bases__:",father.__bases__) # output: father.__bases__: (&lt;class 'object'&gt;,) print("son.__bases__:",son.__bases__) # output: son.__bases__: (&lt;class '__main__.father'&gt;,) #下面测试isinstance()函数 s = son() print(isinstance(s,son)) # output: Trueprint(isinstance(s,father)) # output: Trueprint(isinstance(s,str)) # output: False#下面使用__class__ print("s.__class__:",s.__class__) # output: s.__class__: &lt;class '__main__.son'&gt;]]></content>
      <categories>
        <category>Python3 进阶</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 鸭子类型]]></title>
    <url>%2F2018%2F03%2F03%2FPython3-%E9%B8%AD%E5%AD%90%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[来源和解释Duck typing 这个概念来源于美国印第安纳州的诗人詹姆斯·惠特科姆·莱利（James Whitcomb Riley,1849-1916）的诗句： “ When I see a bird that walks like a duck and swims like a duck and quacks like a duck, I call that bird a duck.” 中文： “当看到一只鸟走起来像鸭子、游泳起来像鸭子、叫起来也像鸭子，那么这只鸟就可以被称为鸭子。” “鸭子类型”的语言是这么推断的：一只鸟走起来像鸭子、游起泳来像鸭子、叫起来也像鸭子，那它就可以被当做鸭子。也就是说，它不关注对象的类型，而是关注对象具有的行为(方法)。 鸭子类型是程序设计中的一种类型推断风格，这种风格适用于动态语言(比如PHP、Python、Ruby、Typescript、Perl、Objective-C、Lua、Julia、JavaScript、Java、Groovy、C#等)和某些静态语言(比如Golang,一般来说，静态类型语言在编译时便已确定了变量的类型，但是Golang的实现是：在编译时推断变量的类型)，支持”鸭子类型”的语言的解释器/编译器将会在解析(Parse)或编译时，推断对象的类型。 在鸭子类型中，关注的不是对象的类型本身，而是它是如何使用的。 例如，在不使用鸭子类型的语言中，我们可以编写一个函数，它接受一个类型为鸭的对象，并调用它的走和叫方法。在使用鸭子类型的语言中，这样的一个函数可以接受一个任意类型的对象，并调用它的走和叫方法。如果这些需要被调用的方法不存在，那么将引发一个运行时错误。任何拥有这样的正确的走和叫方法的对象都可被函数接受的这种行为引出了以上表述，这种决定类型的方式因此得名。 鸭子类型通常得益于不测试方法和函数中参数的类型，而是依赖文档、清晰的代码和测试来确保正确使用。从静态类型语言转向动态类型语言的用户通常试图添加一些静态的（在运行之前的）类型检查，从而影响了鸭子类型的益处和可伸缩性，并约束了语言的动态特性。 不足“鸭子类型”没有任何静态检查，如类型检查、属性检查、方法签名检查等。 “鸭子类型”语言的程序可能会在运行时因为不具备某种特定的方法而抛出异常：如果一只小狗(对象)想加入合唱团(以对象会不会嘎嘎嘎叫的方法为检验标准)，也学鸭子那么嘎嘎嘎叫，好吧，它加入了，可是加入之后，却不会像鸭子那样走路，那么，迟早要出问题的。 再举个例子：一只小老鼠被猫盯上了，情急之下，它学了狗叫，猫撤了之后，小老鼠的妈妈不无感叹的对它说：看吧，我让你学的这门儿外语多么重要啊。这虽然是个段子，但是，由于猫在思考时，使用了 “鸭子测试”，它以为会叫的就是狗，会对自己产生威胁，所以撤退了，也正是因为这个错误的判断，它误失了一次进食机会。 静态类型语言和动态类型语言的区别静态类型语言在编译时便已确定变量的类型，而动态类型语言的变量类型要到程序运行的时候，待变量被赋予某个值之后，才会具有某种类型。 静态类型语言的优点首先是在编译时就能发现类型不匹配的错误，编辑器可以帮助我们提前避免程序在运行期间有可能发生的一些错误。其次，如果在程序中明确地规定了数据类型，编译器还可以针对这些信息对程序进行一些优化工作，提高程序执行速度。 静态类型语言的缺点首先是迫使程序员依照强契约来编写程序，为每个变量规定数据类型，归根结底只是辅助我们编写可靠性高程序的一种手段，而不是编写程序的目的，毕竟大部分人编写程序的目的是为了完成需求交付生产。其次，类型的声明也会增加更多的代码，在程序编写过程中，这些细节会让程序员的精力从思考业务逻辑上分散开来。 动态类型语言的优点是编写的代码数量更少，看起来也更加简洁，程序员可以把精力更多地放在业务逻辑上面。虽然不区分类型在某些情况下会让程序变得难以理解，但整体而言，代码量越少，越专注于逻辑表达，对阅读程序是越有帮助的。动态类型语言的缺点是无法保证变量的类型，从而在程序的运行期有可能发生跟类型相关的错误。 动态类型语言对变量类型的宽容给实际编码带来了很大的灵活性。由于无需进行类型检测，我们可以尝试调用任何对象的任意方法，而无需去考虑它原本是否被设计为拥有该方法。 面向接口编程动态类型语言的面向对象设计中，鸭子类型的概念至关重要。利用鸭子类型的思想，我们不必借助超类型的帮助，就能轻松地在动态类型语言中实现一个原则：“面向接口编程，而不是面向实现编程”。例如, 一个对象若有push和pop方法，并且这些方法提供了正确的实现，它就可以被当作栈来使用。 一个对象如果有length属性，也可以依照下标来存取属性（最好还要拥有slice和splice等方法），这个对象就可以被当作数组来使用。 比如在python中，有很多file-like的东西，比如StringIO,GzipFile,socket。它们有很多相同的方法，我们把它们当作文件使用。 又比如list.extend()方法中,我们并不关心它的参数是不是list,只要它是可迭代的,所以它的参数可以是list/tuple/dict/字符串/生成器等. 鸭子类型在动态语言中经常使用，非常灵活，使得python不想java那样专门去弄一大堆的设计模式。 在静态类型语言中，要实现“面向接口编程”并不是一件容易的事情，往往要通过抽象类或者接口等将对象进行向上转型。当对象的真正类型被隐藏在它的超类型身后，这些对象才能在类型检查系统的“监视”之下互相被替换使用。只有当对象能够被互相替换使用，才能体现出对象多态性的价值。 Python中的多态Python中的鸭子类型允许我们使用任何提供所需方法的对象，而不需要迫使它成为一个子类。由于python属于动态语言，当你定义了一个基类和基类中的方法，并编写几个继承该基类的子类时，由于python在定义变量时不指定变量的类型，而是由解释器根据变量内容推断变量类型的（也就是说变量的类型取决于所关联的对象），这就使得python的多态不像是c++或java中那样—定义一个基类类型变量而隐藏了具体子类的细节。 请看下面的例子和说明：1234567891011121314151617181920212223242526272829303132333435class AudioFile: def __init__(self, filename): if not filename.endswith(self.ext): raise Exception("Invalid file format") self.filename = filenameclass MP3File(AudioFile): ext = "mp3" def play(self): print("Playing &#123;&#125; as mp3".format(self.filename))class WavFile(AudioFile): ext = "wav" def play(self): print("Playing &#123;&#125; as wav".format(self.filename))class OggFile(AudioFile): ext = "ogg" def play(self): print("Playing &#123;&#125; as ogg".format(self.filename))class FlacFile: """ Though FlacFile class doesn't inherit AudioFile class, it also has the same interface as three subclass of AudioFile. It is called duck typing. """ def __init__(self, filename): if not filename.endswith(".flac"): raise Exception("Invalid file format") self.filename = filename def play(self): print("Playing &#123;&#125; as flac".format(self.filename)) Though FlacFile class doesn’t inherit AudioFile class,it also has the same interface as three subclass of AudioFile.It is called duck typing. 上面的代码中，MP3File、WavFile、OggFile三个类型继承了AudioFile这一积累，而FlacFile没有扩展AudioFile，但是可以在python中使用完全相同的接口与之交互。 因为任何提供正确接口的对象都可以在python中交替使用，它减少了多态的一般超类的需求。继承仍然可以用来共享代码，但是如果所有被共享的都是公共接口，鸭子类型就是所有所需的。这减少了继承的需要，同时也减少了多重继承的需要；通常，当多重继承似乎是一个有效方案的时候，我们只需要使用鸭子类型去模拟多个超类之一（定义和那个超类一样的接口和实现）就可以了。 作者：JasonDing链接：https://www.jianshu.com/p/650485b78d11來源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 参考 https://baike.baidu.com/item/鸭子类型/10845665?fr=aladdin https://www.jianshu.com/p/650485b78d11]]></content>
      <categories>
        <category>Python3 进阶</category>
        <category>Python3 鸭子类型</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python3 如何用一个表达式合并两个字典]]></title>
    <url>%2F2018%2F03%2F03%2Fpython3-%E5%A6%82%E4%BD%95%E7%94%A8%E4%B8%80%E4%B8%AA%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E5%AD%97%E5%85%B8%2F</url>
    <content type="text"><![CDATA[有两个Python字典,写一个表达式来返回两个字典的合并。update()方法返回的是空值而不是返回合并后的对象.1234567&gt;&gt;&gt; x = &#123;'a':1, 'b': 2&#125;&gt;&gt;&gt; y = &#123;'b':10, 'c': 11&#125;&gt;&gt;&gt; z = x.update(y)&gt;&gt;&gt; print zNone&gt;&gt;&gt; x&#123;'a': 1, 'b': 10, 'c': 11&#125; 如何才能让值保存在z而不是x? 对于python2可以用下面的方法:1z = dict(x.items() + y.items()) 最后就是你想要的最终结果保存在字典z中,而键b的值会被第二个字典的值覆盖.12345&gt;&gt;&gt; x = &#123;'a':1, 'b': 2&#125;&gt;&gt;&gt; y = &#123;'b':10, 'c': 11&#125;&gt;&gt;&gt; z = dict(x.items() + y.items())&gt;&gt;&gt; z&#123;'a': 1, 'c': 11, 'b': 10&#125; 对于Python3：123&gt;&gt;&gt; z = dict(list(x.items()) + list(y.items()))&gt;&gt;&gt; z&#123;'a': 1, 'c': 11, 'b': 10&#125; 还可以这样:12z = x.copy()z.update(y)]]></content>
      <categories>
        <category>Python3 进阶</category>
        <category>Python3 数据结构</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python3 可迭代对象、迭代器和生成器]]></title>
    <url>%2F2018%2F03%2F01%2Fpython3-%E5%8F%AF%E8%BF%AD%E4%BB%A3%E5%AF%B9%E8%B1%A1%E3%80%81%E8%BF%AD%E4%BB%A3%E5%99%A8%E5%92%8C%E7%94%9F%E6%88%90%E5%99%A8%2F</url>
    <content type="text"><![CDATA[前言迭代是数据处理的基石。扫描内存中放不下数据集时，我们要找到一种惰性获取数据项的方式，即按需一次获取一个数据项，这就是迭代器模式（iterator pattern）。所有的生成器都是迭代器，因为生成器完全实现了迭代器接口。在python社区中，大多数时候都把迭代器和生成器视作同一概念。 所有python程序员都知道，序列可迭代，下面说明具体原因。 序列可迭代的原因：iter函数解释器需要迭代对象x时，会自动调用iter(x)。内置的iter函数有以下作用。 检查对象是否实现了__iter__方法，如果实现了就调用它，获得一个迭代器。 如果没有实现__iter__方法，但是实现了__getitem__方法，python会创建一个迭代器，尝试按顺序（从索引0开始）获取元素。 如果尝试失败，python会抛出TypeError异常，通常会提示”C object is not iterable”,其中C是目标对象所属的类。 任何Python序列都可迭代的原因是它们实现了__getitem__方法。其实标准的序列也都实现了__iter__方法。之所以对__getitem__方法做特殊处理是为了向后兼容。 从Python3.4开始，检查x能否迭代，最准确的方法是调用iter(x)函数，如果不可迭代，再处理TypeError异常。这比使用isinstance(x, abc.Iterable)更准确，因为iter(x)函数会考虑到遗留的__getitem__方法，而abc.Iterable类则不考虑。 可迭代的对象与迭代器的对比可迭代对象使用iter内置函数可以获取迭代器的对象。 如果实现了能返回迭代器的__iter__方法，那么对象就是可迭代的。序列都可以迭代；实现了__getitem__方法，而且七参数是从零开始的索引，这种对象也是可迭代的。 我们要明确可迭代对象和迭代器之间的关系：Python从可迭代的对象中获取迭代器 标准的迭代器接口有两个方法，即： __next__:返回下一个可用元素，如果没有元素，抛出StopIteration异常 __iter__:返回self,以便在应该使用可迭代对象的地方使用迭代器，比如for循环中。 因为迭代器只需__next__和__iter__两个方法，所以除了调用next()方法，以及捕获StopIteration异常之外，没有办法检查是否还有遗留的元素。此外，也没有办法还原迭代器。如果想再次迭代，那就要调用iter(…)，传入之前构建迭代器的可迭代对象。 迭代器迭代器是这样的对象：实现了无参数的__next__方法，返回序列中的下一个元素；如果没有元素了，那么抛出StopIteration异常。Python迭代器还实现了__iter__方法，因此迭代器也可以迭代。 构建可迭代对象和迭代器时经常会出现错误，原因是混淆了两者。要知道，可迭代的对象有个__iter__方法，每次都实例化一个新的迭代器；而迭代器要实现__next__方法，返回单个元素，此外还要实现__iter__方法，返回迭代器本身。因此，迭代器可以迭代，但是可迭代的对象不是迭代器。 可迭代的对象一定不是自身的迭代器。也就是说，可迭代的对象必须实现__iter__方法，但不能实现__next__方法。另一方面，迭代器应该一直可以迭代，迭代器的__iter__方法应该返回自身。 123a = [1,2,3]'__iter__' in dir(a) # True'__iter__' in dir(iter(a)) # True 生成器函数只要Python函数的定义体中有yield关键字，该函数就是生成器函数。调用生成器函数时，会返回一个生成器对象。也就是说，生成器函数是生成器工厂。 普通的函数与生成器函数在句法上的唯一区别是，在后者的定义体中有yield关键字。有些人认为定义生成器函数应该使用一个新的关键字，例如gen，而不是def，但是Guido不同意。 生成器函数工作原理1234567891011121314151617181920212223242526def gen_123(): # 只要Python代码中包含yield，该函数就是生成器函数 yield 1 #生成器函数的定义体中通常都有循环，不过这不是必要条件；此处重复使用了3次yield yield 2 yield 3if __name__ == '__main__': print(gen_123) # 可以看出gen_123是函数对象 # &lt;function gen_123 at 0x10be199d8&gt; print(gen_123()) # 函数调用时返回的是一个生成器对象 # &lt;generator object gen_123 at 0x10be31ca8&gt; for i in gen_123(): # 生成器是迭代器，会生成传给yield关键字的表达式的值 print(i) # 1 # 2 # 3 g = gen_123() # 为了仔细检查，把生成器对象赋值给g print(next(g)) # 1 print(next(g)) # 2 print(next(g)) # 3 print(next(g)) # 生成器函数的定义体执行完毕后，生成器对象会抛出异常。# Traceback (most recent call last):# File "test.py", line 17, in &lt;module&gt;# print(next(g))# StopIteration 如上述代码所示： 只要Python代码中包含yield，该函数就是生成器函数 生成器函数的定义体中通常都有循环，不过这不是必要条件；此处重复使用了3次yield 可以看出gen_123是函数对象 函数调用时返回的是一个生成器对象 生成器是迭代器，会生成传给yield关键字的表达式的值 为了仔细检查，把生成器对象赋值给g 因为g是迭代器，所以调用nest(g)会获取yield生成的下一个元素 生成器函数的定义体执行完毕后，生成器对象会抛出异常。 使用准确的词语描述从生成器中获取结果的过程有助于理解生成器。注意，此处说的是产出或生成值。如果说生成器返回值，就会让人难以理解。 函数返回值; 调用生成器函数返回生成器; 生成器产出或生成值。生成器不会以常规方式返回值; 1234567891011121314151617In [66]: def gen_AB(): # 1 ...: print('start') ...: yield 'A' # 2 ...: print('continue') ...: yield 'B' # 3 ...: print('end.') # 4 ...:In [67]: for c in gen_AB(): # 5 ...: print('--&gt;', c) # 6 ...:start # 7--&gt; A # 8continue # 9--&gt; B # 10end. # 11 定义生成的器函数的方式与普通函数无异，只不过要使用yield关键字 在for循环中第一次隐式调用next()函数时（序号5），会打印’start’，然后停在第一个yield语句，生成值 ‘A’ 在for循环第二次隐式调用next()函数时，会打印’continue’，然后停在第二个yield语句，生成值’B’ 第三次调用 next()函数时，会打印’end.’，然后到达函数定义体末尾。导致生成器对象抛出StopIteration异常 迭代时, for 机制的作用与g = iter(gen_AB())一样，用于获取生成器对象，然后每次迭代时调用next(g) 循环打印 –&gt; 与 next(g)返回的值。但是，生成器函数中的print函数输出结果之后才会看到这个输出 ‘start’是生成器函数定义体中print(‘start’)输出的记过 生成器函数定义体中的yield ‘A’ 语句会生成值 A，提供给for循环使用，而A会赋值给变量c，最终输出–&gt; A 第二次调用next(g)，继续迭代，生成器函数定义体中的代码由yield ‘A’前进到 yield ‘B’。文本continue是由生成器函数定义体中的第二个print函数输出的 生成器函数定义体中的yield ‘B’ 语句会生成值 B，提供给for循环使用，而B会赋值给变量c，最终输出–&gt; B 第三次调用next(g)，继续迭代，前进到生成器函数的结尾。文本 end. 是由生成器函数定义体中第三个print函数输出的。 到达生成器函数定义体结尾时，生成器对象抛出StopIteration异常。for 机制会捕捉异常，因此循环终止没有报错。 生成器表达式简单的生成器函数，可以替换成生成器表达式。生成器表达式可以理解为列表推导的惰性版本：不会迫切的构建列表，而是返回一个生成器，按需惰性生成元素。也就是说，如果列表推导是制造工厂的列表，那么生成器表达式就是制造生成器的工厂。如下演示了一个简单的生成器表达式，并且与列表推导做了对比。 123456789101112131415161718192021222324252627282930313233In [66]: def gen_AB(): # 1 ...: print('start') ...: yield 'A' ...: print('continue') ...: yield 'B' ...: print('end.') ...:In [67]: res1 = [x*3 for x in gen_AB()] # 2startcontinueend.In [68]: for i in res1(): # 3 ...: print('--&gt;', i) ...:AAABBBIn [69]: res2 = (x*3 for x in gen_AB()) # 4In [70]: res2 # 5&lt;generator object &lt;genexpr&gt; at 0x106a07620&gt;In [71]: for i in res2(): # 6 ...: print('--&gt;', i) ...:start --&gt; A continue--&gt; B end. 创建gen_AB函数 列表推到迫切的迭代gen_AB()函数生成的生成器对象产出的元素：’A’和’B’。注意。下面输出的是start、continue、end.。 for循环迭代列表推导生成的res1列表 把生成器表达式返回的值赋值给res2。只需调用gen_AB()函数，虽然调用时会返回一个生成器，但是这里并不使用。 可以看出res2是一个生成器对象。 只有for循环迭代res2时，gen_AB函数的定义体才会真正执行。for循环每次迭代时会隐式调用next(res2)，前进到gen_AB函数中的下一个yield语句。注意，gen_AB函数的输出与for循环中print函数的输出夹杂在一起。 生成器表达式会产出生成器，因此可以使用生成器表达式进一步减少代码量。生成器表达式是一种语法糖，完全可以替换成生成器函数，不过有时候使用生成器表达式更便利。 何时使用生成器表达式生成器表达式是创建生成器的简洁句法，这样无需定义函数再调用。不过，生成器函数灵活的多，可以使用多个语句实现复杂的逻辑，也可以作为协程使用。遇到简单的情况时，可以使用生成器表达式，因为这样扫一眼就知道代码的作用。其实选择那种句法很容易判断：如果生成器表达式需要分行写，倾向于定义成生成器函数，以便提高可读性。此外生成器函数有名称，因此可以重用。]]></content>
      <categories>
        <category>Python3 进阶</category>
        <category>Python3 迭代器和生成器</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[if __name__ == '__main__': ?]]></title>
    <url>%2F2018%2F03%2F01%2Fif-name-main%2F</url>
    <content type="text"><![CDATA[Every Python module has it’s __name__ defined and if this is &#39;__main__&#39;, it implies that the module is being run standalone by the user and we can do corresponding appropriate actions. 当Python解析器读取一个源文件时,它会执行所有的代码.在执行代码前,会定义一些特殊的变量.例如,如果解析器运行的模块(源文件)作为主程序,它将会把__name__变量设置成&quot;__main__&quot;.如果只是引入其他的模块,__name__变量将会设置成模块的名字. 这么做的原因是有时你想让你的模块既可以直接的执行,还可以被当做模块导入到其他模块中去.通过检查是不是主函数,可以让你的代码只在它作为主程序运行时执行,而当其他人调用你的模块中的函数的时候不必执行. 直接上一个栗子：1234567# Filename: using_name.pyprint(__name__)if __name__ == '__main__': print('This program is being run by itself')else: print('I am being imported from another module') 12345678910$ python using_name.py__main__This program is being run by itself$ python&gt;&gt;&gt; import using_nameusing_nameI am being imported from another module&gt;&gt;&gt;]]></content>
      <categories>
        <category>Python3 进阶</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 中的单下划线和双下划线]]></title>
    <url>%2F2018%2F03%2F01%2FPython3-%E5%8D%95%E4%B8%8B%E5%88%92%E7%BA%BF%E5%92%8C%E5%8F%8C%E4%B8%8B%E5%88%92%E7%BA%BF%2F</url>
    <content type="text"><![CDATA[单下划线在解释器中在交互解释器中，_符号还是指交互解释器中最后一次执行语句的返回结果。这种用法最初出现在CPython解释器中，其他解释器后来也都跟进了。 作为名称使用这个跟上面有点类似。_用作被丢弃的名称。按照惯例，这样做可以让阅读你代码的人知道，这是个不会被使用的特定名称。举个例子，你可能无所谓一个循环计数的值：123n = 42for _ in range(n): do_something() i18n_还可以被用作函数名。这种情况，单下划线经常被用作国际化和本地化字符串翻译查询的函数名。举个例子，在 Django documentation for translation 中你可能会看到： 123456from django.utils.translation import ugettext as _from django.http import HttpResponsedef my_view(request): output = _("Welcome to my site.") return HttpResponse(output) 注意：第二种和第三种用法会引起冲突，所以在任意代码块中，如果使用了_作i18n翻译查询函数，就应该避免再用作被丢弃的变量名。 单下划线前缀的名称首先是单下划线开头，这个被常用于模块中，在一个模块中以单下划线开头的变量和函数被默认当作内部函数,用来指定私有变量。如果使用 from a_module import * 导入时，这部分变量和函数不会被导入。不过值得注意的是，如果使用 import a_module 这样导入模块，仍然可以用 a_module._some_var 这样的形式访问到这样的对象。 另外单下划线开头还有一种一般不会用到的情况在于使用一个 C 编写的扩展库有时会用下划线开头命名，然后使用一个去掉下划线的 Python 模块进行包装。如 struct 这个模块实际上是 C 模块 _struct 的一个 Python 包装。 单下划线后缀的名称在 Python 的官方推荐的代码样式中，还有一种单下划线结尾的样式，这在解析时并没有特别的含义，但通常用于和 Python 关键词区分开来，比如如果我们需要一个变量叫做 class，但 class 是 Python 的关键词，就可以以单下划线结尾写作 class_。 双下划线双下划线开头的命名形式在 Python 的类成员中使用表示名字改编 (Name Mangling)，即如果有一 Test 类里有一成员 __x，那么 dir(Test) 时会看到 _Test__x 而非 __x。这是为了避免该成员的名称与子类中的名称冲突。但要注意这要求该名称末尾最多有一个下划线 python document. 双下划线开头双下划线结尾的是一些 Python 的“魔术”对象，如类成员的 __init__、__del__、__add__、__getitem__ 等，以及全局的 __file__、__name__ 等。 Python 官方推荐永远不要将这样的命名方式应用于自己的变量或函数，而是按照文档说明来使用。 举个栗子1234567891011121314&gt;&gt;&gt; class MyClass():... def __init__(self):... self.__superprivate = "Hello"... self._semiprivate = ", world!"...&gt;&gt;&gt; mc = MyClass()&gt;&gt;&gt; print mc.__superprivateTraceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;AttributeError: myClass instance has no attribute '__superprivate'&gt;&gt;&gt; print mc._semiprivate, world!&gt;&gt;&gt; print mc.__dict__&#123;'_MyClass__superprivate': 'Hello', '_semiprivate': ', world!'&#125; __foo__:一种约定,Python内部的名字,用来区别其他用户自定义的命名,以防冲突，就是例如__init__(),__del__(),__call__()这些特殊方法 _foo:一种约定,用来指定变量私有.程序员用来指定私有变量的一种方式.不能用from module import * 导入，其他方面和公有一样访问； __foo:这个有真正的意义:解析器用_classname__foo来代替这个名字,以区别和其他类相同的命名,它无法直接像公有成员一样随便访问,但是可以通过对象名 _类名__xxx 这样的方式可以访问. 参考详情见: http://stackoverflow.com/questions/1301346/the-meaning-of-a-single-and-a-double-underscore-before-an-object-name-in-python http://www.zhihu.com/question/19754941 https://segmentfault.com/a/1190000002611411 https://docs.python.org/3.4/tutorial/classes.html#tut-private]]></content>
      <categories>
        <category>Python3 进阶</category>
        <category>Python3 下划线</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 自省]]></title>
    <url>%2F2018%2F03%2F01%2FPython3-%E8%87%AA%E7%9C%81%2F</url>
    <content type="text"><![CDATA[自省是python彪悍的特性之一. 自省（introspection）是一种自我检查行为。在计算机编程中，自省是指这种能力：检查某些事物以确定它是什么、它知道什么以及它能做什么。自省向程序员提供了极大的灵活性和控制力. 自省就是面向对象的语言所写的程序在运行时,所能知道对象的类型.简单一句就是运行时能够获得对象的类型.比如type(),dir(),getattr(),hasattr(),isinstance(). 12345a = [1,2,3]b = &#123;'a':1,'b':2,'c':3&#125;c = Trueprint type(a),type(b),type(c) # &lt;type 'list'&gt; &lt;type 'dict'&gt; &lt;type 'bool'&gt;print isinstance(a,list) # True 未完待续 参考： http://python.jobbole.com/82110/ http://blog.csdn.net/IAlexanderI/article/details/78768378]]></content>
      <categories>
        <category>Python3 进阶</category>
        <category>Python3 自省</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 类变量和实例变量]]></title>
    <url>%2F2018%2F03%2F01%2FPython3-%E7%B1%BB%E5%8F%98%E9%87%8F%E5%92%8C%E5%AE%9E%E4%BE%8B%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[写在前面首先来一张图 类变量和实例变量在Python Tutorial中对于类变量和实例变量是这样描述的： Generally speaking, instance variables are for data unique to each instance and class variables are for attributes and methods shared by all instances of the class: 1234class Dog: kind = 'canine' # class variable shared by all instances def __init__(self, name): self.name = name # instance variable unique to each instance 类Dog中，类属性kind为所有实例所共享；实例属性name为每个Dog的实例独有。 类变量： ​ 是可在类的所有实例之间共享的值（也就是说，它们不是单独分配给每个实例的）。例如下例中，num_of_instance 就是类变量，用于跟踪存在着多少个Test 的实例。 实例变量： 实例化之后，每个实例单独拥有的变量。 12345678910111213class Test(object): num_of_instance = 0 def __init__(self, name): self.name = name Test.num_of_instance += 1 if __name__ == '__main__': print Test.num_of_instance # 0 t1 = Test('jack') print Test.num_of_instance # 1 t2 = Test('lucy') print t1.name , t1.num_of_instance # jack 2 print t2.name , t2.num_of_instance # lucy 2 补充的例子 123456789class Person: name="aaa"p1=Person()p2=Person()p1.name="bbb"print p1.name # bbbprint p2.name # aaaprint Person.name # aaa 这里p1.name=&quot;bbb&quot;是实例调用了类变量,属于函数传参的问题,p1.name一开始是指向的类变量name=&quot;aaa&quot;,但是在实例的作用域里把类变量的引用改变了,就变成了一个实例变量,self.name不再引用Person的类变量name了. 可以看看下面的例子: 123456789class Person: name=[]p1=Person()p2=Person()p1.name.append(1)print p1.name # [1]print p2.name # [1]print Person.name # [1] 类对象和实例对象类对象Python中一切皆对象；类定义完成后，会在当前作用域中定义一个以类名为名字，指向类对象的名字。如12class Dog: pass 会在当前作用域定义名字Dog，指向类对象Dog。 类对象支持的操作：总的来说，类对象仅支持两个操作： 实例化；使用instance_name = class_name()的方式实例化，实例化操作创建该类的实例。 属性引用；使用class_name.attr_name的方式引用类属性。 实例对象实例对象是类对象实例化的产物，实例对象仅支持一个操作: 属性引用；与类对象属性引用的方式相同，使用instance_name.attr_name的方式。 按照严格的面向对象思想，所有属性都应该是实例的，类属性不应该存在。那么在Python中，由于类属性绑定就不应该存在，类定义中就只剩下函数定义了。 在Python tutorial关于类定义也这么说： In practice, the statements inside a class definition will usually be function definitions, but other statements are allowed, and sometimes useful. 实践中，类定义中的语句通常是函数定义，但是其他语句也是允许的，有时也是有用的。 这里说的其他语句，就是指类属性的绑定语句。 属性绑定在定义类时，通常我们说的定义属性，其实是分为两个方面的： 类属性绑定 实例属性绑定 用绑定这个词更加确切；不管是类对象还是实例对象，属性都是依托对象而存在的。 我们说的属性绑定，首先需要一个可变对象，才能执行绑定操作，使用 objname.attr = attr_value 的方式，为对象objname绑定属性attr。 这分两种情况： 若属性attr已经存在，绑定操作会将属性名指向新的对象； 若不存在，则为该对象添加新的属性，后面就可以引用新增属性。 类属性绑定Python作为动态语言，类对象和实例对象都可以在运行时绑定任意属性。因此，类属性的绑定发生在两个地方： 类定义时； 运行时任意阶段。 下面这个例子说明了类属性绑定发生的时期：12345678class Dog: kind = 'canine'Dog.country = 'China'print(Dog.kind, ' - ', Dog.country) # output: canine - Chinadel Dog.kindprint(Dog.kind, ' - ', Dog.country)# AttributeError: type object 'Dog' has no attribute 'kind' 在类定义中，类属性的绑定并没有使用objname.attr = attr_value的方式，这是一个特例，其实是等同于后面使用类名绑定属性的方式。因为是动态语言，所以可以在运行时增加属性，删除属性。 实例属性绑定与类属性绑定相同，实例属性绑定也发生在两个地方： 类定义时； 运行时任意阶段。 示例：123456789class Dog: def __init__(self, name, age): self.name = name self.age = agedog = Dog('Lily', 3)dog.fur_color = 'red'print('%s is %s years old, it has %s fur' % (dog.name, dog.age, dog.fur_color))# Output: Lily is 3 years old, it has red fur Python类实例有两个特殊之处： __init__在实例化时执行 Python实例调用方法时，会将实例对象作为第一个参数传递 因此，__init__方法中的self就是实例对象本身，这里是dog，语句12self.name = nameself.age = age 以及后面的语句1dog.fur_color = 'red' 为实例dog增加三个属性name, age, fur_color。 属性引用类属属性引用类属性的引用，肯定是需要类对象的，属性分为两种： 数据属性 函数属性 数据属性引用很简单，示例：12345class Dog: kind = 'canine'Dog.country = 'China'print(Dog.kind, ' - ', Dog.country) # output: canine - China 通常很少有引用类函数属性的需求，示例：123456class Dog: kind = 'canine' def tell_kind(): print(Dog.kind)Dog.tell_kind() # Output: canine 函数tell_kind在引用kind需要使用Dog.kind而不是直接使用kind，涉及到作用域，这一点在我的另一篇文章中有介绍：Python进阶 - 命名空间与作用域 实例属性引用使用实例对象引用属性稍微复杂一些，因为实例对象可引用类属性以及实例属性。但是实例对象引用属性时遵循以下规则： 总是先到实例对象中查找属性，再到类属性中查找属性； 属性绑定语句总是为实例对象创建新属性，属性存在时，更新属性指向的对象。 数据属性引用示例1：12345678910111213class Dog: kind = 'canine' country = 'China' def __init__(self, name, age, country): self.name = name self.age = age self.country = countrydog = Dog('Lily', 3, 'Britain')print(dog.name, dog.age, dog.kind, dog.country)# output: Lily 3 canine Britain 类对象Dog与实例对象dog均有属性country，按照规则，dog.country会引用到实例对象的属性；但实例对象dog没有属性kind，按照规则会引用类对象的属性。 示例2：123456789101112131415161718class Dog: kind = 'canine' country = 'China' def __init__(self, name, age, country): self.name = name self.age = age self.country = countrydog = Dog('Lily', 3, 'Britain')print(dog.name, dog.age, dog.kind, dog.country) # Lily 3 canine Britainprint(dog.__dict__) # &#123;'name': 'Lily', 'age': 3, 'country': 'Britain'&#125;dog.kind = 'feline'print(dog.name, dog.age, dog.kind, dog.country) # Lily 3 feline Britainprint(dog.__dict__) # &#123;'name': 'Lily', 'age': 3, 'country': 'Britain', 'kind': 'feline'&#125;print(Dog.kind) # canine 没有改变类属性的指向 示例3，可变类属性引用： 1234567891011121314151617class Dog: tricks = [] def __init__(self, name): self.name = name def add_trick(self, trick): # self.tricks.append(trick) Dog.tricks.append(trick)d = Dog('Fido')e = Dog('Buddy')d.add_trick('roll over')e.add_trick('play dead')print(d.tricks) # ['roll over', 'play dead'] 语句self.tricks.append(trick)并不是属性绑定语句，因此还是在类属性上修改可变对象。 方法属性引用与数据成员不同，类函数属性在实例对象中会变成方法属性。先看一个示例： 1234567891011121314class MethodTest: def inner_test(self): print('in class')def outer_test(): print('out of class')mt = MethodTest()mt.outer_test = outer_testprint(type(MethodTest.inner_test)) # &lt;class 'function'&gt; 类函数print(type(mt.inner_test)) #&lt;class 'method'&gt; 类方法print(type(mt.outer_test)) #&lt;class 'function'&gt; 类函数 可以看到，类函数属性在实例对象中变成了方法属性，但是并不是实例对象中所有的函数都是方法。 Python tutorial中这样介绍方法对象： When an instance attribute is referenced that isn’t a data attribute, its class is searched. If the name denotes a valid class attribute that is a function object, a method object is created by packing (pointers to) the instance object and the function object just found together in an abstract object: this is the method object. When the method object is called with an argument list, a new argument list is constructed from the instance object and the argument list, and the function object is called with this new argument list. 引用非数据属性的实例属性时，会搜索它对应的类。如果名字是一个有效的函数对象，Python会将实例对象连同函数对象打包到一个抽象的对象中并且依据这个对象创建方法对象：这就是被调用的方法对象。当使用参数列表调用方法对象时，会使用实例对象以及原有参数列表构建新的参数列表，并且使用新的参数列表调用函数对象。 那么，实例对象只有在引用方法属性时，才会将自身作为第一个参数传递；调用实例对象的普通函数，则不会。所以可以使用如下方式直接调用方法与函数： 12mt.inner_test()mt.outer_test() 除了方法与函数的区别，其引用与数据属性都是一样的 最佳实践虽然Python作为动态语言，支持在运行时绑定属性，但是从面向对象的角度来看，还是在定义类的时候将属性确定下来。 参考: http://stackoverflow.com/questions/6470428/catch-multiple-exceptions-in-one-line-except-block https://www.cnblogs.com/crazyrunning/p/6945183.html]]></content>
      <categories>
        <category>Python3 进阶</category>
        <category>类变量 和 实例变量</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 @staticmethod和@classmethod]]></title>
    <url>%2F2018%2F03%2F01%2FPython3-staticmethod%E5%92%8C-classmethod%2F</url>
    <content type="text"><![CDATA[Python其实有3个方法,即静态方法(staticmethod),类方法(classmethod)和实例方法,如下: 12345678910111213141516def foo(x): print "executing foo(%s)"%(x)class A(object): def foo(self,x): print "executing foo(%s,%s)"%(self,x) @classmethod def class_foo(cls,x): print "executing class_foo(%s,%s)"%(cls,x) @staticmethod def static_foo(x): print "executing static_foo(%s)"%xa=A() 这里先理解下函数参数里面的self和cls。这个self和cls是对类或者实例的绑定,对于一般的函数来说我们可以这么调用foo(x),这个函数就是最常用的,它的工作跟任何东西(类,实例)无关。对于实例方法,我们知道在类里每次定义方法的时候都需要绑定这个实例,就是foo(self, x),为什么要这么做呢?因为实例方法的调用离不开实例,我们需要把实例自己传给函数,调用的时候是这样的a.foo(x)(其实是foo(a, x))。类方法一样,只不过它传递的是类而不是实例,A.class_foo(x)。注意这里的self和cls可以替换别的参数,但是python的约定是这俩,还是不要改的好。 对于静态方法其实和普通的方法一样,不需要对谁进行绑定,唯一的区别是调用的时候需要使用a.static_foo(x)或者A.static_foo(x)来调用. \ 实例方法 类方法 静态方法 a = A() a.foo(x) a.class_foo(x) a.static_foo(x) A 不可用 A.class_foo(x) A.static_foo(x) 更多关于这个问题: http://stackoverflow.com/questions/136097/what-is-the-difference-between-staticmethod-and-classmethod-in-python https://realpython.com/blog/python/instance-class-and-static-methods-demystified/]]></content>
      <categories>
        <category>Python3 进阶</category>
        <category>Staticmethod &amp; Classmethod</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo博客Next主题添加Fork me on GitHub标签]]></title>
    <url>%2F2018%2F02%2F28%2FHexo%E5%8D%9A%E5%AE%A2Next%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0Fork-me-on-GitHub%E6%A0%87%E7%AD%BE%2F</url>
    <content type="text"><![CDATA[给自己的个人博客添加Fork me on GitHub标签感觉很专业很逼格，添加的方法也很简单，介绍如下。 打开文件：hexo博客根目录/themes/next/layout/_layout.swig 找到如下代码块12345...&lt;div class="&#123;&#123; container_class &#125;&#125; &#123;% block page_class %&#125;&#123;% endblock %&#125; "&gt; &lt;div class="headband"&gt;&lt;/div&gt; [样式代码]... 样式代码 看这里 ，挑选自己喜欢的样式。 然后将样式代码添加到上述 _layout.swig 代码块后面，比如选择黑色经典款，即：1234567...&lt;div class="&#123;&#123; container_class &#125;&#125; &#123;% block page_class %&#125;&#123;% endblock %&#125; "&gt; &lt;div class="headband"&gt;&lt;/div&gt; # [样式代码] &lt;a href="https://github.com/you"&gt;&lt;img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"&gt;&lt;/a&gt;... 重新部署一下就可以查看了，如果显示不出来，需要清理浏览器的cookie,多刷新几次就OK了。大家看我的，感觉很搭( ⊙ o ⊙ )！]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo部署的网站项目(.deploy_git)中添加README.md]]></title>
    <url>%2F2018%2F02%2F28%2FHexo%E9%83%A8%E7%BD%B2%E7%9A%84%E7%BD%91%E7%AB%99%E9%A1%B9%E7%9B%AE-deploy-git-%E4%B8%AD%E6%B7%BB%E5%8A%A0README-md%2F</url>
    <content type="text"><![CDATA[终端中执行hexo generate时，会将source文件夹中的.md文件渲染为.html文件到public文件夹中，所以我们可以将README.md文件放到source文件夹中，这样在执行hexo deploy时，生成的.deploy_git文件夹中就会有README文件。但此时并不是我们想要的README.md。需要注意的是，我们要防止此.md文件被渲染为.html文件，因此，需要在站点配置文件_config.yml中设置skip_render: README.md，这样部署完成后我们就可以在配置的.deploy_git中看到README.md了。 配置截图如下：]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始学爬虫-05]]></title>
    <url>%2F2018%2F02%2F28%2F%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E7%88%AC%E8%99%AB-05%2F</url>
    <content type="text"><![CDATA[为什么要使用CookieCookie，指某些网站为了辨别用户身份、进行session跟踪而储存在用户本地终端上的数据（通常经过加密)。比如说有些网站需要登录后才能访问某个页面，在登录之前，你想抓取某个页面内容，登陆前与登陆后是不同的，或者不允许的。使用Cookie和使用代理IP一样，也需要创建一个自己的opener。在HTTP包中，提供了cookiejar模块，用于提供对Cookie的支持。 http.cookiejar功能强大，我们可以利用本模块的CookieJar类的对象来捕获cookie并在后续连接请求时重新发送，比如可以实现模拟登录功能。该模块主要的对象有CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar。 它们的关系： CookieJar–派生–&gt;FileCookieJar–派生–&gt;MozillaCookieJar和LWPCookieJar 工作原理： 创建一个带有cookie的opener，在访问登录的URL时，将登录后的cookie保存下来，然后利用这个cookie来访问其他网址。查看登录之后才能看到的信息。 同样，我们以实例进行讲解，爬取伯乐在线的面向对象的漂亮MM的邮箱联系方式。 实战(1).背景介绍在伯乐在线有这么一个有趣的模块，面向对象，它说白了就是提供了一个程序员(媛)网上相亲的平台。 URL: http://date.jobbole.com/ 它的样子是这样的： 可以看到，这里有很多的相亲贴，随便点进去就会有网上相亲MM的详细信息，想获取MM的联系方式，需要积分，积分可以通过签到的方式获取。如果没有登陆账户，获取联系方式的地方是这个样子的： 如果登陆了账号，获取联系方式的地方是这个样子的： 想要爬取MM的联系邮箱，就需要用到我们本次讲到的知识，Cookie的使用。当然，首先你积分也得够。 在讲解之前，推荐一款抓包工具–Fiddler，可以在Google Chrome的Google商店下载这个插件，它的样子是这样的： 可以看到，通过这个插件，我们可以很容易找到Post的Form Data等信息，很方便，当然也可以用之前讲得浏览器审查元素的方式查看这些信息。 (2).过程分析在伯乐在线首页点击登陆的按钮，Fiddler的抓包内容如下： 从上图可以看出，真正请求的url是 http://www.jobbole.com/wp-admin/admin-ajax.php Form Data的内容记住，这些是我们编程需要用到的。user_login是用户名，user_pass是用户密码。 在点击取得联系邮箱按钮的时候，Fiddler的抓包内容如下： 从上图可以看出，此刻真正请求的url是 http://date.jobbole.com/wp-admin/admin-ajax.php 同样Form Data中内容要记下来。postId是每个帖子的id。例如，打开一个相亲贴，它的URL是http://date.jobbole.com/4128/，那么它的这个postId就是4128。为了简化程序，这里就不讲解如何自动获取这个postId了，本实例直接指定postId。如果想要自动获取，可以使用beautifulsoup解析http://date.jobbole.com/返回的信息。beautifulsoup的使用。有机会的话，会在后面的爬虫笔记中进行讲解。 (3).测试1)将Cookie保存到变量中 首先，我们先利用CookieJar对象实现获取cookie的功能，存储到变量中，先来感受一下,执行文件cookie01.py： 1234567891011121314151617# -*- coding: UTF-8 -*-from urllib import requestfrom http import cookiejarif __name__ == '__main__': #声明一个CookieJar对象实例来保存cookie cookie = cookiejar.CookieJar() #利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler handler=request.HTTPCookieProcessor(cookie) #通过CookieHandler创建opener opener = request.build_opener(handler) #此处的open方法打开网页 response = opener.open('http://www.baidu.com') #打印cookie信息 for item in cookie: print('Name = %s' % item.name) print('Value = %s' % item.value) 我们使用以上方法将cookie保存到变量中，然后打印出了cookie中的值，运行结果如下: 2)保存Cookie到文件 在上面的方法中，我们将cookie保存到了cookie这个变量中，如果我们想将cookie保存到文件中该怎么做呢？方便以后直接读取文件使用，这时，我们就要用到FileCookieJar这个对象了，在这里我们使用它的子类MozillaCookieJar来实现Cookie的保存，编写代码如下,文件为cookie02.py：123456789101112131415161718# -*- coding: UTF-8 -*-from urllib import requestfrom http import cookiejarif __name__ == '__main__': #设置保存cookie的文件，同级目录下的cookie.txt filename = 'cookie.txt' #声明一个MozillaCookieJar对象实例来保存cookie，之后写入文件 cookie = cookiejar.MozillaCookieJar(filename) #利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler handler=request.HTTPCookieProcessor(cookie) #通过CookieHandler创建opener opener = request.build_opener(handler) #此处的open方法打开网页 response = opener.open('http://www.baidu.com') #保存cookie到文件 cookie.save(ignore_discard=True, ignore_expires=True) cookie.save的参数说明： ignore_discard的意思是即使cookies将被丢弃也将它保存下来； ignore_expires的意思是如果在该文件中cookies已经存在，则覆盖原文件写入。 在这里，我们将这两个全部设置为True。 运行之后，cookies将被保存到cookie.txt文件中。我们可以查看自己查看下cookie.txt这个文件的内容。 3)从文件中获取Cookie并访问 我们已经做到把Cookie保存到文件中了，如果以后想使用，可以利用下面的方法来读取cookie并访问网站，感受一下cookie03.py：12345678910111213141516171819# -*- coding: UTF-8 -*-from urllib import requestfrom http import cookiejarif __name__ == '__main__': #设置保存cookie的文件的文件名,相对路径,也就是同级目录下 filename = 'cookie.txt' #创建MozillaCookieJar实例对象 cookie = cookiejar.MozillaCookieJar() #从文件中读取cookie内容到变量 cookie.load(filename, ignore_discard=True, ignore_expires=True) #利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler handler=request.HTTPCookieProcessor(cookie) #通过CookieHandler创建opener opener = request.build_opener(handler) #此用opener的open方法打开网页 response = opener.open('http://www.baidu.com') #打印信息 print(response.read().decode('utf-8')) 了解到以上内容，我们那就可以开始正式编写模拟登陆伯乐在线的程序了。同时，我们也可以获取相亲MM的联系方式。 (4).编写代码我们利用CookieJar对象实现获取cookie的功能，存储到变量中。然后使用这个cookie变量创建opener，使用这个设置好cookie的opener即可模拟登陆，同笔记四中讲到的IP代理的使用方法类似。 创建cookie_test.py文件，编写代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# -*- coding: UTF-8 -*-from urllib import requestfrom urllib import errorfrom urllib import parsefrom http import cookiejarif __name__ == '__main__': #登陆地址 login_url = 'http://www.jobbole.com/wp-admin/admin-ajax.php' #User-Agent信息 user_agent = r'Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36' #Headers信息 head = &#123;'User-Agnet': user_agent, 'Connection': 'keep-alive'&#125; #登陆Form_Data信息 Login_Data = &#123;&#125; Login_Data['action'] = 'user_login' Login_Data['redirect_url'] = 'http://www.jobbole.com/' Login_Data['remember_me'] = '0' #是否一个月内自动登陆 Login_Data['user_login'] = '********' #改成你自己的用户名 Login_Data['user_pass'] = '********' #改成你自己的密码 #使用urlencode方法转换标准格式 logingpostdata = parse.urlencode(Login_Data).encode('utf-8') #声明一个CookieJar对象实例来保存cookie cookie = cookiejar.CookieJar() #利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler cookie_support = request.HTTPCookieProcessor(cookie) #通过CookieHandler创建opener opener = request.build_opener(cookie_support) #创建Request对象 req1 = request.Request(url=login_url, data=logingpostdata, headers=head) #面向对象地址 date_url = 'http://date.jobbole.com/wp-admin/admin-ajax.php' #面向对象 Date_Data = &#123;&#125; Date_Data['action'] = 'get_date_contact' Date_Data['postId'] = '4128' #使用urlencode方法转换标准格式 datepostdata = parse.urlencode(Date_Data).encode('utf-8') req2 = request.Request(url=date_url, data=datepostdata, headers=head) try: #使用自己创建的opener的open方法 response1 = opener.open(req1) response2 = opener.open(req2) html = response2.read().decode('utf-8') index = html.find('jb_contact_email') #打印查询结果 print('联系邮箱:%s' % html[index+19:-2]) except error.URLError as e: if hasattr(e, 'code'): print("HTTPError:%d" % e.code) elif hasattr(e, 'reason'): print("URLError:%s" % e.reason) (5).运行结果如下。]]></content>
      <categories>
        <category>Spider</category>
        <category>Urllib</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Spider</tag>
        <tag>Urllib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始学爬虫-04]]></title>
    <url>%2F2018%2F02%2F28%2F%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E7%88%AC%E8%99%AB-04%2F</url>
    <content type="text"><![CDATA[说在前面urllib_test10.py已经无法爬取了，因为原网站已经添加了防爬虫机制 (一)、为何要设置User Agent有一些网站不喜欢被爬虫程序访问，所以会检测连接对象，如果是爬虫程序，也就是非人点击访问，它就会不让你继续访问，所以为了要让程序可以正常运行，需要隐藏自己的爬虫程序的身份。此时，我们就可以通过设置User Agent的来达到隐藏身份的目的，User Agent的中文名为用户代理，简称UA。 User Agent存放于Headers中，服务器就是通过查看Headers中的User Agent来判断是谁在访问。在Python中，如果不设置User Agent，程序将使用默认的参数，那么这个User Agent就会有Python的字样，如果服务器检查User Agent，那么没有设置User Agent的Python程序将无法正常访问网站。 Python允许我们修改这个User Agent来模拟浏览器访问，它的强大毋庸置疑。 (二)、常见的User Agent(1).Android Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19 Mozilla/5.0 (Linux; U; Android 4.0.4; en-gb; GT-I9300 Build/IMM76D) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30 Mozilla/5.0 (Linux; U; Android 2.2; en-gb; GT-P1000 Build/FROYO) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1 (2).Firefox Mozilla/5.0 (Windows NT 6.2; WOW64; rv:21.0) Gecko/20100101 Firefox/21.0 Mozilla/5.0 (Android; Mobile; rv:14.0) Gecko/14.0 Firefox/14.0 (3).Google Chrome Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36 Mozilla/5.0 (Linux; Android 4.0.4; Galaxy Nexus Build/IMM76B) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.133 Mobile Safari/535.19 (4).iOS Mozilla/5.0 (iPad; CPU OS 5_0 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9A334 Safari/7534.48.3 Mozilla/5.0 (iPod; U; CPU like Mac OS X; en) AppleWebKit/420.1 (KHTML, like Gecko) Version/3.0 Mobile/3A101a Safari/419.3 上面列举了Andriod、Firefox、Google Chrome、iOS的一些User Agent，直接copy就能用。 (三)、设置User Agent的方法先看下urllib.request.Request() 从上图可以看出，在创建Request对象的时候，可以传入headers参数。因此，想要设置User Agent，有两种方法： 1.在创建Request对象的时候，填入headers参数(包含User Agent信息)，这个Headers参数要求为字典； 2.在创建Request对象的时候不添加headers参数，在创建完成之后，使用add_header()的方法，添加headers。 方法一创建文件urllib_test08.py，使用上面提到的Android的第一个User Agent，在创建Request对象的时候传入headers参数，编写代码如下：12345678910111213141516# -*- coding: UTF-8 -*-from urllib import requestif __name__ == "__main__": url = 'http://www.csdn.net/' head = &#123;&#125; #写入User Agent信息 head['User-Agent'] = 'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19' #创建Request对象 req = request.Request(url, headers=head) #传入创建好的Request对象 response = request.urlopen(req) #读取响应信息并解码 html = response.read().decode('utf-8') #打印信息 print(html) 运行结果如下： 方法二创建文件urllib_test09.py，使用上面提到的Android的第一个User Agent，在创建Request对象时不传入headers参数，创建之后使用add_header()方法，添加headers，编写代码如下：12345678910111213141516# -*- coding: UTF-8 -*-from urllib import requestif __name__ == "__main__": #以CSDN为例，CSDN不更改User Agent是无法访问的 url = 'http://www.csdn.net/' #创建Request对象 req = request.Request(url) #传入headers req.add_header('User-Agent', 'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19') #传入创建好的Request对象 response = request.urlopen(req) #读取响应信息并解码 html = response.read().decode('utf-8') #打印信息 print(html) 运行结果和上一个方法是一样的。 (四)、IP代理的使用(1).为何使用IP代理User Agent已经设置好了，但是还应该考虑一个问题，程序的运行速度是很快的，如果我们利用一个爬虫程序在网站爬取东西，一个固定IP的访问频率就会很高，这不符合人为操作的标准，因为人操作不可能在几ms内，进行如此频繁的访问。所以一些网站会设置一个IP访问频率的阈值，如果一个IP访问频率超过这个阈值，说明这个不是人在访问，而是一个爬虫程序。 (2).一般步骤说明一个很简单的解决办法就是设置延时，但是这显然不符合爬虫快速爬取信息的目的，所以另一种更好的方法就是使用IP代理。使用代理的步骤： (1) 调用urlib.request.ProxyHandler()，proxies参数为一个字典。 (2) 创建Opener(类似于urlopen，这个代开方式是我们自己定制的) (3) 安装Opener 使用install_opener方法之后，会将程序默认的urlopen方法替换掉。也就是说，如果使用install_opener之后，在该文件中，再次调用urlopen会使用自己创建好的opener。如果不想替换掉，只是想临时使用一下，可以使用opener.open(url)，这样就不会对程序默认的urlopen有影响。 (3).代理IP选取在写代码之前，先在代理IP网站选好一个IP地址，推荐西刺代理IP。 URL：http://www.xicidaili.com/ 注意：当然也可以写个正则表达式从网站直接爬取IP，但是要记住不要太频繁爬取，加个延时什么的，太频繁给服务器带来压力了，服务器会直接把你block，不让你访问的，我就被封了两天。 从西刺网站选出信号好的IP，我的选择如下：(106.46.136.112:808) 编写代码访问http://www.whatismyip.com.tw/，该网站是测试自己IP为多少的网址，服务器会返回访问者的IP。 (4).代码实例创建文件urllib_test10.py，编写代码如下：12345678910111213141516171819202122# -*- coding: UTF-8 -*-from urllib import requestif __name__ == "__main__": #访问网址 url = 'http://www.whatismyip.com.tw/' #这是代理IP proxy = &#123;'http':'106.46.136.112:808'&#125; #创建ProxyHandler proxy_support = request.ProxyHandler(proxy) #创建Opener opener = request.build_opener(proxy_support) #添加User Angent opener.addheaders = [('User-Agent','Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36')] #安装OPener request.install_opener(opener) #使用自己安装好的Opener response = request.urlopen(url) #读取相应信息并解码 html = response.read().decode("utf-8") #打印信息 print(html) 运行结果如下： 从上图可以看出，访问的IP已经伪装成了106.46.136.112。]]></content>
      <categories>
        <category>Spider</category>
        <category>Urllib</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Spider</tag>
        <tag>Urllib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始学爬虫-03]]></title>
    <url>%2F2018%2F02%2F28%2F%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E7%88%AC%E8%99%AB-03%2F</url>
    <content type="text"><![CDATA[urllib.errorurllib.error可以接收有urllib.request产生的异常。urllib.error有两个方法，URLError和HTTPError。如下图所示： URLError是OSError的一个子类，HTTPError是URLError的一个子类，服务器上HTTP的响应会返回一个状态码，根据这个HTTP状态码，我们可以知道我们的访问是否成功。例如第二个笔记中提到的200状态码，表示请求成功，再比如常见的404错误等。 (1).URLError让我们先看下URLError的异常，创建文件urllib_test05.py，编写如下代码：1234567891011121314# -*- coding: UTF-8 -*-from urllib import requestfrom urllib import errorif __name__ == "__main__": #一个不存在的连接 url = "http://www.dskfclyfiydl.com/" req = request.Request(url) try: response = request.urlopen(req) html = response.read().decode('utf-8') print(html) except error.URLError as e: print(e.reason) 可以看到如下运行结果： (2).HTTPError再看下HTTPError异常，创建文件urllib_test06.py，编写如下代码：12345678910111213# -*- coding: UTF-8 -*-from urllib import requestfrom urllib import errorif __name__ == "__main__": #一个不存在的连接 url = "http://www.douyu.com/wkx.html" req = request.Request(url) try: responese = request.urlopen(req) # html = responese.read() except error.HTTPError as e: print(e.code, '\n' ,e.reason, '\n', e.headers) 运行之后，我们可以看到404，这说明请求的资源没有在服务器上找到，www.douyu.com 这个服务器是存在的，但是我们要查找的 Jack_Cui.html 资源是没有的，所以抛出404异常。 (3).URLError和HTTPError混合使用最后值得注意的一点是，如果想用HTTPError和URLError一起捕获异常，那么需要将HTTPError放在URLError的前面，因为HTTPError是URLError的一个子类。如果URLError放在前面，出现HTTP异常会先响应URLError，这样HTTPError就捕获不到错误信息了。 如果不用上面的方法，也可以使用hasattr函数判断URLError含有的属性，如果含有reason属性表明是URLError，如果含有code属性表明是HTTPError。创建文件urllib_test07.py，编写代码如下：1234567891011121314151617# -*- coding: UTF-8 -*-from urllib import requestfrom urllib import errorif __name__ == "__main__": #一个不存在的连接 url = "http://www.douyu.com/wkx.html" req = request.Request(url) try: responese = request.urlopen(req) except error.URLError as e: if hasattr(e, 'code'): print("HTTPError") print(e.code) elif hasattr(e, 'reason'): print("URLError") print(e.reason) 运行结果如下：]]></content>
      <categories>
        <category>Spider</category>
        <category>Urllib</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Spider</tag>
        <tag>Urllib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始学爬虫-02]]></title>
    <url>%2F2018%2F02%2F28%2F%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E7%88%AC%E8%99%AB-02%2F</url>
    <content type="text"><![CDATA[一个疑问尚未解决疑问，小弟在此跪求大牛解答一下为什么把url里的 “_o” 删掉后就可以正常爬取呢？ urlopen的url参数 Agenturl不仅可以是一个字符串，例如:http://www.baidu.com。 url也可以是一个Request对象，这就需要我们先定义一个Request对象，然后将这个Request对象作为urlopen的参数使用，方法如下： 123456789# -*- coding: UTF-8 -*-from urllib import requestif __name__ == "__main__": req = request.Request("http://fanyi.baidu.com/") response = request.urlopen(req) html = response.read() html = html.decode("utf-8") print(html) 同样，运行这段代码同样可以得到网页信息。可以看一下这段代码和上个笔记中代码的不同，对比一下就明白了。 urlopen()返回的对象，可以使用read()进行读取，同样也可以使用geturl()方法、info()方法、getcode()方法。 geturl()返回的是一个url的字符串； info()返回的是一些meta标记的元信息，包括一些服务器的信息； getcode()返回的是HTTP的状态码，如果返回200表示请求成功。 关于META标签和HTTP状态码的内容可以自行百度百科，里面有很详细的介绍。 了解到这些，我们就可以进行新一轮的测试，新建文件名urllib_test04.py，编写如下代码：1234567891011# -*- coding: UTF-8 -*-from urllib import requestif __name__ == "__main__": req = request.Request("http://fanyi.baidu.com/") response = request.urlopen(req) print("geturl打印信息：%s"%(response.geturl())) print('**********************************************') print("info打印信息：%s"%(response.info())) print('**********************************************') print("getcode打印信息：%s"%(response.getcode())) urlopen的data参数我们可以使用data参数，向服务器发送数据。根据HTTP规范，GET用于信息获取，POST是向服务器提交数据的一种请求，再换句话说： 从客户端向服务器提交数据使用POST； 从服务器获得数据到客户端使用GET(GET也可以提交，暂不考虑)。 如果没有设置urlopen()函数的data参数，HTTP请求采用GET方式，也就是我们从服务器获取信息，如果我们设置data参数，HTTP请求采用POST方式，也就是我们向服务器传递数据。 data参数有自己的格式，它是一个基于application/x-www.form-urlencoded的格式，具体格式我们不用了解， 因为我们可以使用urllib.parse.urlencode()函数将字符串自动转换成上面所说的格式。 发送data实例向有道翻译发送data，得到翻译结果。 (1).打开有道翻译界面，如下图所示： (2).鼠标右键检查，也就是审查元素，如下图所示： (3).选择右侧出现的Network，如下图所示： (4).在左侧输入翻译内容，输入Jack，如下图所示： (5).点击自动翻译按钮，我们就可以看到右侧出现的内容，如下图所示： (6).点击上图红框中的内容，查看它的信息，如下图所示： (7).记住这些信息，这是我们一会儿写程序需要用到的。 新建文件translate_test.py，编写如下代码： 1234567891011121314151617181920212223242526272829# -*- coding: UTF-8 -*-from urllib import requestfrom urllib import parseimport jsonif __name__ == "__main__": #对应上图的Request URL Request_URL = 'http://fanyi.youdao.com/translate?smartresult=dict&amp;smartresult=rule&amp;sessionFrom=null' #创建Form_Data字典，存储上图的Form Data Form_Data = &#123;&#125; Form_Data['type'] = 'AUTO' Form_Data['i'] = 'Jack' Form_Data['doctype'] = 'json' Form_Data['xmlVersion'] = '1.8' Form_Data['keyfrom'] = 'fanyi.web' Form_Data['ue'] = 'ue:UTF-8' Form_Data['action'] = 'FY_BY_CLICKBUTTON' #使用urlencode方法转换标准格式 data = parse.urlencode(Form_Data).encode('utf-8') #传递Request对象和转换完格式的数据 response = request.urlopen(Request_URL,data) #读取信息并解码 html = response.read().decode('utf-8') #使用JSON translate_results = json.loads(html) #找到翻译结果 translate_results = translate_results['translateResult'][0][0]['tgt'] #打印翻译信息 print("翻译的结果是：%s" % translate_results) 运行查看翻译结果]]></content>
      <categories>
        <category>Spider</category>
        <category>Urllib</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Spider</tag>
        <tag>Urllib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始学爬虫-01]]></title>
    <url>%2F2018%2F02%2F28%2F%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E7%88%AC%E8%99%AB-01%2F</url>
    <content type="text"><![CDATA[本节关键字urllib | chardet urllib 简介在Python3.x中，我们可以使用urlib这个组件抓取网页，urllib是一个URL处理包，这个包中集合了一些处理URL的模块，如下： 1.urllib.request模块是用来打开和读取URLs的； 2.urllib.error模块包含一些有urllib.request产生的错误，可以使用try进行捕捉处理； 3.urllib.parse模块包含了一些解析URLs的方法； 4.urllib.robotparser模块用来解析robots.txt文本文件.它提供了一个单独的RobotFileParser类，通过该类提供的can_fetch()方法测试爬虫是否可以下载一个页面。 使用urllib.request.urlopen()这个接口函数就可以访问一个网站，读取并打印信息。urlopen有一些可选参数，具体信息可以查阅Python自带的documentation。 urllib 测试了解到这些，我们就可以写一个最简单的程序，文件名为urllib_test01.py，感受一个urllib库的魅力：1234567# -*- coding: UTF-8 -*-from urllib import requestif __name__ == "__main__": response = request.urlopen("http://fanyi.baidu.com") html = response.read() print(html) urllib使用使用request.urlopen()访问和读取URLs信息，返回的对象response如同一个文本对象，我们可以调用read()，进行读取。再通过print()屏幕打印。 浏览器就是作为客户端从服务器端获取信息，然后将信息解析，再展示给我们的。但是显然他们都是二进制的乱码。 我们可以通过简单的decode()命令将网页的信息进行解码，并显示出来，我们新创建一个文件，命名为urllib_test02.py，编写如下代码： 12345678# -*- coding: UTF-8 -*-from urllib import requestif __name__ == "__main__": response = request.urlopen("http://fanyi.baidu.com/") html = response.read() html = html.decode("utf-8") print(html) 这样我们就可以得到这样的结果，显然解码后的信息看起来工整和舒服多了： 当然这个前提是我们已经知道了这个网页是使用utf-8编码的，怎么查看网页的编码方式呢？非常简单的方法是使用使用浏览器审查元素，只需要找到head标签开始位置的chareset，就知道网页是采用何种编码。 这样我们就知道了这个网站的编码方式，但是这需要我们每次都打开浏览器，并找下编码方式，显然有些费事，使用几行代码解决更加省事并且显得酷一些。 我们需要安装第三方库chardet，它是用来判断编码的模块。安装好后，我们就可以使用chardet.detect()方法，判断网页的编码方式了。至此，我们就可以编写一个小程序判断网页的编码方式了，新建文件名为chardet_test01.py：123456789# -*- coding: UTF-8 -*-from urllib import requestimport chardetif __name__ == "__main__": response = request.urlopen("http://fanyi.baidu.com") html = response.read() charset = chardet.detect(html) print(charset)]]></content>
      <categories>
        <category>Spider</category>
        <category>Urllib</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Spider</tag>
        <tag>Urllib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一种Git保留两个repo的commit信息进行合并的方法]]></title>
    <url>%2F2018%2F02%2F27%2F%E4%B8%80%E7%A7%8DGit%E4%BF%9D%E7%95%99%E4%B8%A4%E4%B8%AArepo%E7%9A%84commit%E4%BF%A1%E6%81%AF%E8%BF%9B%E8%A1%8C%E5%90%88%E5%B9%B6%E7%9A%84%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[以往的合并时首先要删除repo的.git文件夹，然后重新add-commit-push。带来的问题是会丢失某一个仓库的提交信息，不利于时光倒退。经过摸索终于实现了保留两个仓库提交信息的合并方法。介绍如下： 比如要将DownloadPicsBySeleniumAndPhantomJS这个项目合并到Web-Spider中，终端中执行：12345$ git remote add other git@github.com:ByiProX/****.git$ git fetch other$ git checkout -b repo1 other/mster$ git checkout master$ git merge repo1 --allow-unrelated-histories 接下来解决merge冲突即可（可以尝试使用mergetool），如有需要可以删除多余分支和远程连接 1$ git mergetool # 解决merge冲突 123$ git remote rm other # 删除远程连接 $ git branch -d repo1 # 删除分支操作]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 使用Selenium&PhantomJS爬火影忍者漫画]]></title>
    <url>%2F2018%2F02%2F27%2FPython3-%E4%BD%BF%E7%94%A8Selenium-PhantomJS%E7%88%AC%E7%81%AB%E5%BD%B1%E5%BF%8D%E8%80%85%E6%BC%AB%E7%94%BB%2F</url>
    <content type="text"><![CDATA[近期学习爬虫，发现懂的越多，不懂的知识点越多（所以当个傻子还是很幸福的）。好记性不如烂笔头，之前都是把看到的资料链接直接挂到一些平台，比如知乎、简书、Github等。今天有点时间，就好好码一下字，排排版，方便以后查阅。 Selenium用来模拟浏览器的行为，比如点击、最大化、滚动窗口等；PhantomJS是一种浏览器，不过这种浏览器没有UI界面，感觉就像是专门为爬虫设计，优点很明显，可以有效减小内存的使用。 爬虫使用到的模块12345from selenium import webdriverfrom myLogging import MyLoggingimport osimport timeimport re myLogging模块是自己配置的日志包，想要的可以点击这里自己看 爬虫很关键的一点就是能够看懂网页的源代码，记得当初刚刚真正开始接触编程的时候，有很长的一段时间在看HTML、CSS、JS的一些知识，虽然忘得很多，但是印象还是有的，对于后面看网页源代码很有帮助。学习爬虫，除了会基本的python知识以外，还要会网页的一些知识。 爬取图片思路： 已知连接，分析网页的代码结构，看所需的数据是否需要切换frame，并定位所需数据的位于哪个标签之下 采用不同的模块有不同的保存图片方式，如果采用request模块，保存图片方式是可以采用requests.get(comicUrl).content方法，使用该方法需要确定网页的地址。该项目中没有涉及request的使用，所以此后不再表述。对于selenium可以使用 get_screenshot_as_file() 方法，使用该方法强烈建议使用phantomjs，如果使用chrome浏览器，图片尺寸太大的话，会出现截图不完整，对比如下： 找到下一张图片的连接位置并点击更新网页，一般来讲新网页与之前网页结构相同。 在新网页的基础上保存图片，设置循环如此反复。 爬取网页的URL为：爬取火影漫画第一话 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class DownloadPics(object): def __init__(self, url): self.url = url self.log = MyLogging() self.browser = self.get_browser() self.save_pics(self.browser) def get_browser(self): browser = webdriver.PhantomJS() try: browser.get(self.url) except: MyLogging.error('open the url %s failed' % self.url) browser.implicitly_wait(20) return browser def save_pics(self, browser): pics_title = browser.title.split('_')[0] self.create_dir(pics_title) os.chdir(pics_title) sum_page = self.find_total_page_num(browser) i = 1 while i &lt; sum_page: image_name = str(i) + '.png' browser.get_screenshot_as_file(image_name) # 使用PhantomJS避免了截图的不完整，可以与Chrome比较 self.log.info('saving image %s' % image_name) i += 1 css_selector = "a[href='/comiclist/3/3/%s.htm']" % i # 该方法感觉还不错呢，不过这个网站确实挺差劲的 next_page = browser.find_element_by_css_selector(css_selector) next_page.click() time.sleep(2) # browser.implicitly_wait(20) def find_total_page_num(self, browser): page_element = browser.find_element_by_css_selector("table[cellspacing='1']") num = re.search(r'共\d+页', page_element.text).group()[1:-1] return int(num) def create_dir(self, dir_name): if os.path.exists(dir_name): self.log.error('create directory %s failed cause a same directory exists' % dir_name) else: try: os.makedirs(dir_name) except: self.log.error('create directory %s failed' % dir_name) else: self.log.info('create directory %s success' % dir_name)if __name__ == '__main__': start_url = 'http://comic.kukudm.com/comiclist/3/3/1.htm' DL = DownloadPics(start_url) 运行结果]]></content>
      <categories>
        <category>Spider</category>
        <category>Selenium</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Spider</tag>
        <tag>Selenium</tag>
        <tag>PhantomJS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django2.0.1搭建电影网站]]></title>
    <url>%2F2018%2F02%2F27%2FDjango2-0-1%E6%90%AD%E5%BB%BA%E7%94%B5%E5%BD%B1%E7%BD%91%E7%AB%99%2F</url>
    <content type="text"><![CDATA[本项目已经部署到服务器，可以通过该IP查看http://59.110.221.56/GitHub源代码 技术栈 Bootstrap 3 + Django 2.0.1 + MySQL 5.7.17 + Nginx + locust 本地服务运行方法终端在venv文件夹路径下开启虚拟环境(根据自己的shell进行选择)： 123source ../venv/bin/activate.fishsource ../venv/bin/activatesource ../venv/bin/activate.csh 然后执行：1python3 TWS_Cinema/manage.py runserver 如果报错，终端进入requirements.txt所在目录，运行命令：1pip3 install -r requirements.txt 然后执行：1python3 TWS_Cinema/manage.py runserver 单元测试运行方法在manage.py路径下终端运行 1python3 manage.py test 网站功能描述 实现导航栏搜索电影，支持按年份搜索和类型搜索 – 显示分类列表 – 点击分类显示符合分类要求的电影 实现搜索功能，支持按电影名称模糊搜索 实现电影详细信息查看功能 – 显示电影详细信息 – 显示豆瓣 Top 5 影评 – 在电影详细页面显示相似电影推荐 – 增加电影观看链接 API 按电影id搜索 —— api/movie/id/ # 例如：api/movie/id/1291545 按电影名搜索 —— api/movie/title/ # 例如：api/movie/title/大鱼 按电影原始名搜索 —— api/movie/original_title/ # 例如：api/movie/original_title/Big Fish 按电影类型搜索 —— api/movie/genre/ # 例如：api/movie/genre/剧情 按电影年份搜索 —— api/movie/year/ # 例如：api/movie/year/2003 网站性能测试结果在文件locustfile.py路径下运行1locust --host=http://59.110.221.56 压力测试 采取的框架：locust 服务器性能： CPU：1核 内存：2 GB (I/O优化) 带宽：1Mbps 测试结果： 500人：100%正确 1000人：40%出错率 测试截图 电影网站的其他截图 ReferenceLocust 简介以及使用]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简谈爬虫攻与防]]></title>
    <url>%2F2018%2F02%2F27%2F%E7%AE%80%E8%B0%88%E7%88%AC%E8%99%AB%E6%94%BB%E4%B8%8E%E9%98%B2%2F</url>
    <content type="text"><![CDATA[封锁间隔时间破解Scrapy在两次请求之间的时间设置是DOWNLOAD_DELAY。如果不考虑反爬虫的因素，该值当然越小越好。如果DOWNLOAD_DELAY设为0.001，也就是每1毫秒请求一次网页，这简直非人类干的事情。有些网站会检测一个ip的访问时间，异常情况下会封锁该ip。 封锁Cookies众所周知，网站是通过Cookie来确定用户身份的，Scrapy在爬取数据时使用同一个Cookies发起请求。该做法和把DOWNLOAD_DELAY设为0.001没有本质区别。在scrapy中，直接设禁用Cookies就可以了。在settings.py中设置12# Disable cookies (enabled by default)COOKIES_ENABLED = False 封锁user-agent和proxy破解user-agent是浏览器的身份标识。网站通过UA来确定浏览器类型。很多浏览器拒绝不符合一定标准的UA请求网页。同一个UA高频率的访问网站会有被网站列入黑名单的危险。破解的方法很简单，可以准备一个UA池，每次请求时随机挑选一个进行请求。 在middlewares.py同级目录下创建UAResource.py,文件内容如下： 12345678910111213141516171819202122232425#-*- coding: utf-8 -*-UserAgents = [ "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)", "Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)", "Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)", "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)", "Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)", "Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)", "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)", "Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6", "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1", "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0", "Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5", "Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11", "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20", "Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52",]Proxies = ['http://122.114.31.177:808','http://1.2.3.4:80',] 修改middlewares.py，添加内容为1234567891011121314from .UAResource import UserAgentsfrom .UAResource import Proxiesimport randomclass RandomProxy(object): def process_request(self, request, spider): proxy = random.choice(Proxies) request.meta['proxy'] = proxyclass RandomUserAgent(object): """docstring for RandomUerAgent.""" def process_request(self, request, spider): ua = random.choice(UserAgents) request.headers.setdefault('User-Agent', ua) 最后修改setting.py,将RandomUserAgent和RandomProxy添加到DOWNLOADER_MIDDLEWARES中，注意RandomProxy要放到RandomUserAgent之前，即将RandomProxy的值比RandomUserAgent后的值小12345678DOWNLOADER_MIDDLEWARES = &#123; # 'meijutt.middlewares.MeijuttDownloaderMiddleware': 543, 'meijutt.middlewares.RandomProxy': 10, 'meijutt.middlewares.RandomUserAgent': 30, # 禁止内置的(在 DOWNLOADER_MIDDLEWARES_BASE 中设置并默认启用的)中间件 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,&#125; 免费代理不够稳定，如果不想用proxy，设置RandomProxy为None,即禁止使用1'meijutt.middlewares.RandomProxy': None, 补充Referer当浏览器发送请求时，请求头header一般都会带上这个，这个可以让网站管理者知道我是通过哪个链接访问到这个网站的，上面就说明我是从网易云音乐的主页来访问到这个页面的，若你是用python来直接请求是，就没有访问来源，那么管理者就轻而易举地判断你是机器在操作。对于scrapy爬虫123456# Override the default request headers:DEFAULT_REQUEST_HEADERS = &#123; # 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', # 'Accept-Language': 'en', 'Referer':'http://artso.artron.net/auction/search_auction.php?keyword=%E8%B1%A1%E7%89%99&amp;page=' + str(randrange(100)),&#125; authorization:有的网站还会有这个请求头，这个是在用户在访问该网站的时候就会分配一个id给用户，然后在后台验证该id有没有访问权限从而来进行发爬虫。]]></content>
      <categories>
        <category>Spider</category>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Spider</tag>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Github多分支管理Hexo-Blog项目]]></title>
    <url>%2F2018%2F02%2F27%2FGithub%E5%A4%9A%E5%88%86%E6%94%AF%E7%AE%A1%E7%90%86Hexo-Blog%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[Hexo在部署之后在github的仓库中我们只能找到生成的静态文件。然而博客的源文件：主题、文章、配置等文件都还在本地，并没有备份。对于多台终端设备的用户不够友好，而且存在一定的风险，万一那天电脑坏了或者是出现一些其他问题，就得从头再来。为了解决上述问题，我们可以利用github的分支思想来备份我们的源文件。 备份之前，需要了解博客根目录下面的文件以及文件夹作用：123456789.deploy_git/ 网站静态文件(git)node_modules/ 插件public/ 网站静态文件scaffolds/ 文章模板source/ 博文等themes/ 主题_config.yml 网站配置文件package.json Hexo信息db.json 数据文件 备份的思路master分支存放部署生成的静态文件，Hexo-Bog分支存放我们要备份项目源文件。实际备份中，.deploy_git、public文件夹和我们的master分支内容重复，所以略过。因此，我们在根目录下面建一个.gitignore文件来建立“黑名单”，禁止备份。 编辑.gitignore过滤文件文件内容如下：123.DS_Storepublic/.deploy*/ 关于备份终端中在项目的根目录下执行，对于作者自己的项目，命令执行的路径为ByiProX/下：123456$ git init$ git remote add origin git@github.com:username/username.github.io.git # username为博客项目的名称，也就是git的用户名$ git add .$ git commit -m "ready for backup of the project"$ git push origin master:Hexo-Blog 执行完毕后会发现github博客仓库已经有了一个新分支Hexo-Blog，于是备份工作完成。以后，开始写博文时，即终端运行1$ hexo new [layout] &lt;title&gt; 完成文章后,对编辑后的文章进行备份保存，即终端运行,为下面的部署做准备123$ git add .$ git commit -m "add one article"$ git push origin master:Hexo-Blog 部署运行一下命令进行仓库master分支静态文件部署123$ hexo clean$ hexo generate$ hexo deploy 以上完成项目源文件以及静态文件的Git管理 参考文献及进阶Hexo+github搭建个人博客并实现多终端管理如何在github上面备份HexoHexo的版本控制与持续集成使用hexo，如果换了电脑怎么更新博客]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F02%2F26%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. 非常好的一篇markdown参考手册 Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
