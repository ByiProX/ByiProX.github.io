<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python3 is和==的区别]]></title>
    <url>%2F2018%2F03%2F03%2FPython3-is%E5%92%8C-%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[python 三要素要理解Python中is和==的区别，首先要理解Python对象的三个要素: 要素 说明 获取方式 id 身份标识，基本就是内存地址，用来唯一标识一个对象 id(obj) type 数据类型 type(obj) value 值 :—–: is和==区别 标识 名称 判断方法 is 同一性运算符 id == 比较运算符 value 程序举例例1： 12345a = &#123;"a":1, "b":2&#125;b = a.copy()a == b # True value一样a is b # False id不一样 例2： 1234567891011121314&gt;&gt;&gt; x = y = [4,5,6]&gt;&gt;&gt; z = [4,5,6]&gt;&gt;&gt; x == yTrue&gt;&gt;&gt; x == zTrue&gt;&gt;&gt; x is yTrue&gt;&gt;&gt; x is zFalse&gt;&gt;&gt;&gt;&gt;&gt; print id(x)&gt;&gt;&gt; print id(y)&gt;&gt;&gt; print id(z) 123456789101112131415161718192021222324252627282930313233343536&gt;&gt;&gt; a = 1 #a和b为数值类型&gt;&gt;&gt; b = 1&gt;&gt;&gt; a is bTrue&gt;&gt;&gt; id(a)&gt;&gt;&gt; id(b)&gt;&gt;&gt; a = 'cheesezh' #a和b为字符串类型&gt;&gt;&gt; b = 'cheesezh'&gt;&gt;&gt; a is bTrue&gt;&gt;&gt; id(a)&gt;&gt;&gt; id(b)&gt;&gt;&gt; a = (1,2,3) #a和b为元组类型&gt;&gt;&gt; b = (1,2,3)&gt;&gt;&gt; a is bFalse&gt;&gt;&gt; id(a)&gt;&gt;&gt; id(b)&gt;&gt;&gt; a = [1,2,3] #a和b为list类型&gt;&gt;&gt; b = [1,2,3]&gt;&gt;&gt; a is bFalse&gt;&gt;&gt; id(a)&gt;&gt;&gt; id(b)&gt;&gt;&gt; a = &#123;'cheese':1,'zh':2&#125; #a和b为dict类型&gt;&gt;&gt; b = &#123;'cheese':1,'zh':2&#125;&gt;&gt;&gt; a is bFalse&gt;&gt;&gt; id(a)&gt;&gt;&gt; id(b)&gt;&gt;&gt; a = set([1,2,3])#a和b为set类型&gt;&gt;&gt; b = set([1,2,3])&gt;&gt;&gt; a is bFalse&gt;&gt;&gt; id(a)&gt;&gt;&gt; id(b)]]></content>
      <categories>
        <category>Python3 进阶</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 拷贝对象(深拷贝deepcopy和浅拷贝copy)]]></title>
    <url>%2F2018%2F03%2F03%2FPython3-%E6%8B%B7%E8%B4%9D%E5%AF%B9%E8%B1%A1-%E6%B7%B1%E6%8B%B7%E8%B4%9Ddeepcopy%E5%92%8C%E6%B5%85%E6%8B%B7%E8%B4%9Dcopy%2F</url>
    <content type="text"><![CDATA[copy.copy 浅拷贝 只拷贝父对象，不会拷贝对象的内部的子对象。 copy.deepcopy 深拷贝 拷贝对象及其子对象 1234567891011121314151617181920# -*-coding:utf-8 -*-import copya = [1, 2, 3, 4, ['a', 'b']] #原始对象b = a #赋值，传对象的引用c = copy.copy(a) #对象拷贝，浅拷贝d = copy.deepcopy(a) #对象拷贝，深拷贝a.append(5) #修改对象aa[4].append('c') #修改对象a中的['a', 'b']数组对象print 'a = ', aprint 'b = ', bprint 'c = ', cprint 'd = ', d输出结果：a = [1, 2, 3, 4, ['a', 'b', 'c'], 5]b = [1, 2, 3, 4, ['a', 'b', 'c'], 5]c = [1, 2, 3, 4, ['a', 'b', 'c']]d = [1, 2, 3, 4, ['a', 'b']]]]></content>
      <categories>
        <category>Python3 进阶</category>
        <category>Python3 拷贝对象</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 作用域]]></title>
    <url>%2F2018%2F03%2F03%2FPython3-%E4%BD%9C%E7%94%A8%E5%9F%9F%2F</url>
    <content type="text"><![CDATA[Python 中，一个变量的作用域总是由在代码中被赋值的地方所决定的。 Python 获取变量中的值的搜索顺序为： 本地作用域（Local）→ 当前作用域被嵌入的本地作用域（Enclosing locals）→ 全局/模块作用域（Global）→内置作用域（Built-in）]]></content>
      <categories>
        <category>Python3 进阶</category>
        <category>Python3 作用域</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 函数重载]]></title>
    <url>%2F2018%2F03%2F03%2FPython3-%E5%87%BD%E6%95%B0%E9%87%8D%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[函数重载的目的动态语言中，有鸭子类型，如果走起路来像鸭子，叫起来也像鸭子，那么它就是鸭子。一个对象的特征不是由它的类型决定，而是通过对象中的方法决定，所以函数重载在动态语言中就显得没有意义了，因为函数可以通过鸭子类型来处理不同类型的对象，鸭子类型也是多态性的一种表现。 在Python中实现函数重载：123456789101112131415from io import StringIOclass Writer: @staticmethod def write(output, content): # output对象只要实现了write方法就行 output.write(content)# stringIO类型output = StringIO()Writer.write(output, 'hello world')# file 类型output = open('out.txt', 'w')Writer.write(output, 'hello world') 在静态语言中，方法重载是希望类可以以统一的方式处理不同类型的数据提供了可能。多个同名函数同时存在，具有不同的参数个数/类型，重载是一个类中多态性的一种表现。 在Java中实现函数重载：1234567891011class Writer&#123; public static void write(StringIO output, String content)&#123; output.write(content); return null; &#125; public static void write(File output, String content)&#123; output.write(content); return null; &#125; 参考自知乎用户刘志军：https://www.zhihu.com/question/20053359 函数重载主要是为了解决两个问题 可变参数类型。 可变参数个数。 另外，一个基本的设计原则是，仅仅当两个函数除了参数类型和参数个数不同以外，其功能是完全相同的，此时才使用函数重载，如果两个函数的功能其实不同，那么不应当使用重载，而应当使用一个名字不同的函数。 那么对于情况 1 ，函数功能相同，但是参数类型不同，python 如何处理？答案是根本不需要处理，因为 python 可以接受任何类型的参数，如果函数的功能相同，那么不同的参数类型在 python 中很可能是相同的代码，没有必要做成两个不同函数。 那么对于情况 2 ，函数功能相同，但参数个数不同，python 如何处理？大家知道，答案就是缺省参数。对那些缺少的参数设定为缺省参数即可解决问题。因为你假设函数功能相同，那么那些缺少的参数终归是需要用的。好了，鉴于情况 1 跟 情况 2 都有了解决方案，python 自然就不需要函数重载了。 参考自知乎用户pansz：https://www.zhihu.com/question/20053359]]></content>
      <categories>
        <category>Python3 进阶</category>
        <category>Python3 重载</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 子类的查看与类的对象判断]]></title>
    <url>%2F2018%2F03%2F03%2FPython3-%E5%AD%90%E7%B1%BB%E7%9A%84%E6%9F%A5%E7%9C%8B%E4%B8%8E%E7%B1%BB%E7%9A%84%E5%AF%B9%E8%B1%A1%E5%88%A4%E6%96%AD%2F</url>
    <content type="text"><![CDATA[如果想要查看一个类是不是另一个类的子类，可以使用內建的 issubclass 函数或者使用它的特殊特性__base__； 如果想要检查一个对象是不是一个类的实例，可以使用內建的 isinstance 函数或者使用它的特殊特性__class__; 12345678910111213141516171819202122232425262728# -*- coding: utf-8 -*- __metaclass__ = type #确定使新式类 class father(): def init(self): print("father()已经创建") class son(father): def init(self): print("son()已经创建") #下面测试issubclass()函数 print(issubclass(father,son)) # output: Falseprint(issubclass(son,father)) # output: True#下面使用__bases__ print("father.__bases__:",father.__bases__) # output: father.__bases__: (&lt;class 'object'&gt;,) print("son.__bases__:",son.__bases__) # output: son.__bases__: (&lt;class '__main__.father'&gt;,) #下面测试isinstance()函数 s = son() print(isinstance(s,son)) # output: Trueprint(isinstance(s,father)) # output: Trueprint(isinstance(s,str)) # output: False#下面使用__class__ print("s.__class__:",s.__class__) # output: s.__class__: &lt;class '__main__.son'&gt;]]></content>
      <categories>
        <category>Python3 进阶</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python3 鸭子类型]]></title>
    <url>%2F2018%2F03%2F03%2Fpython3-%E9%B8%AD%E5%AD%90%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[来源和解释Duck typing 这个概念来源于美国印第安纳州的诗人詹姆斯·惠特科姆·莱利（James Whitcomb Riley,1849-1916）的诗句： “ When I see a bird that walks like a duck and swims like a duck and quacks like a duck, I call that bird a duck.” 中文： “当看到一只鸟走起来像鸭子、游泳起来像鸭子、叫起来也像鸭子，那么这只鸟就可以被称为鸭子。” “鸭子类型”的语言是这么推断的：一只鸟走起来像鸭子、游起泳来像鸭子、叫起来也像鸭子，那它就可以被当做鸭子。也就是说，它不关注对象的类型，而是关注对象具有的行为(方法)。 鸭子类型是程序设计中的一种类型推断风格，这种风格适用于动态语言(比如PHP、Python、Ruby、Typescript、Perl、Objective-C、Lua、Julia、JavaScript、Java、Groovy、C#等)和某些静态语言(比如Golang,一般来说，静态类型语言在编译时便已确定了变量的类型，但是Golang的实现是：在编译时推断变量的类型)，支持”鸭子类型”的语言的解释器/编译器将会在解析(Parse)或编译时，推断对象的类型。 在鸭子类型中，关注的不是对象的类型本身，而是它是如何使用的。 例如，在不使用鸭子类型的语言中，我们可以编写一个函数，它接受一个类型为鸭的对象，并调用它的走和叫方法。在使用鸭子类型的语言中，这样的一个函数可以接受一个任意类型的对象，并调用它的走和叫方法。如果这些需要被调用的方法不存在，那么将引发一个运行时错误。任何拥有这样的正确的走和叫方法的对象都可被函数接受的这种行为引出了以上表述，这种决定类型的方式因此得名。 鸭子类型通常得益于不测试方法和函数中参数的类型，而是依赖文档、清晰的代码和测试来确保正确使用。从静态类型语言转向动态类型语言的用户通常试图添加一些静态的（在运行之前的）类型检查，从而影响了鸭子类型的益处和可伸缩性，并约束了语言的动态特性。 不足“鸭子类型”没有任何静态检查，如类型检查、属性检查、方法签名检查等。 “鸭子类型”语言的程序可能会在运行时因为不具备某种特定的方法而抛出异常：如果一只小狗(对象)想加入合唱团(以对象会不会嘎嘎嘎叫的方法为检验标准)，也学鸭子那么嘎嘎嘎叫，好吧，它加入了，可是加入之后，却不会像鸭子那样走路，那么，迟早要出问题的。 再举个例子：一只小老鼠被猫盯上了，情急之下，它学了狗叫，猫撤了之后，小老鼠的妈妈不无感叹的对它说：看吧，我让你学的这门儿外语多么重要啊。这虽然是个段子，但是，由于猫在思考时，使用了 “鸭子测试”，它以为会叫的就是狗，会对自己产生威胁，所以撤退了，也正是因为这个错误的判断，它误失了一次进食机会。 静态类型语言和动态类型语言的区别静态类型语言在编译时便已确定变量的类型，而动态类型语言的变量类型要到程序运行的时候，待变量被赋予某个值之后，才会具有某种类型。 静态类型语言的优点首先是在编译时就能发现类型不匹配的错误，编辑器可以帮助我们提前避免程序在运行期间有可能发生的一些错误。其次，如果在程序中明确地规定了数据类型，编译器还可以针对这些信息对程序进行一些优化工作，提高程序执行速度。 静态类型语言的缺点首先是迫使程序员依照强契约来编写程序，为每个变量规定数据类型，归根结底只是辅助我们编写可靠性高程序的一种手段，而不是编写程序的目的，毕竟大部分人编写程序的目的是为了完成需求交付生产。其次，类型的声明也会增加更多的代码，在程序编写过程中，这些细节会让程序员的精力从思考业务逻辑上分散开来。 动态类型语言的优点是编写的代码数量更少，看起来也更加简洁，程序员可以把精力更多地放在业务逻辑上面。虽然不区分类型在某些情况下会让程序变得难以理解，但整体而言，代码量越少，越专注于逻辑表达，对阅读程序是越有帮助的。动态类型语言的缺点是无法保证变量的类型，从而在程序的运行期有可能发生跟类型相关的错误。 动态类型语言对变量类型的宽容给实际编码带来了很大的灵活性。由于无需进行类型检测，我们可以尝试调用任何对象的任意方法，而无需去考虑它原本是否被设计为拥有该方法。 面向接口编程动态类型语言的面向对象设计中，鸭子类型的概念至关重要。利用鸭子类型的思想，我们不必借助超类型的帮助，就能轻松地在动态类型语言中实现一个原则：“面向接口编程，而不是面向实现编程”。例如, 一个对象若有push和pop方法，并且这些方法提供了正确的实现，它就可以被当作栈来使用。 一个对象如果有length属性，也可以依照下标来存取属性（最好还要拥有slice和splice等方法），这个对象就可以被当作数组来使用。 比如在python中，有很多file-like的东西，比如StringIO,GzipFile,socket。它们有很多相同的方法，我们把它们当作文件使用。 又比如list.extend()方法中,我们并不关心它的参数是不是list,只要它是可迭代的,所以它的参数可以是list/tuple/dict/字符串/生成器等. 鸭子类型在动态语言中经常使用，非常灵活，使得python不想java那样专门去弄一大堆的设计模式。 在静态类型语言中，要实现“面向接口编程”并不是一件容易的事情，往往要通过抽象类或者接口等将对象进行向上转型。当对象的真正类型被隐藏在它的超类型身后，这些对象才能在类型检查系统的“监视”之下互相被替换使用。只有当对象能够被互相替换使用，才能体现出对象多态性的价值。 Python中的多态Python中的鸭子类型允许我们使用任何提供所需方法的对象，而不需要迫使它成为一个子类。由于python属于动态语言，当你定义了一个基类和基类中的方法，并编写几个继承该基类的子类时，由于python在定义变量时不指定变量的类型，而是由解释器根据变量内容推断变量类型的（也就是说变量的类型取决于所关联的对象），这就使得python的多态不像是c++或java中那样—定义一个基类类型变量而隐藏了具体子类的细节。 请看下面的例子和说明：1234567891011121314151617181920212223242526272829303132333435class AudioFile: def __init__(self, filename): if not filename.endswith(self.ext): raise Exception("Invalid file format") self.filename = filenameclass MP3File(AudioFile): ext = "mp3" def play(self): print("Playing &#123;&#125; as mp3".format(self.filename))class WavFile(AudioFile): ext = "wav" def play(self): print("Playing &#123;&#125; as wav".format(self.filename))class OggFile(AudioFile): ext = "ogg" def play(self): print("Playing &#123;&#125; as ogg".format(self.filename))class FlacFile: """ Though FlacFile class doesn't inherit AudioFile class, it also has the same interface as three subclass of AudioFile. It is called duck typing. """ def __init__(self, filename): if not filename.endswith(".flac"): raise Exception("Invalid file format") self.filename = filename def play(self): print("Playing &#123;&#125; as flac".format(self.filename)) Though FlacFile class doesn’t inherit AudioFile class,it also has the same interface as three subclass of AudioFile.It is called duck typing. 上面的代码中，MP3File、WavFile、OggFile三个类型继承了AudioFile这一积累，而FlacFile没有扩展AudioFile，但是可以在python中使用完全相同的接口与之交互。 因为任何提供正确接口的对象都可以在python中交替使用，它减少了多态的一般超类的需求。继承仍然可以用来共享代码，但是如果所有被共享的都是公共接口，鸭子类型就是所有所需的。这减少了继承的需要，同时也减少了多重继承的需要；通常，当多重继承似乎是一个有效方案的时候，我们只需要使用鸭子类型去模拟多个超类之一（定义和那个超类一样的接口和实现）就可以了。 作者：JasonDing链接：https://www.jianshu.com/p/650485b78d11來源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 参考 https://baike.baidu.com/item/鸭子类型/10845665?fr=aladdin https://www.jianshu.com/p/650485b78d11]]></content>
      <categories>
        <category>Python3 进阶</category>
        <category>Python3 鸭子类型</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python3 如何用一个表达式合并两个字典]]></title>
    <url>%2F2018%2F03%2F03%2Fpython3-%E5%A6%82%E4%BD%95%E7%94%A8%E4%B8%80%E4%B8%AA%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E5%AD%97%E5%85%B8%2F</url>
    <content type="text"><![CDATA[有两个Python字典,写一个表达式来返回两个字典的合并。update()方法返回的是空值而不是返回合并后的对象.1234567&gt;&gt;&gt; x = &#123;'a':1, 'b': 2&#125;&gt;&gt;&gt; y = &#123;'b':10, 'c': 11&#125;&gt;&gt;&gt; z = x.update(y)&gt;&gt;&gt; print zNone&gt;&gt;&gt; x&#123;'a': 1, 'b': 10, 'c': 11&#125; 如何才能让值保存在z而不是x? 对于python2可以用下面的方法:1z = dict(x.items() + y.items()) 最后就是你想要的最终结果保存在字典z中,而键b的值会被第二个字典的值覆盖.12345&gt;&gt;&gt; x = &#123;'a':1, 'b': 2&#125;&gt;&gt;&gt; y = &#123;'b':10, 'c': 11&#125;&gt;&gt;&gt; z = dict(x.items() + y.items())&gt;&gt;&gt; z&#123;'a': 1, 'c': 11, 'b': 10&#125; 对于Python3：123&gt;&gt;&gt; z = dict(list(x.items()) + list(y.items()))&gt;&gt;&gt; z&#123;'a': 1, 'c': 11, 'b': 10&#125; 还可以这样:12z = x.copy()z.update(y)]]></content>
      <categories>
        <category>Python3 进阶</category>
        <category>Python3 数据结构</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python3 可迭代对象、迭代器和生成器]]></title>
    <url>%2F2018%2F03%2F01%2Fpython3-%E5%8F%AF%E8%BF%AD%E4%BB%A3%E5%AF%B9%E8%B1%A1%E3%80%81%E8%BF%AD%E4%BB%A3%E5%99%A8%E5%92%8C%E7%94%9F%E6%88%90%E5%99%A8%2F</url>
    <content type="text"><![CDATA[前言迭代是数据处理的基石。扫描内存中放不下数据集时，我们要找到一种惰性获取数据项的方式，即按需一次获取一个数据项，这就是迭代器模式（iterator pattern）。所有的生成器都是迭代器，因为生成器完全实现了迭代器接口。在python社区中，大多数时候都把迭代器和生成器视作同一概念。 所有python程序员都知道，序列可迭代，下面说明具体原因。 序列可迭代的原因：iter函数解释器需要迭代对象x时，会自动调用iter(x)。内置的iter函数有以下作用。 检查对象是否实现了__iter__方法，如果实现了就调用它，获得一个迭代器。 如果没有实现__iter__方法，但是实现了__getitem__方法，python会创建一个迭代器，尝试按顺序（从索引0开始）获取元素。 如果尝试失败，python会抛出TypeError异常，通常会提示”C object is not iterable”,其中C是目标对象所属的类。 任何Python序列都可迭代的原因是它们实现了__getitem__方法。其实标准的序列也都实现了__iter__方法。之所以对__getitem__方法做特殊处理是为了向后兼容。 从Python3.4开始，检查x能否迭代，最准确的方法是调用iter(x)函数，如果不可迭代，再处理TypeError异常。这比使用isinstance(x, abc.Iterable)更准确，因为iter(x)函数会考虑到遗留的__getitem__方法，而abc.Iterable类则不考虑。 可迭代的对象与迭代器的对比可迭代对象使用iter内置函数可以获取迭代器的对象。 如果实现了能返回迭代器的__iter__方法，那么对象就是可迭代的。序列都可以迭代；实现了__getitem__方法，而且七参数是从零开始的索引，这种对象也是可迭代的。 我们要明确可迭代对象和迭代器之间的关系：Python从可迭代的对象中获取迭代器 标准的迭代器接口有两个方法，即： __next__:返回下一个可用元素，如果没有元素，抛出StopIteration异常 __iter__:返回self,以便在应该使用可迭代对象的地方使用迭代器，比如for循环中。 因为迭代器只需__next__和__iter__两个方法，所以除了调用next()方法，以及捕获StopIteration异常之外，没有办法检查是否还有遗留的元素。此外，也没有办法还原迭代器。如果想再次迭代，那就要调用iter(…)，传入之前构建迭代器的可迭代对象。 迭代器迭代器是这样的对象：实现了无参数的__next__方法，返回序列中的下一个元素；如果没有元素了，那么抛出StopIteration异常。Python迭代器还实现了__iter__方法，因此迭代器也可以迭代。 构建可迭代对象和迭代器时经常会出现错误，原因是混淆了两者。要知道，可迭代的对象有个__iter__方法，每次都实例化一个新的迭代器；而迭代器要实现__next__方法，返回单个元素，此外还要实现__iter__方法，返回迭代器本身。因此，迭代器可以迭代，但是可迭代的对象不是迭代器。 可迭代的对象一定不是自身的迭代器。也就是说，可迭代的对象必须实现__iter__方法，但不能实现__next__方法。另一方面，迭代器应该一直可以迭代，迭代器的__iter__方法应该返回自身。 123a = [1,2,3]'__iter__' in dir(a) # True'__iter__' in dir(iter(a)) # True 生成器函数只要Python函数的定义体中有yield关键字，该函数就是生成器函数。调用生成器函数时，会返回一个生成器对象。也就是说，生成器函数是生成器工厂。 普通的函数与生成器函数在句法上的唯一区别是，在后者的定义体中有yield关键字。有些人认为定义生成器函数应该使用一个新的关键字，例如gen，而不是def，但是Guido不同意。 生成器函数工作原理1234567891011121314151617181920212223242526def gen_123(): # 只要Python代码中包含yield，该函数就是生成器函数 yield 1 #生成器函数的定义体中通常都有循环，不过这不是必要条件；此处重复使用了3次yield yield 2 yield 3if __name__ == '__main__': print(gen_123) # 可以看出gen_123是函数对象 # &lt;function gen_123 at 0x10be199d8&gt; print(gen_123()) # 函数调用时返回的是一个生成器对象 # &lt;generator object gen_123 at 0x10be31ca8&gt; for i in gen_123(): # 生成器是迭代器，会生成传给yield关键字的表达式的值 print(i) # 1 # 2 # 3 g = gen_123() # 为了仔细检查，把生成器对象赋值给g print(next(g)) # 1 print(next(g)) # 2 print(next(g)) # 3 print(next(g)) # 生成器函数的定义体执行完毕后，生成器对象会抛出异常。# Traceback (most recent call last):# File "test.py", line 17, in &lt;module&gt;# print(next(g))# StopIteration 如上述代码所示： 只要Python代码中包含yield，该函数就是生成器函数 生成器函数的定义体中通常都有循环，不过这不是必要条件；此处重复使用了3次yield 可以看出gen_123是函数对象 函数调用时返回的是一个生成器对象 生成器是迭代器，会生成传给yield关键字的表达式的值 为了仔细检查，把生成器对象赋值给g 因为g是迭代器，所以调用nest(g)会获取yield生成的下一个元素 生成器函数的定义体执行完毕后，生成器对象会抛出异常。 使用准确的词语描述从生成器中获取结果的过程有助于理解生成器。注意，此处说的是产出或生成值。如果说生成器返回值，就会让人难以理解。 函数返回值; 调用生成器函数返回生成器; 生成器产出或生成值。生成器不会以常规方式返回值; 1234567891011121314151617In [66]: def gen_AB(): # 1 ...: print('start') ...: yield 'A' # 2 ...: print('continue') ...: yield 'B' # 3 ...: print('end.') # 4 ...:In [67]: for c in gen_AB(): # 5 ...: print('--&gt;', c) # 6 ...:start # 7--&gt; A # 8continue # 9--&gt; B # 10end. # 11 定义生成的器函数的方式与普通函数无异，只不过要使用yield关键字 在for循环中第一次隐式调用next()函数时（序号5），会打印’start’，然后停在第一个yield语句，生成值 ‘A’ 在for循环第二次隐式调用next()函数时，会打印’continue’，然后停在第二个yield语句，生成值’B’ 第三次调用 next()函数时，会打印’end.’，然后到达函数定义体末尾。导致生成器对象抛出StopIteration异常 迭代时, for 机制的作用与g = iter(gen_AB())一样，用于获取生成器对象，然后每次迭代时调用next(g) 循环打印 –&gt; 与 next(g)返回的值。但是，生成器函数中的print函数输出结果之后才会看到这个输出 ‘start’是生成器函数定义体中print(‘start’)输出的记过 生成器函数定义体中的yield ‘A’ 语句会生成值 A，提供给for循环使用，而A会赋值给变量c，最终输出–&gt; A 第二次调用next(g)，继续迭代，生成器函数定义体中的代码由yield ‘A’前进到 yield ‘B’。文本continue是由生成器函数定义体中的第二个print函数输出的 生成器函数定义体中的yield ‘B’ 语句会生成值 B，提供给for循环使用，而B会赋值给变量c，最终输出–&gt; B 第三次调用next(g)，继续迭代，前进到生成器函数的结尾。文本 end. 是由生成器函数定义体中第三个print函数输出的。 到达生成器函数定义体结尾时，生成器对象抛出StopIteration异常。for 机制会捕捉异常，因此循环终止没有报错。 生成器表达式简单的生成器函数，可以替换成生成器表达式。生成器表达式可以理解为列表推导的惰性版本：不会迫切的构建列表，而是返回一个生成器，按需惰性生成元素。也就是说，如果列表推导是制造工厂的列表，那么生成器表达式就是制造生成器的工厂。如下演示了一个简单的生成器表达式，并且与列表推导做了对比。 123456789101112131415161718192021222324252627282930313233In [66]: def gen_AB(): # 1 ...: print('start') ...: yield 'A' ...: print('continue') ...: yield 'B' ...: print('end.') ...:In [67]: res1 = [x*3 for x in gen_AB()] # 2startcontinueend.In [68]: for i in res1(): # 3 ...: print('--&gt;', i) ...:AAABBBIn [69]: res2 = (x*3 for x in gen_AB()) # 4In [70]: res2 # 5&lt;generator object &lt;genexpr&gt; at 0x106a07620&gt;In [71]: for i in res2(): # 6 ...: print('--&gt;', i) ...:start --&gt; A continue--&gt; B end. 创建gen_AB函数 列表推到迫切的迭代gen_AB()函数生成的生成器对象产出的元素：’A’和’B’。注意。下面输出的是start、continue、end.。 for循环迭代列表推导生成的res1列表 把生成器表达式返回的值赋值给res2。只需调用gen_AB()函数，虽然调用时会返回一个生成器，但是这里并不使用。 可以看出res2是一个生成器对象。 只有for循环迭代res2时，gen_AB函数的定义体才会真正执行。for循环每次迭代时会隐式调用next(res2)，前进到gen_AB函数中的下一个yield语句。注意，gen_AB函数的输出与for循环中print函数的输出夹杂在一起。 生成器表达式会产出生成器，因此可以使用生成器表达式进一步减少代码量。生成器表达式是一种语法糖，完全可以替换成生成器函数，不过有时候使用生成器表达式更便利。 何时使用生成器表达式生成器表达式是创建生成器的简洁句法，这样无需定义函数再调用。不过，生成器函数灵活的多，可以使用多个语句实现复杂的逻辑，也可以作为协程使用。遇到简单的情况时，可以使用生成器表达式，因为这样扫一眼就知道代码的作用。其实选择那种句法很容易判断：如果生成器表达式需要分行写，倾向于定义成生成器函数，以便提高可读性。此外生成器函数有名称，因此可以重用。]]></content>
      <categories>
        <category>Python3 进阶</category>
        <category>Python3 迭代器和生成器</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[if __name__ == '__main__': ?]]></title>
    <url>%2F2018%2F03%2F01%2Fif-name-main%2F</url>
    <content type="text"><![CDATA[Every Python module has it’s __name__ defined and if this is &#39;__main__&#39;, it implies that the module is being run standalone by the user and we can do corresponding appropriate actions. 当Python解析器读取一个源文件时,它会执行所有的代码.在执行代码前,会定义一些特殊的变量.例如,如果解析器运行的模块(源文件)作为主程序,它将会把__name__变量设置成&quot;__main__&quot;.如果只是引入其他的模块,__name__变量将会设置成模块的名字. 这么做的原因是有时你想让你的模块既可以直接的执行,还可以被当做模块导入到其他模块中去.通过检查是不是主函数,可以让你的代码只在它作为主程序运行时执行,而当其他人调用你的模块中的函数的时候不必执行. 直接上一个栗子：1234567# Filename: using_name.pyprint(__name__)if __name__ == '__main__': print('This program is being run by itself')else: print('I am being imported from another module') 12345678910$ python using_name.py__main__This program is being run by itself$ python&gt;&gt;&gt; import using_nameusing_nameI am being imported from another module&gt;&gt;&gt;]]></content>
      <categories>
        <category>Python3 进阶</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 中的单下划线和双下划线]]></title>
    <url>%2F2018%2F03%2F01%2FPython3-%E5%8D%95%E4%B8%8B%E5%88%92%E7%BA%BF%E5%92%8C%E5%8F%8C%E4%B8%8B%E5%88%92%E7%BA%BF%2F</url>
    <content type="text"><![CDATA[单下划线在解释器中在交互解释器中，_符号还是指交互解释器中最后一次执行语句的返回结果。这种用法最初出现在CPython解释器中，其他解释器后来也都跟进了。 作为名称使用这个跟上面有点类似。_用作被丢弃的名称。按照惯例，这样做可以让阅读你代码的人知道，这是个不会被使用的特定名称。举个例子，你可能无所谓一个循环计数的值：123n = 42for _ in range(n): do_something() i18n_还可以被用作函数名。这种情况，单下划线经常被用作国际化和本地化字符串翻译查询的函数名。举个例子，在 Django documentation for translation 中你可能会看到： 123456from django.utils.translation import ugettext as _from django.http import HttpResponsedef my_view(request): output = _("Welcome to my site.") return HttpResponse(output) 注意：第二种和第三种用法会引起冲突，所以在任意代码块中，如果使用了_作i18n翻译查询函数，就应该避免再用作被丢弃的变量名。 单下划线前缀的名称首先是单下划线开头，这个被常用于模块中，在一个模块中以单下划线开头的变量和函数被默认当作内部函数,用来指定私有变量。如果使用 from a_module import * 导入时，这部分变量和函数不会被导入。不过值得注意的是，如果使用 import a_module 这样导入模块，仍然可以用 a_module._some_var 这样的形式访问到这样的对象。 另外单下划线开头还有一种一般不会用到的情况在于使用一个 C 编写的扩展库有时会用下划线开头命名，然后使用一个去掉下划线的 Python 模块进行包装。如 struct 这个模块实际上是 C 模块 _struct 的一个 Python 包装。 单下划线后缀的名称在 Python 的官方推荐的代码样式中，还有一种单下划线结尾的样式，这在解析时并没有特别的含义，但通常用于和 Python 关键词区分开来，比如如果我们需要一个变量叫做 class，但 class 是 Python 的关键词，就可以以单下划线结尾写作 class_。 双下划线双下划线开头的命名形式在 Python 的类成员中使用表示名字改编 (Name Mangling)，即如果有一 Test 类里有一成员 __x，那么 dir(Test) 时会看到 _Test__x 而非 __x。这是为了避免该成员的名称与子类中的名称冲突。但要注意这要求该名称末尾最多有一个下划线 python document. 双下划线开头双下划线结尾的是一些 Python 的“魔术”对象，如类成员的 __init__、__del__、__add__、__getitem__ 等，以及全局的 __file__、__name__ 等。 Python 官方推荐永远不要将这样的命名方式应用于自己的变量或函数，而是按照文档说明来使用。 举个栗子1234567891011121314&gt;&gt;&gt; class MyClass():... def __init__(self):... self.__superprivate = "Hello"... self._semiprivate = ", world!"...&gt;&gt;&gt; mc = MyClass()&gt;&gt;&gt; print mc.__superprivateTraceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;AttributeError: myClass instance has no attribute '__superprivate'&gt;&gt;&gt; print mc._semiprivate, world!&gt;&gt;&gt; print mc.__dict__&#123;'_MyClass__superprivate': 'Hello', '_semiprivate': ', world!'&#125; __foo__:一种约定,Python内部的名字,用来区别其他用户自定义的命名,以防冲突，就是例如__init__(),__del__(),__call__()这些特殊方法 _foo:一种约定,用来指定变量私有.程序员用来指定私有变量的一种方式.不能用from module import * 导入，其他方面和公有一样访问； __foo:这个有真正的意义:解析器用_classname__foo来代替这个名字,以区别和其他类相同的命名,它无法直接像公有成员一样随便访问,但是可以通过对象名 _类名__xxx 这样的方式可以访问. 参考详情见: http://stackoverflow.com/questions/1301346/the-meaning-of-a-single-and-a-double-underscore-before-an-object-name-in-python http://www.zhihu.com/question/19754941 https://segmentfault.com/a/1190000002611411 https://docs.python.org/3.4/tutorial/classes.html#tut-private]]></content>
      <categories>
        <category>Python3 进阶</category>
        <category>Python3 下划线</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 自省]]></title>
    <url>%2F2018%2F03%2F01%2FPython3-%E8%87%AA%E7%9C%81%2F</url>
    <content type="text"><![CDATA[自省是python彪悍的特性之一. 自省（introspection）是一种自我检查行为。在计算机编程中，自省是指这种能力：检查某些事物以确定它是什么、它知道什么以及它能做什么。自省向程序员提供了极大的灵活性和控制力. 自省就是面向对象的语言所写的程序在运行时,所能知道对象的类型.简单一句就是运行时能够获得对象的类型.比如type(),dir(),getattr(),hasattr(),isinstance(). 12345a = [1,2,3]b = &#123;'a':1,'b':2,'c':3&#125;c = Trueprint type(a),type(b),type(c) # &lt;type 'list'&gt; &lt;type 'dict'&gt; &lt;type 'bool'&gt;print isinstance(a,list) # True 未完待续 参考： http://python.jobbole.com/82110/ http://blog.csdn.net/IAlexanderI/article/details/78768378]]></content>
      <categories>
        <category>Python3 进阶</category>
        <category>Python3 自省</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 类变量和实例变量]]></title>
    <url>%2F2018%2F03%2F01%2FPython3-%E7%B1%BB%E5%8F%98%E9%87%8F%E5%92%8C%E5%AE%9E%E4%BE%8B%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[写在前面首先来一张图 类变量和实例变量在Python Tutorial中对于类变量和实例变量是这样描述的： Generally speaking, instance variables are for data unique to each instance and class variables are for attributes and methods shared by all instances of the class: 1234class Dog: kind = 'canine' # class variable shared by all instances def __init__(self, name): self.name = name # instance variable unique to each instance 类Dog中，类属性kind为所有实例所共享；实例属性name为每个Dog的实例独有。 类变量： ​ 是可在类的所有实例之间共享的值（也就是说，它们不是单独分配给每个实例的）。例如下例中，num_of_instance 就是类变量，用于跟踪存在着多少个Test 的实例。 实例变量： 实例化之后，每个实例单独拥有的变量。 12345678910111213class Test(object): num_of_instance = 0 def __init__(self, name): self.name = name Test.num_of_instance += 1 if __name__ == '__main__': print Test.num_of_instance # 0 t1 = Test('jack') print Test.num_of_instance # 1 t2 = Test('lucy') print t1.name , t1.num_of_instance # jack 2 print t2.name , t2.num_of_instance # lucy 2 补充的例子 123456789class Person: name="aaa"p1=Person()p2=Person()p1.name="bbb"print p1.name # bbbprint p2.name # aaaprint Person.name # aaa 这里p1.name=&quot;bbb&quot;是实例调用了类变量,属于函数传参的问题,p1.name一开始是指向的类变量name=&quot;aaa&quot;,但是在实例的作用域里把类变量的引用改变了,就变成了一个实例变量,self.name不再引用Person的类变量name了. 可以看看下面的例子: 123456789class Person: name=[]p1=Person()p2=Person()p1.name.append(1)print p1.name # [1]print p2.name # [1]print Person.name # [1] 类对象和实例对象类对象Python中一切皆对象；类定义完成后，会在当前作用域中定义一个以类名为名字，指向类对象的名字。如12class Dog: pass 会在当前作用域定义名字Dog，指向类对象Dog。 类对象支持的操作：总的来说，类对象仅支持两个操作： 实例化；使用instance_name = class_name()的方式实例化，实例化操作创建该类的实例。 属性引用；使用class_name.attr_name的方式引用类属性。 实例对象实例对象是类对象实例化的产物，实例对象仅支持一个操作: 属性引用；与类对象属性引用的方式相同，使用instance_name.attr_name的方式。 按照严格的面向对象思想，所有属性都应该是实例的，类属性不应该存在。那么在Python中，由于类属性绑定就不应该存在，类定义中就只剩下函数定义了。 在Python tutorial关于类定义也这么说： In practice, the statements inside a class definition will usually be function definitions, but other statements are allowed, and sometimes useful. 实践中，类定义中的语句通常是函数定义，但是其他语句也是允许的，有时也是有用的。 这里说的其他语句，就是指类属性的绑定语句。 属性绑定在定义类时，通常我们说的定义属性，其实是分为两个方面的： 类属性绑定 实例属性绑定 用绑定这个词更加确切；不管是类对象还是实例对象，属性都是依托对象而存在的。 我们说的属性绑定，首先需要一个可变对象，才能执行绑定操作，使用 objname.attr = attr_value 的方式，为对象objname绑定属性attr。 这分两种情况： 若属性attr已经存在，绑定操作会将属性名指向新的对象； 若不存在，则为该对象添加新的属性，后面就可以引用新增属性。 类属性绑定Python作为动态语言，类对象和实例对象都可以在运行时绑定任意属性。因此，类属性的绑定发生在两个地方： 类定义时； 运行时任意阶段。 下面这个例子说明了类属性绑定发生的时期：12345678class Dog: kind = 'canine'Dog.country = 'China'print(Dog.kind, ' - ', Dog.country) # output: canine - Chinadel Dog.kindprint(Dog.kind, ' - ', Dog.country)# AttributeError: type object 'Dog' has no attribute 'kind' 在类定义中，类属性的绑定并没有使用objname.attr = attr_value的方式，这是一个特例，其实是等同于后面使用类名绑定属性的方式。因为是动态语言，所以可以在运行时增加属性，删除属性。 实例属性绑定与类属性绑定相同，实例属性绑定也发生在两个地方： 类定义时； 运行时任意阶段。 示例：123456789class Dog: def __init__(self, name, age): self.name = name self.age = agedog = Dog('Lily', 3)dog.fur_color = 'red'print('%s is %s years old, it has %s fur' % (dog.name, dog.age, dog.fur_color))# Output: Lily is 3 years old, it has red fur Python类实例有两个特殊之处： __init__在实例化时执行 Python实例调用方法时，会将实例对象作为第一个参数传递 因此，__init__方法中的self就是实例对象本身，这里是dog，语句12self.name = nameself.age = age 以及后面的语句1dog.fur_color = 'red' 为实例dog增加三个属性name, age, fur_color。 属性引用类属属性引用类属性的引用，肯定是需要类对象的，属性分为两种： 数据属性 函数属性 数据属性引用很简单，示例：12345class Dog: kind = 'canine'Dog.country = 'China'print(Dog.kind, ' - ', Dog.country) # output: canine - China 通常很少有引用类函数属性的需求，示例：123456class Dog: kind = 'canine' def tell_kind(): print(Dog.kind)Dog.tell_kind() # Output: canine 函数tell_kind在引用kind需要使用Dog.kind而不是直接使用kind，涉及到作用域，这一点在我的另一篇文章中有介绍：Python进阶 - 命名空间与作用域 实例属性引用使用实例对象引用属性稍微复杂一些，因为实例对象可引用类属性以及实例属性。但是实例对象引用属性时遵循以下规则： 总是先到实例对象中查找属性，再到类属性中查找属性； 属性绑定语句总是为实例对象创建新属性，属性存在时，更新属性指向的对象。 数据属性引用示例1：12345678910111213class Dog: kind = 'canine' country = 'China' def __init__(self, name, age, country): self.name = name self.age = age self.country = countrydog = Dog('Lily', 3, 'Britain')print(dog.name, dog.age, dog.kind, dog.country)# output: Lily 3 canine Britain 类对象Dog与实例对象dog均有属性country，按照规则，dog.country会引用到实例对象的属性；但实例对象dog没有属性kind，按照规则会引用类对象的属性。 示例2：123456789101112131415161718class Dog: kind = 'canine' country = 'China' def __init__(self, name, age, country): self.name = name self.age = age self.country = countrydog = Dog('Lily', 3, 'Britain')print(dog.name, dog.age, dog.kind, dog.country) # Lily 3 canine Britainprint(dog.__dict__) # &#123;'name': 'Lily', 'age': 3, 'country': 'Britain'&#125;dog.kind = 'feline'print(dog.name, dog.age, dog.kind, dog.country) # Lily 3 feline Britainprint(dog.__dict__) # &#123;'name': 'Lily', 'age': 3, 'country': 'Britain', 'kind': 'feline'&#125;print(Dog.kind) # canine 没有改变类属性的指向 示例3，可变类属性引用： 1234567891011121314151617class Dog: tricks = [] def __init__(self, name): self.name = name def add_trick(self, trick): # self.tricks.append(trick) Dog.tricks.append(trick)d = Dog('Fido')e = Dog('Buddy')d.add_trick('roll over')e.add_trick('play dead')print(d.tricks) # ['roll over', 'play dead'] 语句self.tricks.append(trick)并不是属性绑定语句，因此还是在类属性上修改可变对象。 方法属性引用与数据成员不同，类函数属性在实例对象中会变成方法属性。先看一个示例： 1234567891011121314class MethodTest: def inner_test(self): print('in class')def outer_test(): print('out of class')mt = MethodTest()mt.outer_test = outer_testprint(type(MethodTest.inner_test)) # &lt;class 'function'&gt; 类函数print(type(mt.inner_test)) #&lt;class 'method'&gt; 类方法print(type(mt.outer_test)) #&lt;class 'function'&gt; 类函数 可以看到，类函数属性在实例对象中变成了方法属性，但是并不是实例对象中所有的函数都是方法。 Python tutorial中这样介绍方法对象： When an instance attribute is referenced that isn’t a data attribute, its class is searched. If the name denotes a valid class attribute that is a function object, a method object is created by packing (pointers to) the instance object and the function object just found together in an abstract object: this is the method object. When the method object is called with an argument list, a new argument list is constructed from the instance object and the argument list, and the function object is called with this new argument list. 引用非数据属性的实例属性时，会搜索它对应的类。如果名字是一个有效的函数对象，Python会将实例对象连同函数对象打包到一个抽象的对象中并且依据这个对象创建方法对象：这就是被调用的方法对象。当使用参数列表调用方法对象时，会使用实例对象以及原有参数列表构建新的参数列表，并且使用新的参数列表调用函数对象。 那么，实例对象只有在引用方法属性时，才会将自身作为第一个参数传递；调用实例对象的普通函数，则不会。所以可以使用如下方式直接调用方法与函数： 12mt.inner_test()mt.outer_test() 除了方法与函数的区别，其引用与数据属性都是一样的 最佳实践虽然Python作为动态语言，支持在运行时绑定属性，但是从面向对象的角度来看，还是在定义类的时候将属性确定下来。 参考: http://stackoverflow.com/questions/6470428/catch-multiple-exceptions-in-one-line-except-block https://www.cnblogs.com/crazyrunning/p/6945183.html]]></content>
      <categories>
        <category>Python3 进阶</category>
        <category>类变量 和 实例变量</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 @staticmethod和@classmethod]]></title>
    <url>%2F2018%2F03%2F01%2FPython3-staticmethod%E5%92%8C-classmethod%2F</url>
    <content type="text"><![CDATA[Python其实有3个方法,即静态方法(staticmethod),类方法(classmethod)和实例方法,如下: 12345678910111213141516def foo(x): print "executing foo(%s)"%(x)class A(object): def foo(self,x): print "executing foo(%s,%s)"%(self,x) @classmethod def class_foo(cls,x): print "executing class_foo(%s,%s)"%(cls,x) @staticmethod def static_foo(x): print "executing static_foo(%s)"%xa=A() 这里先理解下函数参数里面的self和cls。这个self和cls是对类或者实例的绑定,对于一般的函数来说我们可以这么调用foo(x),这个函数就是最常用的,它的工作跟任何东西(类,实例)无关。对于实例方法,我们知道在类里每次定义方法的时候都需要绑定这个实例,就是foo(self, x),为什么要这么做呢?因为实例方法的调用离不开实例,我们需要把实例自己传给函数,调用的时候是这样的a.foo(x)(其实是foo(a, x))。类方法一样,只不过它传递的是类而不是实例,A.class_foo(x)。注意这里的self和cls可以替换别的参数,但是python的约定是这俩,还是不要改的好。 对于静态方法其实和普通的方法一样,不需要对谁进行绑定,唯一的区别是调用的时候需要使用a.static_foo(x)或者A.static_foo(x)来调用. \ 实例方法 类方法 静态方法 a = A() a.foo(x) a.class_foo(x) a.static_foo(x) A 不可用 A.class_foo(x) A.static_foo(x) 更多关于这个问题: http://stackoverflow.com/questions/136097/what-is-the-difference-between-staticmethod-and-classmethod-in-python https://realpython.com/blog/python/instance-class-and-static-methods-demystified/]]></content>
      <categories>
        <category>Python3 进阶</category>
        <category>Staticmethod &amp; Classmethod</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo博客Next主题添加Fork me on GitHub标签]]></title>
    <url>%2F2018%2F02%2F28%2FHexo%E5%8D%9A%E5%AE%A2Next%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0Fork-me-on-GitHub%E6%A0%87%E7%AD%BE%2F</url>
    <content type="text"><![CDATA[给自己的个人博客添加Fork me on GitHub标签感觉很专业很逼格，添加的方法也很简单，介绍如下。 打开文件：hexo博客根目录/themes/next/layout/_layout.swig 找到如下代码块12345...&lt;div class="&#123;&#123; container_class &#125;&#125; &#123;% block page_class %&#125;&#123;% endblock %&#125; "&gt; &lt;div class="headband"&gt;&lt;/div&gt; [样式代码]... 样式代码 看这里 ，挑选自己喜欢的样式。 然后将样式代码添加到上述 _layout.swig 代码块后面，比如选择黑色经典款，即：1234567...&lt;div class="&#123;&#123; container_class &#125;&#125; &#123;% block page_class %&#125;&#123;% endblock %&#125; "&gt; &lt;div class="headband"&gt;&lt;/div&gt; # [样式代码] &lt;a href="https://github.com/you"&gt;&lt;img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"&gt;&lt;/a&gt;... 重新部署一下就可以查看了，如果显示不出来，需要清理浏览器的cookie,多刷新几次就OK了。大家看我的，感觉很搭( ⊙ o ⊙ )！]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo部署的网站项目(.deploy_git)中添加README.md]]></title>
    <url>%2F2018%2F02%2F28%2FHexo%E9%83%A8%E7%BD%B2%E7%9A%84%E7%BD%91%E7%AB%99%E9%A1%B9%E7%9B%AE-deploy-git-%E4%B8%AD%E6%B7%BB%E5%8A%A0README-md%2F</url>
    <content type="text"><![CDATA[终端中执行hexo generate时，会将source文件夹中的.md文件渲染为.html文件到public文件夹中，所以我们可以将README.md文件放到source文件夹中，这样在执行hexo deploy时，生成的.deploy_git文件夹中就会有README文件。但此时并不是我们想要的README.md。需要注意的是，我们要防止此.md文件被渲染为.html文件，因此，需要在站点配置文件_config.yml中设置skip_render: README.md，这样部署完成后我们就可以在配置的.deploy_git中看到README.md了。 配置截图如下：]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始学爬虫-05]]></title>
    <url>%2F2018%2F02%2F28%2F%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E7%88%AC%E8%99%AB-05%2F</url>
    <content type="text"><![CDATA[为什么要使用CookieCookie，指某些网站为了辨别用户身份、进行session跟踪而储存在用户本地终端上的数据（通常经过加密)。比如说有些网站需要登录后才能访问某个页面，在登录之前，你想抓取某个页面内容，登陆前与登陆后是不同的，或者不允许的。使用Cookie和使用代理IP一样，也需要创建一个自己的opener。在HTTP包中，提供了cookiejar模块，用于提供对Cookie的支持。 http.cookiejar功能强大，我们可以利用本模块的CookieJar类的对象来捕获cookie并在后续连接请求时重新发送，比如可以实现模拟登录功能。该模块主要的对象有CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar。 它们的关系： CookieJar–派生–&gt;FileCookieJar–派生–&gt;MozillaCookieJar和LWPCookieJar 工作原理： 创建一个带有cookie的opener，在访问登录的URL时，将登录后的cookie保存下来，然后利用这个cookie来访问其他网址。查看登录之后才能看到的信息。 同样，我们以实例进行讲解，爬取伯乐在线的面向对象的漂亮MM的邮箱联系方式。 实战(1).背景介绍在伯乐在线有这么一个有趣的模块，面向对象，它说白了就是提供了一个程序员(媛)网上相亲的平台。 URL: http://date.jobbole.com/ 它的样子是这样的： 可以看到，这里有很多的相亲贴，随便点进去就会有网上相亲MM的详细信息，想获取MM的联系方式，需要积分，积分可以通过签到的方式获取。如果没有登陆账户，获取联系方式的地方是这个样子的： 如果登陆了账号，获取联系方式的地方是这个样子的： 想要爬取MM的联系邮箱，就需要用到我们本次讲到的知识，Cookie的使用。当然，首先你积分也得够。 在讲解之前，推荐一款抓包工具–Fiddler，可以在Google Chrome的Google商店下载这个插件，它的样子是这样的： 可以看到，通过这个插件，我们可以很容易找到Post的Form Data等信息，很方便，当然也可以用之前讲得浏览器审查元素的方式查看这些信息。 (2).过程分析在伯乐在线首页点击登陆的按钮，Fiddler的抓包内容如下： 从上图可以看出，真正请求的url是 http://www.jobbole.com/wp-admin/admin-ajax.php Form Data的内容记住，这些是我们编程需要用到的。user_login是用户名，user_pass是用户密码。 在点击取得联系邮箱按钮的时候，Fiddler的抓包内容如下： 从上图可以看出，此刻真正请求的url是 http://date.jobbole.com/wp-admin/admin-ajax.php 同样Form Data中内容要记下来。postId是每个帖子的id。例如，打开一个相亲贴，它的URL是http://date.jobbole.com/4128/，那么它的这个postId就是4128。为了简化程序，这里就不讲解如何自动获取这个postId了，本实例直接指定postId。如果想要自动获取，可以使用beautifulsoup解析http://date.jobbole.com/返回的信息。beautifulsoup的使用。有机会的话，会在后面的爬虫笔记中进行讲解。 (3).测试1)将Cookie保存到变量中 首先，我们先利用CookieJar对象实现获取cookie的功能，存储到变量中，先来感受一下,执行文件cookie01.py： 1234567891011121314151617# -*- coding: UTF-8 -*-from urllib import requestfrom http import cookiejarif __name__ == '__main__': #声明一个CookieJar对象实例来保存cookie cookie = cookiejar.CookieJar() #利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler handler=request.HTTPCookieProcessor(cookie) #通过CookieHandler创建opener opener = request.build_opener(handler) #此处的open方法打开网页 response = opener.open('http://www.baidu.com') #打印cookie信息 for item in cookie: print('Name = %s' % item.name) print('Value = %s' % item.value) 我们使用以上方法将cookie保存到变量中，然后打印出了cookie中的值，运行结果如下: 2)保存Cookie到文件 在上面的方法中，我们将cookie保存到了cookie这个变量中，如果我们想将cookie保存到文件中该怎么做呢？方便以后直接读取文件使用，这时，我们就要用到FileCookieJar这个对象了，在这里我们使用它的子类MozillaCookieJar来实现Cookie的保存，编写代码如下,文件为cookie02.py：123456789101112131415161718# -*- coding: UTF-8 -*-from urllib import requestfrom http import cookiejarif __name__ == '__main__': #设置保存cookie的文件，同级目录下的cookie.txt filename = 'cookie.txt' #声明一个MozillaCookieJar对象实例来保存cookie，之后写入文件 cookie = cookiejar.MozillaCookieJar(filename) #利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler handler=request.HTTPCookieProcessor(cookie) #通过CookieHandler创建opener opener = request.build_opener(handler) #此处的open方法打开网页 response = opener.open('http://www.baidu.com') #保存cookie到文件 cookie.save(ignore_discard=True, ignore_expires=True) cookie.save的参数说明： ignore_discard的意思是即使cookies将被丢弃也将它保存下来； ignore_expires的意思是如果在该文件中cookies已经存在，则覆盖原文件写入。 在这里，我们将这两个全部设置为True。 运行之后，cookies将被保存到cookie.txt文件中。我们可以查看自己查看下cookie.txt这个文件的内容。 3)从文件中获取Cookie并访问 我们已经做到把Cookie保存到文件中了，如果以后想使用，可以利用下面的方法来读取cookie并访问网站，感受一下cookie03.py：12345678910111213141516171819# -*- coding: UTF-8 -*-from urllib import requestfrom http import cookiejarif __name__ == '__main__': #设置保存cookie的文件的文件名,相对路径,也就是同级目录下 filename = 'cookie.txt' #创建MozillaCookieJar实例对象 cookie = cookiejar.MozillaCookieJar() #从文件中读取cookie内容到变量 cookie.load(filename, ignore_discard=True, ignore_expires=True) #利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler handler=request.HTTPCookieProcessor(cookie) #通过CookieHandler创建opener opener = request.build_opener(handler) #此用opener的open方法打开网页 response = opener.open('http://www.baidu.com') #打印信息 print(response.read().decode('utf-8')) 了解到以上内容，我们那就可以开始正式编写模拟登陆伯乐在线的程序了。同时，我们也可以获取相亲MM的联系方式。 (4).编写代码我们利用CookieJar对象实现获取cookie的功能，存储到变量中。然后使用这个cookie变量创建opener，使用这个设置好cookie的opener即可模拟登陆，同笔记四中讲到的IP代理的使用方法类似。 创建cookie_test.py文件，编写代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# -*- coding: UTF-8 -*-from urllib import requestfrom urllib import errorfrom urllib import parsefrom http import cookiejarif __name__ == '__main__': #登陆地址 login_url = 'http://www.jobbole.com/wp-admin/admin-ajax.php' #User-Agent信息 user_agent = r'Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36' #Headers信息 head = &#123;'User-Agnet': user_agent, 'Connection': 'keep-alive'&#125; #登陆Form_Data信息 Login_Data = &#123;&#125; Login_Data['action'] = 'user_login' Login_Data['redirect_url'] = 'http://www.jobbole.com/' Login_Data['remember_me'] = '0' #是否一个月内自动登陆 Login_Data['user_login'] = '********' #改成你自己的用户名 Login_Data['user_pass'] = '********' #改成你自己的密码 #使用urlencode方法转换标准格式 logingpostdata = parse.urlencode(Login_Data).encode('utf-8') #声明一个CookieJar对象实例来保存cookie cookie = cookiejar.CookieJar() #利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler cookie_support = request.HTTPCookieProcessor(cookie) #通过CookieHandler创建opener opener = request.build_opener(cookie_support) #创建Request对象 req1 = request.Request(url=login_url, data=logingpostdata, headers=head) #面向对象地址 date_url = 'http://date.jobbole.com/wp-admin/admin-ajax.php' #面向对象 Date_Data = &#123;&#125; Date_Data['action'] = 'get_date_contact' Date_Data['postId'] = '4128' #使用urlencode方法转换标准格式 datepostdata = parse.urlencode(Date_Data).encode('utf-8') req2 = request.Request(url=date_url, data=datepostdata, headers=head) try: #使用自己创建的opener的open方法 response1 = opener.open(req1) response2 = opener.open(req2) html = response2.read().decode('utf-8') index = html.find('jb_contact_email') #打印查询结果 print('联系邮箱:%s' % html[index+19:-2]) except error.URLError as e: if hasattr(e, 'code'): print("HTTPError:%d" % e.code) elif hasattr(e, 'reason'): print("URLError:%s" % e.reason) (5).运行结果如下。]]></content>
      <categories>
        <category>Spider</category>
        <category>Urllib</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Spider</tag>
        <tag>Urllib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始学爬虫-04]]></title>
    <url>%2F2018%2F02%2F28%2F%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E7%88%AC%E8%99%AB-04%2F</url>
    <content type="text"><![CDATA[说在前面urllib_test10.py已经无法爬取了，因为原网站已经添加了防爬虫机制 (一)、为何要设置User Agent有一些网站不喜欢被爬虫程序访问，所以会检测连接对象，如果是爬虫程序，也就是非人点击访问，它就会不让你继续访问，所以为了要让程序可以正常运行，需要隐藏自己的爬虫程序的身份。此时，我们就可以通过设置User Agent的来达到隐藏身份的目的，User Agent的中文名为用户代理，简称UA。 User Agent存放于Headers中，服务器就是通过查看Headers中的User Agent来判断是谁在访问。在Python中，如果不设置User Agent，程序将使用默认的参数，那么这个User Agent就会有Python的字样，如果服务器检查User Agent，那么没有设置User Agent的Python程序将无法正常访问网站。 Python允许我们修改这个User Agent来模拟浏览器访问，它的强大毋庸置疑。 (二)、常见的User Agent(1).Android Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19 Mozilla/5.0 (Linux; U; Android 4.0.4; en-gb; GT-I9300 Build/IMM76D) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30 Mozilla/5.0 (Linux; U; Android 2.2; en-gb; GT-P1000 Build/FROYO) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1 (2).Firefox Mozilla/5.0 (Windows NT 6.2; WOW64; rv:21.0) Gecko/20100101 Firefox/21.0 Mozilla/5.0 (Android; Mobile; rv:14.0) Gecko/14.0 Firefox/14.0 (3).Google Chrome Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36 Mozilla/5.0 (Linux; Android 4.0.4; Galaxy Nexus Build/IMM76B) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.133 Mobile Safari/535.19 (4).iOS Mozilla/5.0 (iPad; CPU OS 5_0 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9A334 Safari/7534.48.3 Mozilla/5.0 (iPod; U; CPU like Mac OS X; en) AppleWebKit/420.1 (KHTML, like Gecko) Version/3.0 Mobile/3A101a Safari/419.3 上面列举了Andriod、Firefox、Google Chrome、iOS的一些User Agent，直接copy就能用。 (三)、设置User Agent的方法先看下urllib.request.Request() 从上图可以看出，在创建Request对象的时候，可以传入headers参数。因此，想要设置User Agent，有两种方法： 1.在创建Request对象的时候，填入headers参数(包含User Agent信息)，这个Headers参数要求为字典； 2.在创建Request对象的时候不添加headers参数，在创建完成之后，使用add_header()的方法，添加headers。 方法一创建文件urllib_test08.py，使用上面提到的Android的第一个User Agent，在创建Request对象的时候传入headers参数，编写代码如下：12345678910111213141516# -*- coding: UTF-8 -*-from urllib import requestif __name__ == "__main__": url = 'http://www.csdn.net/' head = &#123;&#125; #写入User Agent信息 head['User-Agent'] = 'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19' #创建Request对象 req = request.Request(url, headers=head) #传入创建好的Request对象 response = request.urlopen(req) #读取响应信息并解码 html = response.read().decode('utf-8') #打印信息 print(html) 运行结果如下： 方法二创建文件urllib_test09.py，使用上面提到的Android的第一个User Agent，在创建Request对象时不传入headers参数，创建之后使用add_header()方法，添加headers，编写代码如下：12345678910111213141516# -*- coding: UTF-8 -*-from urllib import requestif __name__ == "__main__": #以CSDN为例，CSDN不更改User Agent是无法访问的 url = 'http://www.csdn.net/' #创建Request对象 req = request.Request(url) #传入headers req.add_header('User-Agent', 'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19') #传入创建好的Request对象 response = request.urlopen(req) #读取响应信息并解码 html = response.read().decode('utf-8') #打印信息 print(html) 运行结果和上一个方法是一样的。 (四)、IP代理的使用(1).为何使用IP代理User Agent已经设置好了，但是还应该考虑一个问题，程序的运行速度是很快的，如果我们利用一个爬虫程序在网站爬取东西，一个固定IP的访问频率就会很高，这不符合人为操作的标准，因为人操作不可能在几ms内，进行如此频繁的访问。所以一些网站会设置一个IP访问频率的阈值，如果一个IP访问频率超过这个阈值，说明这个不是人在访问，而是一个爬虫程序。 (2).一般步骤说明一个很简单的解决办法就是设置延时，但是这显然不符合爬虫快速爬取信息的目的，所以另一种更好的方法就是使用IP代理。使用代理的步骤： (1) 调用urlib.request.ProxyHandler()，proxies参数为一个字典。 (2) 创建Opener(类似于urlopen，这个代开方式是我们自己定制的) (3) 安装Opener 使用install_opener方法之后，会将程序默认的urlopen方法替换掉。也就是说，如果使用install_opener之后，在该文件中，再次调用urlopen会使用自己创建好的opener。如果不想替换掉，只是想临时使用一下，可以使用opener.open(url)，这样就不会对程序默认的urlopen有影响。 (3).代理IP选取在写代码之前，先在代理IP网站选好一个IP地址，推荐西刺代理IP。 URL：http://www.xicidaili.com/ 注意：当然也可以写个正则表达式从网站直接爬取IP，但是要记住不要太频繁爬取，加个延时什么的，太频繁给服务器带来压力了，服务器会直接把你block，不让你访问的，我就被封了两天。 从西刺网站选出信号好的IP，我的选择如下：(106.46.136.112:808) 编写代码访问http://www.whatismyip.com.tw/，该网站是测试自己IP为多少的网址，服务器会返回访问者的IP。 (4).代码实例创建文件urllib_test10.py，编写代码如下：12345678910111213141516171819202122# -*- coding: UTF-8 -*-from urllib import requestif __name__ == "__main__": #访问网址 url = 'http://www.whatismyip.com.tw/' #这是代理IP proxy = &#123;'http':'106.46.136.112:808'&#125; #创建ProxyHandler proxy_support = request.ProxyHandler(proxy) #创建Opener opener = request.build_opener(proxy_support) #添加User Angent opener.addheaders = [('User-Agent','Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36')] #安装OPener request.install_opener(opener) #使用自己安装好的Opener response = request.urlopen(url) #读取相应信息并解码 html = response.read().decode("utf-8") #打印信息 print(html) 运行结果如下： 从上图可以看出，访问的IP已经伪装成了106.46.136.112。]]></content>
      <categories>
        <category>Spider</category>
        <category>Urllib</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Spider</tag>
        <tag>Urllib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始学爬虫-03]]></title>
    <url>%2F2018%2F02%2F28%2F%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E7%88%AC%E8%99%AB-03%2F</url>
    <content type="text"><![CDATA[urllib.errorurllib.error可以接收有urllib.request产生的异常。urllib.error有两个方法，URLError和HTTPError。如下图所示： URLError是OSError的一个子类，HTTPError是URLError的一个子类，服务器上HTTP的响应会返回一个状态码，根据这个HTTP状态码，我们可以知道我们的访问是否成功。例如第二个笔记中提到的200状态码，表示请求成功，再比如常见的404错误等。 (1).URLError让我们先看下URLError的异常，创建文件urllib_test05.py，编写如下代码：1234567891011121314# -*- coding: UTF-8 -*-from urllib import requestfrom urllib import errorif __name__ == "__main__": #一个不存在的连接 url = "http://www.dskfclyfiydl.com/" req = request.Request(url) try: response = request.urlopen(req) html = response.read().decode('utf-8') print(html) except error.URLError as e: print(e.reason) 可以看到如下运行结果： (2).HTTPError再看下HTTPError异常，创建文件urllib_test06.py，编写如下代码：12345678910111213# -*- coding: UTF-8 -*-from urllib import requestfrom urllib import errorif __name__ == "__main__": #一个不存在的连接 url = "http://www.douyu.com/wkx.html" req = request.Request(url) try: responese = request.urlopen(req) # html = responese.read() except error.HTTPError as e: print(e.code, '\n' ,e.reason, '\n', e.headers) 运行之后，我们可以看到404，这说明请求的资源没有在服务器上找到，www.douyu.com 这个服务器是存在的，但是我们要查找的 Jack_Cui.html 资源是没有的，所以抛出404异常。 (3).URLError和HTTPError混合使用最后值得注意的一点是，如果想用HTTPError和URLError一起捕获异常，那么需要将HTTPError放在URLError的前面，因为HTTPError是URLError的一个子类。如果URLError放在前面，出现HTTP异常会先响应URLError，这样HTTPError就捕获不到错误信息了。 如果不用上面的方法，也可以使用hasattr函数判断URLError含有的属性，如果含有reason属性表明是URLError，如果含有code属性表明是HTTPError。创建文件urllib_test07.py，编写代码如下：1234567891011121314151617# -*- coding: UTF-8 -*-from urllib import requestfrom urllib import errorif __name__ == "__main__": #一个不存在的连接 url = "http://www.douyu.com/wkx.html" req = request.Request(url) try: responese = request.urlopen(req) except error.URLError as e: if hasattr(e, 'code'): print("HTTPError") print(e.code) elif hasattr(e, 'reason'): print("URLError") print(e.reason) 运行结果如下：]]></content>
      <categories>
        <category>Spider</category>
        <category>Urllib</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Spider</tag>
        <tag>Urllib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始学爬虫-02]]></title>
    <url>%2F2018%2F02%2F28%2F%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E7%88%AC%E8%99%AB-02%2F</url>
    <content type="text"><![CDATA[一个疑问尚未解决疑问，小弟在此跪求大牛解答一下为什么把url里的 “_o” 删掉后就可以正常爬取呢？ urlopen的url参数 Agenturl不仅可以是一个字符串，例如:http://www.baidu.com。 url也可以是一个Request对象，这就需要我们先定义一个Request对象，然后将这个Request对象作为urlopen的参数使用，方法如下： 123456789# -*- coding: UTF-8 -*-from urllib import requestif __name__ == "__main__": req = request.Request("http://fanyi.baidu.com/") response = request.urlopen(req) html = response.read() html = html.decode("utf-8") print(html) 同样，运行这段代码同样可以得到网页信息。可以看一下这段代码和上个笔记中代码的不同，对比一下就明白了。 urlopen()返回的对象，可以使用read()进行读取，同样也可以使用geturl()方法、info()方法、getcode()方法。 geturl()返回的是一个url的字符串； info()返回的是一些meta标记的元信息，包括一些服务器的信息； getcode()返回的是HTTP的状态码，如果返回200表示请求成功。 关于META标签和HTTP状态码的内容可以自行百度百科，里面有很详细的介绍。 了解到这些，我们就可以进行新一轮的测试，新建文件名urllib_test04.py，编写如下代码：1234567891011# -*- coding: UTF-8 -*-from urllib import requestif __name__ == "__main__": req = request.Request("http://fanyi.baidu.com/") response = request.urlopen(req) print("geturl打印信息：%s"%(response.geturl())) print('**********************************************') print("info打印信息：%s"%(response.info())) print('**********************************************') print("getcode打印信息：%s"%(response.getcode())) urlopen的data参数我们可以使用data参数，向服务器发送数据。根据HTTP规范，GET用于信息获取，POST是向服务器提交数据的一种请求，再换句话说： 从客户端向服务器提交数据使用POST； 从服务器获得数据到客户端使用GET(GET也可以提交，暂不考虑)。 如果没有设置urlopen()函数的data参数，HTTP请求采用GET方式，也就是我们从服务器获取信息，如果我们设置data参数，HTTP请求采用POST方式，也就是我们向服务器传递数据。 data参数有自己的格式，它是一个基于application/x-www.form-urlencoded的格式，具体格式我们不用了解， 因为我们可以使用urllib.parse.urlencode()函数将字符串自动转换成上面所说的格式。 发送data实例向有道翻译发送data，得到翻译结果。 (1).打开有道翻译界面，如下图所示： (2).鼠标右键检查，也就是审查元素，如下图所示： (3).选择右侧出现的Network，如下图所示： (4).在左侧输入翻译内容，输入Jack，如下图所示： (5).点击自动翻译按钮，我们就可以看到右侧出现的内容，如下图所示： (6).点击上图红框中的内容，查看它的信息，如下图所示： (7).记住这些信息，这是我们一会儿写程序需要用到的。 新建文件translate_test.py，编写如下代码： 1234567891011121314151617181920212223242526272829# -*- coding: UTF-8 -*-from urllib import requestfrom urllib import parseimport jsonif __name__ == "__main__": #对应上图的Request URL Request_URL = 'http://fanyi.youdao.com/translate?smartresult=dict&amp;smartresult=rule&amp;sessionFrom=null' #创建Form_Data字典，存储上图的Form Data Form_Data = &#123;&#125; Form_Data['type'] = 'AUTO' Form_Data['i'] = 'Jack' Form_Data['doctype'] = 'json' Form_Data['xmlVersion'] = '1.8' Form_Data['keyfrom'] = 'fanyi.web' Form_Data['ue'] = 'ue:UTF-8' Form_Data['action'] = 'FY_BY_CLICKBUTTON' #使用urlencode方法转换标准格式 data = parse.urlencode(Form_Data).encode('utf-8') #传递Request对象和转换完格式的数据 response = request.urlopen(Request_URL,data) #读取信息并解码 html = response.read().decode('utf-8') #使用JSON translate_results = json.loads(html) #找到翻译结果 translate_results = translate_results['translateResult'][0][0]['tgt'] #打印翻译信息 print("翻译的结果是：%s" % translate_results) 运行查看翻译结果]]></content>
      <categories>
        <category>Spider</category>
        <category>Urllib</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Spider</tag>
        <tag>Urllib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始学爬虫-01]]></title>
    <url>%2F2018%2F02%2F28%2F%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E7%88%AC%E8%99%AB-01%2F</url>
    <content type="text"><![CDATA[本节关键字urllib | chardet urllib 简介在Python3.x中，我们可以使用urlib这个组件抓取网页，urllib是一个URL处理包，这个包中集合了一些处理URL的模块，如下： 1.urllib.request模块是用来打开和读取URLs的； 2.urllib.error模块包含一些有urllib.request产生的错误，可以使用try进行捕捉处理； 3.urllib.parse模块包含了一些解析URLs的方法； 4.urllib.robotparser模块用来解析robots.txt文本文件.它提供了一个单独的RobotFileParser类，通过该类提供的can_fetch()方法测试爬虫是否可以下载一个页面。 使用urllib.request.urlopen()这个接口函数就可以访问一个网站，读取并打印信息。urlopen有一些可选参数，具体信息可以查阅Python自带的documentation。 urllib 测试了解到这些，我们就可以写一个最简单的程序，文件名为urllib_test01.py，感受一个urllib库的魅力：1234567# -*- coding: UTF-8 -*-from urllib import requestif __name__ == "__main__": response = request.urlopen("http://fanyi.baidu.com") html = response.read() print(html) urllib使用使用request.urlopen()访问和读取URLs信息，返回的对象response如同一个文本对象，我们可以调用read()，进行读取。再通过print()屏幕打印。 浏览器就是作为客户端从服务器端获取信息，然后将信息解析，再展示给我们的。但是显然他们都是二进制的乱码。 我们可以通过简单的decode()命令将网页的信息进行解码，并显示出来，我们新创建一个文件，命名为urllib_test02.py，编写如下代码： 12345678# -*- coding: UTF-8 -*-from urllib import requestif __name__ == "__main__": response = request.urlopen("http://fanyi.baidu.com/") html = response.read() html = html.decode("utf-8") print(html) 这样我们就可以得到这样的结果，显然解码后的信息看起来工整和舒服多了： 当然这个前提是我们已经知道了这个网页是使用utf-8编码的，怎么查看网页的编码方式呢？非常简单的方法是使用使用浏览器审查元素，只需要找到head标签开始位置的chareset，就知道网页是采用何种编码。 这样我们就知道了这个网站的编码方式，但是这需要我们每次都打开浏览器，并找下编码方式，显然有些费事，使用几行代码解决更加省事并且显得酷一些。 我们需要安装第三方库chardet，它是用来判断编码的模块。安装好后，我们就可以使用chardet.detect()方法，判断网页的编码方式了。至此，我们就可以编写一个小程序判断网页的编码方式了，新建文件名为chardet_test01.py：123456789# -*- coding: UTF-8 -*-from urllib import requestimport chardetif __name__ == "__main__": response = request.urlopen("http://fanyi.baidu.com") html = response.read() charset = chardet.detect(html) print(charset)]]></content>
      <categories>
        <category>Spider</category>
        <category>Urllib</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Spider</tag>
        <tag>Urllib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一种Git保留两个repo的commit信息进行合并的方法]]></title>
    <url>%2F2018%2F02%2F27%2F%E4%B8%80%E7%A7%8DGit%E4%BF%9D%E7%95%99%E4%B8%A4%E4%B8%AArepo%E7%9A%84commit%E4%BF%A1%E6%81%AF%E8%BF%9B%E8%A1%8C%E5%90%88%E5%B9%B6%E7%9A%84%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[以往的合并时首先要删除repo的.git文件夹，然后重新add-commit-push。带来的问题是会丢失某一个仓库的提交信息，不利于时光倒退。经过摸索终于实现了保留两个仓库提交信息的合并方法。介绍如下： 比如要将DownloadPicsBySeleniumAndPhantomJS这个项目合并到Web-Spider中，终端中执行：12345$ git remote add other git@github.com:ByiProX/****.git$ git fetch other$ git checkout -b repo1 other/mster$ git checkout master$ git merge repo1 --allow-unrelated-histories 接下来解决merge冲突即可（可以尝试使用mergetool），如有需要可以删除多余分支和远程连接 1$ git mergetool # 解决merge冲突 123$ git remote rm other # 删除远程连接 $ git branch -d repo1 # 删除分支操作]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 使用Selenium&PhantomJS爬火影忍者漫画]]></title>
    <url>%2F2018%2F02%2F27%2FPython3-%E4%BD%BF%E7%94%A8Selenium-PhantomJS%E7%88%AC%E7%81%AB%E5%BD%B1%E5%BF%8D%E8%80%85%E6%BC%AB%E7%94%BB%2F</url>
    <content type="text"><![CDATA[近期学习爬虫，发现懂的越多，不懂的知识点越多（所以当个傻子还是很幸福的）。好记性不如烂笔头，之前都是把看到的资料链接直接挂到一些平台，比如知乎、简书、Github等。今天有点时间，就好好码一下字，排排版，方便以后查阅。 Selenium用来模拟浏览器的行为，比如点击、最大化、滚动窗口等；PhantomJS是一种浏览器，不过这种浏览器没有UI界面，感觉就像是专门为爬虫设计，优点很明显，可以有效减小内存的使用。 爬虫使用到的模块12345from selenium import webdriverfrom myLogging import MyLoggingimport osimport timeimport re myLogging模块是自己配置的日志包，想要的可以点击这里自己看 爬虫很关键的一点就是能够看懂网页的源代码，记得当初刚刚真正开始接触编程的时候，有很长的一段时间在看HTML、CSS、JS的一些知识，虽然忘得很多，但是印象还是有的，对于后面看网页源代码很有帮助。学习爬虫，除了会基本的python知识以外，还要会网页的一些知识。 爬取图片思路： 已知连接，分析网页的代码结构，看所需的数据是否需要切换frame，并定位所需数据的位于哪个标签之下 采用不同的模块有不同的保存图片方式，如果采用request模块，保存图片方式是可以采用requests.get(comicUrl).content方法，使用该方法需要确定网页的地址。该项目中没有涉及request的使用，所以此后不再表述。对于selenium可以使用 get_screenshot_as_file() 方法，使用该方法强烈建议使用phantomjs，如果使用chrome浏览器，图片尺寸太大的话，会出现截图不完整，对比如下： 找到下一张图片的连接位置并点击更新网页，一般来讲新网页与之前网页结构相同。 在新网页的基础上保存图片，设置循环如此反复。 爬取网页的URL为：爬取火影漫画第一话 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class DownloadPics(object): def __init__(self, url): self.url = url self.log = MyLogging() self.browser = self.get_browser() self.save_pics(self.browser) def get_browser(self): browser = webdriver.PhantomJS() try: browser.get(self.url) except: MyLogging.error('open the url %s failed' % self.url) browser.implicitly_wait(20) return browser def save_pics(self, browser): pics_title = browser.title.split('_')[0] self.create_dir(pics_title) os.chdir(pics_title) sum_page = self.find_total_page_num(browser) i = 1 while i &lt; sum_page: image_name = str(i) + '.png' browser.get_screenshot_as_file(image_name) # 使用PhantomJS避免了截图的不完整，可以与Chrome比较 self.log.info('saving image %s' % image_name) i += 1 css_selector = "a[href='/comiclist/3/3/%s.htm']" % i # 该方法感觉还不错呢，不过这个网站确实挺差劲的 next_page = browser.find_element_by_css_selector(css_selector) next_page.click() time.sleep(2) # browser.implicitly_wait(20) def find_total_page_num(self, browser): page_element = browser.find_element_by_css_selector("table[cellspacing='1']") num = re.search(r'共\d+页', page_element.text).group()[1:-1] return int(num) def create_dir(self, dir_name): if os.path.exists(dir_name): self.log.error('create directory %s failed cause a same directory exists' % dir_name) else: try: os.makedirs(dir_name) except: self.log.error('create directory %s failed' % dir_name) else: self.log.info('create directory %s success' % dir_name)if __name__ == '__main__': start_url = 'http://comic.kukudm.com/comiclist/3/3/1.htm' DL = DownloadPics(start_url) 运行结果]]></content>
      <categories>
        <category>Spider</category>
        <category>Selenium</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Spider</tag>
        <tag>Selenium</tag>
        <tag>PhantomJS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django2.0.1搭建电影网站]]></title>
    <url>%2F2018%2F02%2F27%2FDjango2-0-1%E6%90%AD%E5%BB%BA%E7%94%B5%E5%BD%B1%E7%BD%91%E7%AB%99%2F</url>
    <content type="text"><![CDATA[本项目已经部署到服务器，可以通过该IP查看http://59.110.221.56/GitHub源代码 技术栈 Bootstrap 3 + Django 2.0.1 + MySQL 5.7.17 + Nginx + locust 本地服务运行方法终端在venv文件夹路径下开启虚拟环境(根据自己的shell进行选择)： 123source ../venv/bin/activate.fishsource ../venv/bin/activatesource ../venv/bin/activate.csh 然后执行：1python3 TWS_Cinema/manage.py runserver 如果报错，终端进入requirements.txt所在目录，运行命令：1pip3 install -r requirements.txt 然后执行：1python3 TWS_Cinema/manage.py runserver 单元测试运行方法在manage.py路径下终端运行 1python3 manage.py test 网站功能描述 实现导航栏搜索电影，支持按年份搜索和类型搜索 – 显示分类列表 – 点击分类显示符合分类要求的电影 实现搜索功能，支持按电影名称模糊搜索 实现电影详细信息查看功能 – 显示电影详细信息 – 显示豆瓣 Top 5 影评 – 在电影详细页面显示相似电影推荐 – 增加电影观看链接 API 按电影id搜索 —— api/movie/id/ # 例如：api/movie/id/1291545 按电影名搜索 —— api/movie/title/ # 例如：api/movie/title/大鱼 按电影原始名搜索 —— api/movie/original_title/ # 例如：api/movie/original_title/Big Fish 按电影类型搜索 —— api/movie/genre/ # 例如：api/movie/genre/剧情 按电影年份搜索 —— api/movie/year/ # 例如：api/movie/year/2003 网站性能测试结果在文件locustfile.py路径下运行1locust --host=http://59.110.221.56 压力测试 采取的框架：locust 服务器性能： CPU：1核 内存：2 GB (I/O优化) 带宽：1Mbps 测试结果： 500人：100%正确 1000人：40%出错率 测试截图 电影网站的其他截图 ReferenceLocust 简介以及使用]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Django</tag>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简谈爬虫攻与防]]></title>
    <url>%2F2018%2F02%2F27%2F%E7%AE%80%E8%B0%88%E7%88%AC%E8%99%AB%E6%94%BB%E4%B8%8E%E9%98%B2%2F</url>
    <content type="text"><![CDATA[封锁间隔时间破解Scrapy在两次请求之间的时间设置是DOWNLOAD_DELAY。如果不考虑反爬虫的因素，该值当然越小越好。如果DOWNLOAD_DELAY设为0.001，也就是每1毫秒请求一次网页，这简直非人类干的事情。有些网站会检测一个ip的访问时间，异常情况下会封锁该ip。 封锁Cookies众所周知，网站是通过Cookie来确定用户身份的，Scrapy在爬取数据时使用同一个Cookies发起请求。该做法和把DOWNLOAD_DELAY设为0.001没有本质区别。在scrapy中，直接社禁用Cookies就可以了。在settings.py中设置12# Disable cookies (enabled by default)COOKIES_ENABLED = False 封锁user-agent和proxy破解user-agent是浏览器的身份标识。网站通过UA来确定浏览器类型。很多浏览器拒绝不符合一定标准的UA请求网页。同一个UA高频率的访问网站会有被网站列入黑名单的危险。破解的方法很简单，可以准备一个UA池，每次请求时随机挑选一个进行请求。 在middlewares.py同级目录下创建UAResource.py,文件内容如下： 12345678910111213141516171819202122232425#-*- coding: utf-8 -*-UserAgents = [ "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)", "Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)", "Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)", "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)", "Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)", "Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)", "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)", "Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6", "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1", "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0", "Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5", "Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11", "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20", "Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52",]Proxies = ['http://122.114.31.177:808','http://1.2.3.4:80',] 修改middlewares.py，添加内容为1234567891011121314from .UAResource import UserAgentsfrom .UAResource import Proxiesimport randomclass RandomProxy(object): def process_request(self, request, spider): proxy = random.choice(Proxies) request.meta['proxy'] = proxyclass RandomUserAgent(object): """docstring for RandomUerAgent.""" def process_request(self, request, spider): ua = random.choice(UserAgents) request.headers.setdefault('User-Agent', ua) 最后修改setting.py,将RandomUserAgent和RandomProxy添加到DOWNLOADER_MIDDLEWARES中，注意RandomProxy要放到RandomUserAgent之前，即将RandomProxy的值比RandomUserAgent后的值小12345678DOWNLOADER_MIDDLEWARES = &#123; # 'meijutt.middlewares.MeijuttDownloaderMiddleware': 543, 'meijutt.middlewares.RandomProxy': 10, 'meijutt.middlewares.RandomUserAgent': 30, # 禁止内置的(在 DOWNLOADER_MIDDLEWARES_BASE 中设置并默认启用的)中间件 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,&#125; 免费代理不够稳定，如果不想用proxy，设置RandomProxy为None,即禁止使用1'meijutt.middlewares.RandomProxy': None,]]></content>
      <categories>
        <category>Spider</category>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Spider</tag>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Github多分支管理Hexo-Blog项目]]></title>
    <url>%2F2018%2F02%2F27%2FGithub%E5%A4%9A%E5%88%86%E6%94%AF%E7%AE%A1%E7%90%86Hexo-Blog%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[Hexo在部署之后在github的仓库中我们只能找到生成的静态文件。然而博客的源文件：主题、文章、配置等文件都还在本地，并没有备份。对于多台终端设备的用户不够友好，而且存在一定的风险，万一那天电脑坏了或者是出现一些其他问题，就得从头再来。为了解决上述问题，我们可以利用github的分支思想来备份我们的源文件。 备份之前，需要了解博客根目录下面的文件以及文件夹作用：123456789.deploy_git/ 网站静态文件(git)node_modules/ 插件public/ 网站静态文件scaffolds/ 文章模板source/ 博文等themes/ 主题_config.yml 网站配置文件package.json Hexo信息db.json 数据文件 备份的思路master分支存放部署生成的静态文件，Hexo-Bog分支存放我们要备份项目源文件。实际备份中，.deploy_git、public文件夹和我们的master分支内容重复，所以略过。因此，我们在根目录下面建一个.gitignore文件来建立“黑名单”，禁止备份。 编辑.gitignore过滤文件文件内容如下：123.DS_Storepublic/.deploy*/ 关于备份终端中在项目的根目录下执行，对于作者自己的项目，命令执行的路径为ByiProX/下：123456$ git init$ git remote add origin git@github.com:username/username.github.io.git # username为博客项目的名称，也就是git的用户名$ git add .$ git commit -m "ready for backup of the project"$ git push origin master:Hexo-Blog 执行完毕后会发现github博客仓库已经有了一个新分支Hexo-Blog，于是备份工作完成。以后，开始写博文时，即终端运行1$ hexo new [layout] &lt;title&gt; 完成文章后,对编辑后的文章进行备份保存，即终端运行,为下面的部署做准备123$ git add .$ git commit -m "add one article"$ git push origin master:Hexo-Blog 部署运行一下命令进行仓库master分支静态文件部署123$ hexo clean$ hexo generate$ hexo deploy 以上完成项目源文件以及静态文件的Git管理 参考文献及进阶Hexo+github搭建个人博客并实现多终端管理如何在github上面备份HexoHexo的版本控制与持续集成使用hexo，如果换了电脑怎么更新博客]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F02%2F26%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. 非常好的一篇markdown参考手册 Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
