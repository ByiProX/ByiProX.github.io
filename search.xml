<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python 自省]]></title>
    <url>%2F2018%2F03%2F01%2FPython-%E8%87%AA%E7%9C%81%2F</url>
    <content type="text"><![CDATA[自省是python彪悍的特性之一. 自省（introspection）是一种自我检查行为。在计算机编程中，自省是指这种能力：检查某些事物以确定它是什么、它知道什么以及它能做什么。自省向程序员提供了极大的灵活性和控制力. 自省就是面向对象的语言所写的程序在运行时,所能知道对象的类型.简单一句就是运行时能够获得对象的类型.比如type(),dir(),getattr(),hasattr(),isinstance(). 12345a = [1,2,3]b = &#123;'a':1,'b':2,'c':3&#125;c = Trueprint type(a),type(b),type(c) # &lt;type 'list'&gt; &lt;type 'dict'&gt; &lt;type 'bool'&gt;print isinstance(a,list) # True 未完待续 参考： http://python.jobbole.com/82110/ http://blog.csdn.net/IAlexanderI/article/details/78768378]]></content>
      <categories>
        <category>Python3 进阶</category>
        <category>Python 自省</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 类变量和实例变量]]></title>
    <url>%2F2018%2F03%2F01%2FPython3-%E7%B1%BB%E5%8F%98%E9%87%8F%E5%92%8C%E5%AE%9E%E4%BE%8B%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[写在前面首先来一张图 类变量和实例变量在Python Tutorial中对于类变量和实例变量是这样描述的： Generally speaking, instance variables are for data unique to each instance and class variables are for attributes and methods shared by all instances of the class: 1234class Dog: kind = 'canine' # class variable shared by all instances def __init__(self, name): self.name = name # instance variable unique to each instance 类Dog中，类属性kind为所有实例所共享；实例属性name为每个Dog的实例独有。 类变量： ​ 是可在类的所有实例之间共享的值（也就是说，它们不是单独分配给每个实例的）。例如下例中，num_of_instance 就是类变量，用于跟踪存在着多少个Test 的实例。 实例变量： 实例化之后，每个实例单独拥有的变量。 12345678910111213class Test(object): num_of_instance = 0 def __init__(self, name): self.name = name Test.num_of_instance += 1 if __name__ == '__main__': print Test.num_of_instance # 0 t1 = Test('jack') print Test.num_of_instance # 1 t2 = Test('lucy') print t1.name , t1.num_of_instance # jack 2 print t2.name , t2.num_of_instance # lucy 2 补充的例子 123456789class Person: name="aaa"p1=Person()p2=Person()p1.name="bbb"print p1.name # bbbprint p2.name # aaaprint Person.name # aaa 这里p1.name=&quot;bbb&quot;是实例调用了类变量,属于函数传参的问题,p1.name一开始是指向的类变量name=&quot;aaa&quot;,但是在实例的作用域里把类变量的引用改变了,就变成了一个实例变量,self.name不再引用Person的类变量name了. 可以看看下面的例子: 123456789class Person: name=[]p1=Person()p2=Person()p1.name.append(1)print p1.name # [1]print p2.name # [1]print Person.name # [1] 类对象和实例对象类对象Python中一切皆对象；类定义完成后，会在当前作用域中定义一个以类名为名字，指向类对象的名字。如12class Dog: pass 会在当前作用域定义名字Dog，指向类对象Dog。 类对象支持的操作：总的来说，类对象仅支持两个操作： 实例化；使用instance_name = class_name()的方式实例化，实例化操作创建该类的实例。 属性引用；使用class_name.attr_name的方式引用类属性。 实例对象实例对象是类对象实例化的产物，实例对象仅支持一个操作: 属性引用；与类对象属性引用的方式相同，使用instance_name.attr_name的方式。 按照严格的面向对象思想，所有属性都应该是实例的，类属性不应该存在。那么在Python中，由于类属性绑定就不应该存在，类定义中就只剩下函数定义了。 在Python tutorial关于类定义也这么说： In practice, the statements inside a class definition will usually be function definitions, but other statements are allowed, and sometimes useful. 实践中，类定义中的语句通常是函数定义，但是其他语句也是允许的，有时也是有用的。 这里说的其他语句，就是指类属性的绑定语句。 属性绑定在定义类时，通常我们说的定义属性，其实是分为两个方面的： 类属性绑定 实例属性绑定 用绑定这个词更加确切；不管是类对象还是实例对象，属性都是依托对象而存在的。 我们说的属性绑定，首先需要一个可变对象，才能执行绑定操作，使用 objname.attr = attr_value 的方式，为对象objname绑定属性attr。 这分两种情况： 若属性attr已经存在，绑定操作会将属性名指向新的对象； 若不存在，则为该对象添加新的属性，后面就可以引用新增属性。 类属性绑定Python作为动态语言，类对象和实例对象都可以在运行时绑定任意属性。因此，类属性的绑定发生在两个地方： 类定义时； 运行时任意阶段。 下面这个例子说明了类属性绑定发生的时期：12345678class Dog: kind = 'canine'Dog.country = 'China'print(Dog.kind, ' - ', Dog.country) # output: canine - Chinadel Dog.kindprint(Dog.kind, ' - ', Dog.country)# AttributeError: type object 'Dog' has no attribute 'kind' 在类定义中，类属性的绑定并没有使用objname.attr = attr_value的方式，这是一个特例，其实是等同于后面使用类名绑定属性的方式。因为是动态语言，所以可以在运行时增加属性，删除属性。 实例属性绑定与类属性绑定相同，实例属性绑定也发生在两个地方： 类定义时； 运行时任意阶段。 示例：123456789class Dog: def __init__(self, name, age): self.name = name self.age = agedog = Dog('Lily', 3)dog.fur_color = 'red'print('%s is %s years old, it has %s fur' % (dog.name, dog.age, dog.fur_color))# Output: Lily is 3 years old, it has red fur Python类实例有两个特殊之处： __init__在实例化时执行 Python实例调用方法时，会将实例对象作为第一个参数传递 因此，__init__方法中的self就是实例对象本身，这里是dog，语句12self.name = nameself.age = age 以及后面的语句1dog.fur_color = 'red' 为实例dog增加三个属性name, age, fur_color。 属性引用类属属性引用类属性的引用，肯定是需要类对象的，属性分为两种： 数据属性 函数属性 数据属性引用很简单，示例：12345class Dog: kind = 'canine'Dog.country = 'China'print(Dog.kind, ' - ', Dog.country) # output: canine - China 通常很少有引用类函数属性的需求，示例：123456class Dog: kind = 'canine' def tell_kind(): print(Dog.kind)Dog.tell_kind() # Output: canine 函数tell_kind在引用kind需要使用Dog.kind而不是直接使用kind，涉及到作用域，这一点在我的另一篇文章中有介绍：Python进阶 - 命名空间与作用域 实例属性引用使用实例对象引用属性稍微复杂一些，因为实例对象可引用类属性以及实例属性。但是实例对象引用属性时遵循以下规则： 总是先到实例对象中查找属性，再到类属性中查找属性； 属性绑定语句总是为实例对象创建新属性，属性存在时，更新属性指向的对象。 数据属性引用示例1：12345678910111213class Dog: kind = 'canine' country = 'China' def __init__(self, name, age, country): self.name = name self.age = age self.country = countrydog = Dog('Lily', 3, 'Britain')print(dog.name, dog.age, dog.kind, dog.country)# output: Lily 3 canine Britain 类对象Dog与实例对象dog均有属性country，按照规则，dog.country会引用到实例对象的属性；但实例对象dog没有属性kind，按照规则会引用类对象的属性。 示例2：123456789101112131415161718class Dog: kind = 'canine' country = 'China' def __init__(self, name, age, country): self.name = name self.age = age self.country = countrydog = Dog('Lily', 3, 'Britain')print(dog.name, dog.age, dog.kind, dog.country) # Lily 3 canine Britainprint(dog.__dict__) # &#123;'name': 'Lily', 'age': 3, 'country': 'Britain'&#125;dog.kind = 'feline'print(dog.name, dog.age, dog.kind, dog.country) # Lily 3 feline Britainprint(dog.__dict__) # &#123;'name': 'Lily', 'age': 3, 'country': 'Britain', 'kind': 'feline'&#125;print(Dog.kind) # canine 没有改变类属性的指向 示例3，可变类属性引用： 1234567891011121314151617class Dog: tricks = [] def __init__(self, name): self.name = name def add_trick(self, trick): # self.tricks.append(trick) Dog.tricks.append(trick)d = Dog('Fido')e = Dog('Buddy')d.add_trick('roll over')e.add_trick('play dead')print(d.tricks) # ['roll over', 'play dead'] 语句self.tricks.append(trick)并不是属性绑定语句，因此还是在类属性上修改可变对象。 方法属性引用与数据成员不同，类函数属性在实例对象中会变成方法属性。先看一个示例： 1234567891011121314class MethodTest: def inner_test(self): print('in class')def outer_test(): print('out of class')mt = MethodTest()mt.outer_test = outer_testprint(type(MethodTest.inner_test)) # &lt;class 'function'&gt; 类函数print(type(mt.inner_test)) #&lt;class 'method'&gt; 类方法print(type(mt.outer_test)) #&lt;class 'function'&gt; 类函数 可以看到，类函数属性在实例对象中变成了方法属性，但是并不是实例对象中所有的函数都是方法。 Python tutorial中这样介绍方法对象： When an instance attribute is referenced that isn’t a data attribute, its class is searched. If the name denotes a valid class attribute that is a function object, a method object is created by packing (pointers to) the instance object and the function object just found together in an abstract object: this is the method object. When the method object is called with an argument list, a new argument list is constructed from the instance object and the argument list, and the function object is called with this new argument list. 引用非数据属性的实例属性时，会搜索它对应的类。如果名字是一个有效的函数对象，Python会将实例对象连同函数对象打包到一个抽象的对象中并且依据这个对象创建方法对象：这就是被调用的方法对象。当使用参数列表调用方法对象时，会使用实例对象以及原有参数列表构建新的参数列表，并且使用新的参数列表调用函数对象。 那么，实例对象只有在引用方法属性时，才会将自身作为第一个参数传递；调用实例对象的普通函数，则不会。所以可以使用如下方式直接调用方法与函数： 12mt.inner_test()mt.outer_test() 除了方法与函数的区别，其引用与数据属性都是一样的 最佳实践虽然Python作为动态语言，支持在运行时绑定属性，但是从面向对象的角度来看，还是在定义类的时候将属性确定下来。 参考: http://stackoverflow.com/questions/6470428/catch-multiple-exceptions-in-one-line-except-block https://www.cnblogs.com/crazyrunning/p/6945183.html]]></content>
      <categories>
        <category>Python3 进阶</category>
        <category>类变量 和 实例变量</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 @staticmethod和@classmethod]]></title>
    <url>%2F2018%2F03%2F01%2FPython3-staticmethod%E5%92%8C-classmethod%2F</url>
    <content type="text"><![CDATA[Python其实有3个方法,即静态方法(staticmethod),类方法(classmethod)和实例方法,如下: 12345678910111213141516def foo(x): print "executing foo(%s)"%(x)class A(object): def foo(self,x): print "executing foo(%s,%s)"%(self,x) @classmethod def class_foo(cls,x): print "executing class_foo(%s,%s)"%(cls,x) @staticmethod def static_foo(x): print "executing static_foo(%s)"%xa=A() 这里先理解下函数参数里面的self和cls。这个self和cls是对类或者实例的绑定,对于一般的函数来说我们可以这么调用foo(x),这个函数就是最常用的,它的工作跟任何东西(类,实例)无关。对于实例方法,我们知道在类里每次定义方法的时候都需要绑定这个实例,就是foo(self, x),为什么要这么做呢?因为实例方法的调用离不开实例,我们需要把实例自己传给函数,调用的时候是这样的a.foo(x)(其实是foo(a, x))。类方法一样,只不过它传递的是类而不是实例,A.class_foo(x)。注意这里的self和cls可以替换别的参数,但是python的约定是这俩,还是不要改的好。 对于静态方法其实和普通的方法一样,不需要对谁进行绑定,唯一的区别是调用的时候需要使用a.static_foo(x)或者A.static_foo(x)来调用. \ 实例方法 类方法 静态方法 a = A() a.foo(x) a.class_foo(x) a.static_foo(x) A 不可用 A.class_foo(x) A.static_foo(x) 更多关于这个问题: http://stackoverflow.com/questions/136097/what-is-the-difference-between-staticmethod-and-classmethod-in-python https://realpython.com/blog/python/instance-class-and-static-methods-demystified/]]></content>
      <categories>
        <category>Python3 进阶</category>
        <category>Staticmethod &amp; Classmethod</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo博客Next主题添加Fork me on GitHub标签]]></title>
    <url>%2F2018%2F02%2F28%2FHexo%E5%8D%9A%E5%AE%A2Next%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0Fork-me-on-GitHub%E6%A0%87%E7%AD%BE%2F</url>
    <content type="text"><![CDATA[给自己的个人博客添加Fork me on GitHub标签感觉很专业很逼格，添加的方法也很简单，介绍如下。 打开文件：hexo博客根目录/themes/next/layout/_layout.swig 找到如下代码块12345...&lt;div class="&#123;&#123; container_class &#125;&#125; &#123;% block page_class %&#125;&#123;% endblock %&#125; "&gt; &lt;div class="headband"&gt;&lt;/div&gt; [样式代码]... 样式代码 看这里 ，挑选自己喜欢的样式。 然后将样式代码添加到上述 _layout.swig 代码块后面，比如选择黑色经典款，即：1234567...&lt;div class="&#123;&#123; container_class &#125;&#125; &#123;% block page_class %&#125;&#123;% endblock %&#125; "&gt; &lt;div class="headband"&gt;&lt;/div&gt; # [样式代码] &lt;a href="https://github.com/you"&gt;&lt;img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"&gt;&lt;/a&gt;... 重新部署一下就可以查看了，如果显示不出来，需要清理浏览器的cookie,多刷新几次就OK了。大家看我的，感觉很搭( ⊙ o ⊙ )！]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo部署的网站项目(.deploy_git)中添加README.md]]></title>
    <url>%2F2018%2F02%2F28%2FHexo%E9%83%A8%E7%BD%B2%E7%9A%84%E7%BD%91%E7%AB%99%E9%A1%B9%E7%9B%AE-deploy-git-%E4%B8%AD%E6%B7%BB%E5%8A%A0README-md%2F</url>
    <content type="text"><![CDATA[终端中执行hexo generate时，会将source文件夹中的.md文件渲染为.html文件到public文件夹中，所以我们可以将README.md文件放到source文件夹中，这样在执行hexo deploy时，生成的.deploy_git文件夹中就会有README文件。但此时并不是我们想要的README.md。需要注意的是，我们要防止此.md文件被渲染为.html文件，因此，需要在站点配置文件_config.yml中设置skip_render: README.md，这样部署完成后我们就可以在配置的.deploy_git中看到README.md了。 配置截图如下：]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始学爬虫-05]]></title>
    <url>%2F2018%2F02%2F28%2F%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E7%88%AC%E8%99%AB-05%2F</url>
    <content type="text"><![CDATA[为什么要使用CookieCookie，指某些网站为了辨别用户身份、进行session跟踪而储存在用户本地终端上的数据（通常经过加密)。比如说有些网站需要登录后才能访问某个页面，在登录之前，你想抓取某个页面内容，登陆前与登陆后是不同的，或者不允许的。使用Cookie和使用代理IP一样，也需要创建一个自己的opener。在HTTP包中，提供了cookiejar模块，用于提供对Cookie的支持。 http.cookiejar功能强大，我们可以利用本模块的CookieJar类的对象来捕获cookie并在后续连接请求时重新发送，比如可以实现模拟登录功能。该模块主要的对象有CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar。 它们的关系： CookieJar–派生–&gt;FileCookieJar–派生–&gt;MozillaCookieJar和LWPCookieJar 工作原理： 创建一个带有cookie的opener，在访问登录的URL时，将登录后的cookie保存下来，然后利用这个cookie来访问其他网址。查看登录之后才能看到的信息。 同样，我们以实例进行讲解，爬取伯乐在线的面向对象的漂亮MM的邮箱联系方式。 实战(1).背景介绍在伯乐在线有这么一个有趣的模块，面向对象，它说白了就是提供了一个程序员(媛)网上相亲的平台。 URL: http://date.jobbole.com/ 它的样子是这样的： 可以看到，这里有很多的相亲贴，随便点进去就会有网上相亲MM的详细信息，想获取MM的联系方式，需要积分，积分可以通过签到的方式获取。如果没有登陆账户，获取联系方式的地方是这个样子的： 如果登陆了账号，获取联系方式的地方是这个样子的： 想要爬取MM的联系邮箱，就需要用到我们本次讲到的知识，Cookie的使用。当然，首先你积分也得够。 在讲解之前，推荐一款抓包工具–Fiddler，可以在Google Chrome的Google商店下载这个插件，它的样子是这样的： 可以看到，通过这个插件，我们可以很容易找到Post的Form Data等信息，很方便，当然也可以用之前讲得浏览器审查元素的方式查看这些信息。 (2).过程分析在伯乐在线首页点击登陆的按钮，Fiddler的抓包内容如下： 从上图可以看出，真正请求的url是 http://www.jobbole.com/wp-admin/admin-ajax.php Form Data的内容记住，这些是我们编程需要用到的。user_login是用户名，user_pass是用户密码。 在点击取得联系邮箱按钮的时候，Fiddler的抓包内容如下： 从上图可以看出，此刻真正请求的url是 http://date.jobbole.com/wp-admin/admin-ajax.php 同样Form Data中内容要记下来。postId是每个帖子的id。例如，打开一个相亲贴，它的URL是http://date.jobbole.com/4128/，那么它的这个postId就是4128。为了简化程序，这里就不讲解如何自动获取这个postId了，本实例直接指定postId。如果想要自动获取，可以使用beautifulsoup解析http://date.jobbole.com/返回的信息。beautifulsoup的使用。有机会的话，会在后面的爬虫笔记中进行讲解。 (3).测试1)将Cookie保存到变量中 首先，我们先利用CookieJar对象实现获取cookie的功能，存储到变量中，先来感受一下,执行文件cookie01.py： 1234567891011121314151617# -*- coding: UTF-8 -*-from urllib import requestfrom http import cookiejarif __name__ == '__main__': #声明一个CookieJar对象实例来保存cookie cookie = cookiejar.CookieJar() #利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler handler=request.HTTPCookieProcessor(cookie) #通过CookieHandler创建opener opener = request.build_opener(handler) #此处的open方法打开网页 response = opener.open('http://www.baidu.com') #打印cookie信息 for item in cookie: print('Name = %s' % item.name) print('Value = %s' % item.value) 我们使用以上方法将cookie保存到变量中，然后打印出了cookie中的值，运行结果如下: 2)保存Cookie到文件 在上面的方法中，我们将cookie保存到了cookie这个变量中，如果我们想将cookie保存到文件中该怎么做呢？方便以后直接读取文件使用，这时，我们就要用到FileCookieJar这个对象了，在这里我们使用它的子类MozillaCookieJar来实现Cookie的保存，编写代码如下,文件为cookie02.py：123456789101112131415161718# -*- coding: UTF-8 -*-from urllib import requestfrom http import cookiejarif __name__ == '__main__': #设置保存cookie的文件，同级目录下的cookie.txt filename = 'cookie.txt' #声明一个MozillaCookieJar对象实例来保存cookie，之后写入文件 cookie = cookiejar.MozillaCookieJar(filename) #利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler handler=request.HTTPCookieProcessor(cookie) #通过CookieHandler创建opener opener = request.build_opener(handler) #此处的open方法打开网页 response = opener.open('http://www.baidu.com') #保存cookie到文件 cookie.save(ignore_discard=True, ignore_expires=True) cookie.save的参数说明： ignore_discard的意思是即使cookies将被丢弃也将它保存下来； ignore_expires的意思是如果在该文件中cookies已经存在，则覆盖原文件写入。 在这里，我们将这两个全部设置为True。 运行之后，cookies将被保存到cookie.txt文件中。我们可以查看自己查看下cookie.txt这个文件的内容。 3)从文件中获取Cookie并访问 我们已经做到把Cookie保存到文件中了，如果以后想使用，可以利用下面的方法来读取cookie并访问网站，感受一下cookie03.py：12345678910111213141516171819# -*- coding: UTF-8 -*-from urllib import requestfrom http import cookiejarif __name__ == '__main__': #设置保存cookie的文件的文件名,相对路径,也就是同级目录下 filename = 'cookie.txt' #创建MozillaCookieJar实例对象 cookie = cookiejar.MozillaCookieJar() #从文件中读取cookie内容到变量 cookie.load(filename, ignore_discard=True, ignore_expires=True) #利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler handler=request.HTTPCookieProcessor(cookie) #通过CookieHandler创建opener opener = request.build_opener(handler) #此用opener的open方法打开网页 response = opener.open('http://www.baidu.com') #打印信息 print(response.read().decode('utf-8')) 了解到以上内容，我们那就可以开始正式编写模拟登陆伯乐在线的程序了。同时，我们也可以获取相亲MM的联系方式。 (4).编写代码我们利用CookieJar对象实现获取cookie的功能，存储到变量中。然后使用这个cookie变量创建opener，使用这个设置好cookie的opener即可模拟登陆，同笔记四中讲到的IP代理的使用方法类似。 创建cookie_test.py文件，编写代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# -*- coding: UTF-8 -*-from urllib import requestfrom urllib import errorfrom urllib import parsefrom http import cookiejarif __name__ == '__main__': #登陆地址 login_url = 'http://www.jobbole.com/wp-admin/admin-ajax.php' #User-Agent信息 user_agent = r'Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36' #Headers信息 head = &#123;'User-Agnet': user_agent, 'Connection': 'keep-alive'&#125; #登陆Form_Data信息 Login_Data = &#123;&#125; Login_Data['action'] = 'user_login' Login_Data['redirect_url'] = 'http://www.jobbole.com/' Login_Data['remember_me'] = '0' #是否一个月内自动登陆 Login_Data['user_login'] = '********' #改成你自己的用户名 Login_Data['user_pass'] = '********' #改成你自己的密码 #使用urlencode方法转换标准格式 logingpostdata = parse.urlencode(Login_Data).encode('utf-8') #声明一个CookieJar对象实例来保存cookie cookie = cookiejar.CookieJar() #利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler cookie_support = request.HTTPCookieProcessor(cookie) #通过CookieHandler创建opener opener = request.build_opener(cookie_support) #创建Request对象 req1 = request.Request(url=login_url, data=logingpostdata, headers=head) #面向对象地址 date_url = 'http://date.jobbole.com/wp-admin/admin-ajax.php' #面向对象 Date_Data = &#123;&#125; Date_Data['action'] = 'get_date_contact' Date_Data['postId'] = '4128' #使用urlencode方法转换标准格式 datepostdata = parse.urlencode(Date_Data).encode('utf-8') req2 = request.Request(url=date_url, data=datepostdata, headers=head) try: #使用自己创建的opener的open方法 response1 = opener.open(req1) response2 = opener.open(req2) html = response2.read().decode('utf-8') index = html.find('jb_contact_email') #打印查询结果 print('联系邮箱:%s' % html[index+19:-2]) except error.URLError as e: if hasattr(e, 'code'): print("HTTPError:%d" % e.code) elif hasattr(e, 'reason'): print("URLError:%s" % e.reason) (5).运行结果如下。]]></content>
      <categories>
        <category>Spider</category>
        <category>Urllib</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Spider</tag>
        <tag>Urllib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始学爬虫-04]]></title>
    <url>%2F2018%2F02%2F28%2F%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E7%88%AC%E8%99%AB-04%2F</url>
    <content type="text"><![CDATA[说在前面urllib_test10.py已经无法爬取了，因为原网站已经添加了防爬虫机制 (一)、为何要设置User Agent有一些网站不喜欢被爬虫程序访问，所以会检测连接对象，如果是爬虫程序，也就是非人点击访问，它就会不让你继续访问，所以为了要让程序可以正常运行，需要隐藏自己的爬虫程序的身份。此时，我们就可以通过设置User Agent的来达到隐藏身份的目的，User Agent的中文名为用户代理，简称UA。 User Agent存放于Headers中，服务器就是通过查看Headers中的User Agent来判断是谁在访问。在Python中，如果不设置User Agent，程序将使用默认的参数，那么这个User Agent就会有Python的字样，如果服务器检查User Agent，那么没有设置User Agent的Python程序将无法正常访问网站。 Python允许我们修改这个User Agent来模拟浏览器访问，它的强大毋庸置疑。 (二)、常见的User Agent(1).Android Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19 Mozilla/5.0 (Linux; U; Android 4.0.4; en-gb; GT-I9300 Build/IMM76D) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30 Mozilla/5.0 (Linux; U; Android 2.2; en-gb; GT-P1000 Build/FROYO) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1 (2).Firefox Mozilla/5.0 (Windows NT 6.2; WOW64; rv:21.0) Gecko/20100101 Firefox/21.0 Mozilla/5.0 (Android; Mobile; rv:14.0) Gecko/14.0 Firefox/14.0 (3).Google Chrome Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36 Mozilla/5.0 (Linux; Android 4.0.4; Galaxy Nexus Build/IMM76B) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.133 Mobile Safari/535.19 (4).iOS Mozilla/5.0 (iPad; CPU OS 5_0 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9A334 Safari/7534.48.3 Mozilla/5.0 (iPod; U; CPU like Mac OS X; en) AppleWebKit/420.1 (KHTML, like Gecko) Version/3.0 Mobile/3A101a Safari/419.3 上面列举了Andriod、Firefox、Google Chrome、iOS的一些User Agent，直接copy就能用。 (三)、设置User Agent的方法先看下urllib.request.Request() 从上图可以看出，在创建Request对象的时候，可以传入headers参数。因此，想要设置User Agent，有两种方法： 1.在创建Request对象的时候，填入headers参数(包含User Agent信息)，这个Headers参数要求为字典； 2.在创建Request对象的时候不添加headers参数，在创建完成之后，使用add_header()的方法，添加headers。 方法一创建文件urllib_test08.py，使用上面提到的Android的第一个User Agent，在创建Request对象的时候传入headers参数，编写代码如下：12345678910111213141516# -*- coding: UTF-8 -*-from urllib import requestif __name__ == "__main__": url = 'http://www.csdn.net/' head = &#123;&#125; #写入User Agent信息 head['User-Agent'] = 'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19' #创建Request对象 req = request.Request(url, headers=head) #传入创建好的Request对象 response = request.urlopen(req) #读取响应信息并解码 html = response.read().decode('utf-8') #打印信息 print(html) 运行结果如下： 方法二创建文件urllib_test09.py，使用上面提到的Android的第一个User Agent，在创建Request对象时不传入headers参数，创建之后使用add_header()方法，添加headers，编写代码如下：12345678910111213141516# -*- coding: UTF-8 -*-from urllib import requestif __name__ == "__main__": #以CSDN为例，CSDN不更改User Agent是无法访问的 url = 'http://www.csdn.net/' #创建Request对象 req = request.Request(url) #传入headers req.add_header('User-Agent', 'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19') #传入创建好的Request对象 response = request.urlopen(req) #读取响应信息并解码 html = response.read().decode('utf-8') #打印信息 print(html) 运行结果和上一个方法是一样的。 (四)、IP代理的使用(1).为何使用IP代理User Agent已经设置好了，但是还应该考虑一个问题，程序的运行速度是很快的，如果我们利用一个爬虫程序在网站爬取东西，一个固定IP的访问频率就会很高，这不符合人为操作的标准，因为人操作不可能在几ms内，进行如此频繁的访问。所以一些网站会设置一个IP访问频率的阈值，如果一个IP访问频率超过这个阈值，说明这个不是人在访问，而是一个爬虫程序。 (2).一般步骤说明一个很简单的解决办法就是设置延时，但是这显然不符合爬虫快速爬取信息的目的，所以另一种更好的方法就是使用IP代理。使用代理的步骤： (1) 调用urlib.request.ProxyHandler()，proxies参数为一个字典。 (2) 创建Opener(类似于urlopen，这个代开方式是我们自己定制的) (3) 安装Opener 使用install_opener方法之后，会将程序默认的urlopen方法替换掉。也就是说，如果使用install_opener之后，在该文件中，再次调用urlopen会使用自己创建好的opener。如果不想替换掉，只是想临时使用一下，可以使用opener.open(url)，这样就不会对程序默认的urlopen有影响。 (3).代理IP选取在写代码之前，先在代理IP网站选好一个IP地址，推荐西刺代理IP。 URL：http://www.xicidaili.com/ 注意：当然也可以写个正则表达式从网站直接爬取IP，但是要记住不要太频繁爬取，加个延时什么的，太频繁给服务器带来压力了，服务器会直接把你block，不让你访问的，我就被封了两天。 从西刺网站选出信号好的IP，我的选择如下：(106.46.136.112:808) 编写代码访问http://www.whatismyip.com.tw/，该网站是测试自己IP为多少的网址，服务器会返回访问者的IP。 (4).代码实例创建文件urllib_test10.py，编写代码如下：12345678910111213141516171819202122# -*- coding: UTF-8 -*-from urllib import requestif __name__ == "__main__": #访问网址 url = 'http://www.whatismyip.com.tw/' #这是代理IP proxy = &#123;'http':'106.46.136.112:808'&#125; #创建ProxyHandler proxy_support = request.ProxyHandler(proxy) #创建Opener opener = request.build_opener(proxy_support) #添加User Angent opener.addheaders = [('User-Agent','Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36')] #安装OPener request.install_opener(opener) #使用自己安装好的Opener response = request.urlopen(url) #读取相应信息并解码 html = response.read().decode("utf-8") #打印信息 print(html) 运行结果如下： 从上图可以看出，访问的IP已经伪装成了106.46.136.112。]]></content>
      <categories>
        <category>Spider</category>
        <category>Urllib</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Spider</tag>
        <tag>Urllib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始学爬虫-03]]></title>
    <url>%2F2018%2F02%2F28%2F%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E7%88%AC%E8%99%AB-03%2F</url>
    <content type="text"><![CDATA[urllib.errorurllib.error可以接收有urllib.request产生的异常。urllib.error有两个方法，URLError和HTTPError。如下图所示： URLError是OSError的一个子类，HTTPError是URLError的一个子类，服务器上HTTP的响应会返回一个状态码，根据这个HTTP状态码，我们可以知道我们的访问是否成功。例如第二个笔记中提到的200状态码，表示请求成功，再比如常见的404错误等。 (1).URLError让我们先看下URLError的异常，创建文件urllib_test05.py，编写如下代码：1234567891011121314# -*- coding: UTF-8 -*-from urllib import requestfrom urllib import errorif __name__ == "__main__": #一个不存在的连接 url = "http://www.dskfclyfiydl.com/" req = request.Request(url) try: response = request.urlopen(req) html = response.read().decode('utf-8') print(html) except error.URLError as e: print(e.reason) 可以看到如下运行结果： (2).HTTPError再看下HTTPError异常，创建文件urllib_test06.py，编写如下代码：12345678910111213# -*- coding: UTF-8 -*-from urllib import requestfrom urllib import errorif __name__ == "__main__": #一个不存在的连接 url = "http://www.douyu.com/wkx.html" req = request.Request(url) try: responese = request.urlopen(req) # html = responese.read() except error.HTTPError as e: print(e.code, '\n' ,e.reason, '\n', e.headers) 运行之后，我们可以看到404，这说明请求的资源没有在服务器上找到，www.douyu.com 这个服务器是存在的，但是我们要查找的 Jack_Cui.html 资源是没有的，所以抛出404异常。 (3).URLError和HTTPError混合使用最后值得注意的一点是，如果想用HTTPError和URLError一起捕获异常，那么需要将HTTPError放在URLError的前面，因为HTTPError是URLError的一个子类。如果URLError放在前面，出现HTTP异常会先响应URLError，这样HTTPError就捕获不到错误信息了。 如果不用上面的方法，也可以使用hasattr函数判断URLError含有的属性，如果含有reason属性表明是URLError，如果含有code属性表明是HTTPError。创建文件urllib_test07.py，编写代码如下：1234567891011121314151617# -*- coding: UTF-8 -*-from urllib import requestfrom urllib import errorif __name__ == "__main__": #一个不存在的连接 url = "http://www.douyu.com/wkx.html" req = request.Request(url) try: responese = request.urlopen(req) except error.URLError as e: if hasattr(e, 'code'): print("HTTPError") print(e.code) elif hasattr(e, 'reason'): print("URLError") print(e.reason) 运行结果如下：]]></content>
      <categories>
        <category>Spider</category>
        <category>Urllib</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Spider</tag>
        <tag>Urllib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始学爬虫-02]]></title>
    <url>%2F2018%2F02%2F28%2F%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E7%88%AC%E8%99%AB-02%2F</url>
    <content type="text"><![CDATA[一个疑问尚未解决疑问，小弟在此跪求大牛解答一下为什么把url里的 “_o” 删掉后就可以正常爬取呢？ urlopen的url参数 Agenturl不仅可以是一个字符串，例如:http://www.baidu.com。 url也可以是一个Request对象，这就需要我们先定义一个Request对象，然后将这个Request对象作为urlopen的参数使用，方法如下： 123456789# -*- coding: UTF-8 -*-from urllib import requestif __name__ == "__main__": req = request.Request("http://fanyi.baidu.com/") response = request.urlopen(req) html = response.read() html = html.decode("utf-8") print(html) 同样，运行这段代码同样可以得到网页信息。可以看一下这段代码和上个笔记中代码的不同，对比一下就明白了。 urlopen()返回的对象，可以使用read()进行读取，同样也可以使用geturl()方法、info()方法、getcode()方法。 geturl()返回的是一个url的字符串； info()返回的是一些meta标记的元信息，包括一些服务器的信息； getcode()返回的是HTTP的状态码，如果返回200表示请求成功。 关于META标签和HTTP状态码的内容可以自行百度百科，里面有很详细的介绍。 了解到这些，我们就可以进行新一轮的测试，新建文件名urllib_test04.py，编写如下代码：1234567891011# -*- coding: UTF-8 -*-from urllib import requestif __name__ == "__main__": req = request.Request("http://fanyi.baidu.com/") response = request.urlopen(req) print("geturl打印信息：%s"%(response.geturl())) print('**********************************************') print("info打印信息：%s"%(response.info())) print('**********************************************') print("getcode打印信息：%s"%(response.getcode())) urlopen的data参数我们可以使用data参数，向服务器发送数据。根据HTTP规范，GET用于信息获取，POST是向服务器提交数据的一种请求，再换句话说： 从客户端向服务器提交数据使用POST； 从服务器获得数据到客户端使用GET(GET也可以提交，暂不考虑)。 如果没有设置urlopen()函数的data参数，HTTP请求采用GET方式，也就是我们从服务器获取信息，如果我们设置data参数，HTTP请求采用POST方式，也就是我们向服务器传递数据。 data参数有自己的格式，它是一个基于application/x-www.form-urlencoded的格式，具体格式我们不用了解， 因为我们可以使用urllib.parse.urlencode()函数将字符串自动转换成上面所说的格式。 发送data实例向有道翻译发送data，得到翻译结果。 (1).打开有道翻译界面，如下图所示： (2).鼠标右键检查，也就是审查元素，如下图所示： (3).选择右侧出现的Network，如下图所示： (4).在左侧输入翻译内容，输入Jack，如下图所示： (5).点击自动翻译按钮，我们就可以看到右侧出现的内容，如下图所示： (6).点击上图红框中的内容，查看它的信息，如下图所示： (7).记住这些信息，这是我们一会儿写程序需要用到的。 新建文件translate_test.py，编写如下代码： 1234567891011121314151617181920212223242526272829# -*- coding: UTF-8 -*-from urllib import requestfrom urllib import parseimport jsonif __name__ == "__main__": #对应上图的Request URL Request_URL = 'http://fanyi.youdao.com/translate?smartresult=dict&amp;smartresult=rule&amp;sessionFrom=null' #创建Form_Data字典，存储上图的Form Data Form_Data = &#123;&#125; Form_Data['type'] = 'AUTO' Form_Data['i'] = 'Jack' Form_Data['doctype'] = 'json' Form_Data['xmlVersion'] = '1.8' Form_Data['keyfrom'] = 'fanyi.web' Form_Data['ue'] = 'ue:UTF-8' Form_Data['action'] = 'FY_BY_CLICKBUTTON' #使用urlencode方法转换标准格式 data = parse.urlencode(Form_Data).encode('utf-8') #传递Request对象和转换完格式的数据 response = request.urlopen(Request_URL,data) #读取信息并解码 html = response.read().decode('utf-8') #使用JSON translate_results = json.loads(html) #找到翻译结果 translate_results = translate_results['translateResult'][0][0]['tgt'] #打印翻译信息 print("翻译的结果是：%s" % translate_results) 运行查看翻译结果]]></content>
      <categories>
        <category>Spider</category>
        <category>Urllib</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Spider</tag>
        <tag>Urllib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始学爬虫-01]]></title>
    <url>%2F2018%2F02%2F28%2F%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E7%88%AC%E8%99%AB-01%2F</url>
    <content type="text"><![CDATA[本节关键字urllib | chardet urllib 简介在Python3.x中，我们可以使用urlib这个组件抓取网页，urllib是一个URL处理包，这个包中集合了一些处理URL的模块，如下： 1.urllib.request模块是用来打开和读取URLs的； 2.urllib.error模块包含一些有urllib.request产生的错误，可以使用try进行捕捉处理； 3.urllib.parse模块包含了一些解析URLs的方法； 4.urllib.robotparser模块用来解析robots.txt文本文件.它提供了一个单独的RobotFileParser类，通过该类提供的can_fetch()方法测试爬虫是否可以下载一个页面。 使用urllib.request.urlopen()这个接口函数就可以访问一个网站，读取并打印信息。urlopen有一些可选参数，具体信息可以查阅Python自带的documentation。 urllib 测试了解到这些，我们就可以写一个最简单的程序，文件名为urllib_test01.py，感受一个urllib库的魅力：1234567# -*- coding: UTF-8 -*-from urllib import requestif __name__ == "__main__": response = request.urlopen("http://fanyi.baidu.com") html = response.read() print(html) urllib使用使用request.urlopen()访问和读取URLs信息，返回的对象response如同一个文本对象，我们可以调用read()，进行读取。再通过print()屏幕打印。 浏览器就是作为客户端从服务器端获取信息，然后将信息解析，再展示给我们的。但是显然他们都是二进制的乱码。 我们可以通过简单的decode()命令将网页的信息进行解码，并显示出来，我们新创建一个文件，命名为urllib_test02.py，编写如下代码： 12345678# -*- coding: UTF-8 -*-from urllib import requestif __name__ == "__main__": response = request.urlopen("http://fanyi.baidu.com/") html = response.read() html = html.decode("utf-8") print(html) 这样我们就可以得到这样的结果，显然解码后的信息看起来工整和舒服多了： 当然这个前提是我们已经知道了这个网页是使用utf-8编码的，怎么查看网页的编码方式呢？非常简单的方法是使用使用浏览器审查元素，只需要找到head标签开始位置的chareset，就知道网页是采用何种编码。 这样我们就知道了这个网站的编码方式，但是这需要我们每次都打开浏览器，并找下编码方式，显然有些费事，使用几行代码解决更加省事并且显得酷一些。 我们需要安装第三方库chardet，它是用来判断编码的模块。安装好后，我们就可以使用chardet.detect()方法，判断网页的编码方式了。至此，我们就可以编写一个小程序判断网页的编码方式了，新建文件名为chardet_test01.py：123456789# -*- coding: UTF-8 -*-from urllib import requestimport chardetif __name__ == "__main__": response = request.urlopen("http://fanyi.baidu.com") html = response.read() charset = chardet.detect(html) print(charset)]]></content>
      <categories>
        <category>Spider</category>
        <category>Urllib</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Spider</tag>
        <tag>Urllib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一种Git保留两个repo的commit信息进行合并的方法]]></title>
    <url>%2F2018%2F02%2F27%2F%E4%B8%80%E7%A7%8DGit%E4%BF%9D%E7%95%99%E4%B8%A4%E4%B8%AArepo%E7%9A%84commit%E4%BF%A1%E6%81%AF%E8%BF%9B%E8%A1%8C%E5%90%88%E5%B9%B6%E7%9A%84%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[以往的合并时首先要删除repo的.git文件夹，然后重新add-commit-push。带来的问题是会丢失某一个仓库的提交信息，不利于时光倒退。经过摸索终于实现了保留两个仓库提交信息的合并方法。介绍如下： 比如要将DownloadPicsBySeleniumAndPhantomJS这个项目合并到Web-Spider中，终端中执行：12345$ git remote add other git@github.com:ByiProX/****.git$ git fetch other$ git checkout -b repo1 other/mster$ git checkout master$ git merge repo1 --allow-unrelated-histories 接下来解决merge冲突即可（可以尝试使用mergetool），如有需要可以删除多余分支和远程连接 1$ git mergetool # 解决merge冲突 123$ git remote rm other # 删除远程连接 $ git branch -d repo1 # 删除分支操作]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3下使用Selenium&PhantomJS爬火影忍者漫画]]></title>
    <url>%2F2018%2F02%2F27%2FPython3%E4%B8%8B%E4%BD%BF%E7%94%A8Selenium-PhantomJS%E7%88%AC%E7%81%AB%E5%BD%B1%E5%BF%8D%E8%80%85%E6%BC%AB%E7%94%BB%2F</url>
    <content type="text"><![CDATA[近期学习爬虫，发现懂的越多，不懂的知识点越多（所以当个傻子还是很幸福的）。好记性不如烂笔头，之前都是把看到的资料链接直接挂到一些平台，比如知乎、简书、Github等。今天有点时间，就好好码一下字，排排版，方便以后查阅。 Selenium用来模拟浏览器的行为，比如点击、最大化、滚动窗口等；PhantomJS是一种浏览器，不过这种浏览器没有UI界面，感觉就像是专门为爬虫设计，优点很明显，可以有效减小内存的使用。 爬虫使用到的模块12345from selenium import webdriverfrom myLogging import MyLoggingimport osimport timeimport re myLogging模块是自己配置的日志包，想要的可以点击这里自己看 爬虫很关键的一点就是能够看懂网页的源代码，记得当初刚刚真正开始接触编程的时候，有很长的一段时间在看HTML、CSS、JS的一些知识，虽然忘得很多，但是印象还是有的，对于后面看网页源代码很有帮助。学习爬虫，除了会基本的python知识以外，还要会网页的一些知识。 爬取图片思路： 已知连接，分析网页的代码结构，看所需的数据是否需要切换frame，并定位所需数据的位于哪个标签之下 采用不同的模块有不同的保存图片方式，如果采用request模块，保存图片方式是可以采用requests.get(comicUrl).content方法，使用该方法需要确定网页的地址。该项目中没有涉及request的使用，所以此后不再表述。对于selenium可以使用 get_screenshot_as_file() 方法，使用该方法强烈建议使用phantomjs，如果使用chrome浏览器，图片尺寸太大的话，会出现截图不完整，对比如下： 找到下一张图片的连接位置并点击更新网页，一般来讲新网页与之前网页结构相同。 在新网页的基础上保存图片，设置循环如此反复。 爬取网页的URL为：爬取火影漫画第一话 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class DownloadPics(object): def __init__(self, url): self.url = url self.log = MyLogging() self.browser = self.get_browser() self.save_pics(self.browser) def get_browser(self): browser = webdriver.PhantomJS() try: browser.get(self.url) except: MyLogging.error('open the url %s failed' % self.url) browser.implicitly_wait(20) return browser def save_pics(self, browser): pics_title = browser.title.split('_')[0] self.create_dir(pics_title) os.chdir(pics_title) sum_page = self.find_total_page_num(browser) i = 1 while i &lt; sum_page: image_name = str(i) + '.png' browser.get_screenshot_as_file(image_name) # 使用PhantomJS避免了截图的不完整，可以与Chrome比较 self.log.info('saving image %s' % image_name) i += 1 css_selector = "a[href='/comiclist/3/3/%s.htm']" % i # 该方法感觉还不错呢，不过这个网站确实挺差劲的 next_page = browser.find_element_by_css_selector(css_selector) next_page.click() time.sleep(2) # browser.implicitly_wait(20) def find_total_page_num(self, browser): page_element = browser.find_element_by_css_selector("table[cellspacing='1']") num = re.search(r'共\d+页', page_element.text).group()[1:-1] return int(num) def create_dir(self, dir_name): if os.path.exists(dir_name): self.log.error('create directory %s failed cause a same directory exists' % dir_name) else: try: os.makedirs(dir_name) except: self.log.error('create directory %s failed' % dir_name) else: self.log.info('create directory %s success' % dir_name)if __name__ == '__main__': start_url = 'http://comic.kukudm.com/comiclist/3/3/1.htm' DL = DownloadPics(start_url) 运行结果]]></content>
      <categories>
        <category>Spider</category>
        <category>Selenium</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>Spider</tag>
        <tag>Selenium</tag>
        <tag>PhantomJS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django2.0.1搭建电影网站]]></title>
    <url>%2F2018%2F02%2F27%2FDjango2-0-1%E6%90%AD%E5%BB%BA%E7%94%B5%E5%BD%B1%E7%BD%91%E7%AB%99%2F</url>
    <content type="text"><![CDATA[本项目已经部署到服务器，可以通过该IP查看http://59.110.221.56/GitHub源代码 技术栈 Bootstrap 3 + Django 2.0.1 + MySQL 5.7.17 + Nginx + locust 本地服务运行方法终端在venv文件夹路径下开启虚拟环境(根据自己的shell进行选择)： 123source ../venv/bin/activate.fishsource ../venv/bin/activatesource ../venv/bin/activate.csh 然后执行：1python3 TWS_Cinema/manage.py runserver 如果报错，终端进入requirements.txt所在目录，运行命令：1pip3 install -r requirements.txt 然后执行：1python3 TWS_Cinema/manage.py runserver 单元测试运行方法在manage.py路径下终端运行 1python3 manage.py test 网站功能描述 实现导航栏搜索电影，支持按年份搜索和类型搜索 – 显示分类列表 – 点击分类显示符合分类要求的电影 实现搜索功能，支持按电影名称模糊搜索 实现电影详细信息查看功能 – 显示电影详细信息 – 显示豆瓣 Top 5 影评 – 在电影详细页面显示相似电影推荐 – 增加电影观看链接 API 按电影id搜索 —— api/movie/id/ # 例如：api/movie/id/1291545 按电影名搜索 —— api/movie/title/ # 例如：api/movie/title/大鱼 按电影原始名搜索 —— api/movie/original_title/ # 例如：api/movie/original_title/Big Fish 按电影类型搜索 —— api/movie/genre/ # 例如：api/movie/genre/剧情 按电影年份搜索 —— api/movie/year/ # 例如：api/movie/year/2003 网站性能测试结果在文件locustfile.py路径下运行1locust --host=http://59.110.221.56 压力测试 采取的框架：locust 服务器性能： CPU：1核 内存：2 GB (I/O优化) 带宽：1Mbps 测试结果： 500人：100%正确 1000人：40%出错率 测试截图 电影网站的其他截图 ReferenceLocust 简介以及使用]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Django</tag>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简谈爬虫攻与防]]></title>
    <url>%2F2018%2F02%2F27%2F%E7%AE%80%E8%B0%88%E7%88%AC%E8%99%AB%E6%94%BB%E4%B8%8E%E9%98%B2%2F</url>
    <content type="text"><![CDATA[封锁间隔时间破解Scrapy在两次请求之间的时间设置是DOWNLOAD_DELAY。如果不考虑反爬虫的因素，该值当然越小越好。如果DOWNLOAD_DELAY设为0.001，也就是每1毫秒请求一次网页，这简直非人类干的事情。有些网站会检测一个ip的访问时间，异常情况下会封锁该ip。 封锁Cookies众所周知，网站是通过Cookie来确定用户身份的，Scrapy在爬取数据时使用同一个Cookies发起请求。该做法和把DOWNLOAD_DELAY设为0.001没有本质区别。在scrapy中，直接社禁用Cookies就可以了。在settings.py中设置12# Disable cookies (enabled by default)COOKIES_ENABLED = False 封锁user-agent和proxy破解user-agent是浏览器的身份标识。网站通过UA来确定浏览器类型。很多浏览器拒绝不符合一定标准的UA请求网页。同一个UA高频率的访问网站会有被网站列入黑名单的危险。破解的方法很简单，可以准备一个UA池，每次请求时随机挑选一个进行请求。 在middlewares.py同级目录下创建UAResource.py,文件内容如下： 12345678910111213141516171819202122232425#-*- coding: utf-8 -*-UserAgents = [ "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)", "Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)", "Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)", "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)", "Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)", "Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)", "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)", "Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6", "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1", "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0", "Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5", "Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11", "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20", "Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52",]Proxies = ['http://122.114.31.177:808','http://1.2.3.4:80',] 修改middlewares.py，添加内容为1234567891011121314from .UAResource import UserAgentsfrom .UAResource import Proxiesimport randomclass RandomProxy(object): def process_request(self, request, spider): proxy = random.choice(Proxies) request.meta['proxy'] = proxyclass RandomUserAgent(object): """docstring for RandomUerAgent.""" def process_request(self, request, spider): ua = random.choice(UserAgents) request.headers.setdefault('User-Agent', ua) 最后修改setting.py,将RandomUserAgent和RandomProxy添加到DOWNLOADER_MIDDLEWARES中，注意RandomProxy要放到RandomUserAgent之前，即将RandomProxy的值比RandomUserAgent后的值小12345678DOWNLOADER_MIDDLEWARES = &#123; # 'meijutt.middlewares.MeijuttDownloaderMiddleware': 543, 'meijutt.middlewares.RandomProxy': 10, 'meijutt.middlewares.RandomUserAgent': 30, # 禁止内置的(在 DOWNLOADER_MIDDLEWARES_BASE 中设置并默认启用的)中间件 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,&#125; 免费代理不够稳定，如果不想用proxy，设置RandomProxy为None,即禁止使用1'meijutt.middlewares.RandomProxy': None,]]></content>
      <categories>
        <category>Spider</category>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Spider</tag>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Github多分支管理Hexo-Blog项目]]></title>
    <url>%2F2018%2F02%2F27%2FGithub%E5%A4%9A%E5%88%86%E6%94%AF%E7%AE%A1%E7%90%86Hexo-Blog%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[Hexo在部署之后在github的仓库中我们只能找到生成的静态文件。然而博客的源文件：主题、文章、配置等文件都还在本地，并没有备份。对于多台终端设备的用户不够友好，而且存在一定的风险，万一那天电脑坏了或者是出现一些其他问题，就得从头再来。为了解决上述问题，我们可以利用github的分支思想来备份我们的源文件。 备份之前，需要了解博客根目录下面的文件以及文件夹作用：123456789.deploy_git/ 网站静态文件(git)node_modules/ 插件public/ 网站静态文件scaffolds/ 文章模板source/ 博文等themes/ 主题_config.yml 网站配置文件package.json Hexo信息db.json 数据文件 备份的思路master分支存放部署生成的静态文件，Hexo-Bog分支存放我们要备份项目源文件。实际备份中，.deploy_git、public文件夹和我们的master分支内容重复，所以略过。因此，我们在根目录下面建一个.gitignore文件来建立“黑名单”，禁止备份。 编辑.gitignore过滤文件文件内容如下：123.DS_Storepublic/.deploy*/ 关于备份终端中在项目的根目录下执行，对于作者自己的项目，命令执行的路径为ByiProX/下：123456$ git init$ git remote add origin git@github.com:username/username.github.io.git # username为博客项目的名称，也就是git的用户名$ git add .$ git commit -m "ready for backup of the project"$ git push origin master:Hexo-Blog 执行完毕后会发现github博客仓库已经有了一个新分支Hexo-Blog，于是备份工作完成。以后，开始写博文时，即终端运行1$ hexo new [layout] &lt;title&gt; 完成文章后,对编辑后的文章进行备份保存，即终端运行,为下面的部署做准备123$ git add .$ git commit -m "add one article"$ git push origin master:Hexo-Blog 部署运行一下命令进行仓库master分支静态文件部署123$ hexo clean$ hexo generate$ hexo deploy 以上完成项目源文件以及静态文件的Git管理 参考文献及进阶Hexo+github搭建个人博客并实现多终端管理如何在github上面备份HexoHexo的版本控制与持续集成使用hexo，如果换了电脑怎么更新博客]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F02%2F26%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. 非常好的一篇markdown参考手册 Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
