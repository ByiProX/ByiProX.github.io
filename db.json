{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/uploads/avatar.jpg","path":"uploads/avatar.jpg","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","path":"lib/needsharebutton/font-embedded.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","path":"lib/needsharebutton/needsharebutton.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","path":"lib/needsharebutton/needsharebutton.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","path":"lib/Han/dist/font/han.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":0,"renderable":1}],"Cache":[{"_id":"themes/next/.DS_Store","hash":"f09a86bb4ce7d04901ec1bb4cb55bee99909840c","modified":1519703664064},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1519624147209},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1519624147209},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1519624147209},{"_id":"themes/next/.gitignore","hash":"0b5c2ffd41f66eb1849d6426ba8cf9649eeed329","modified":1519624147210},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1519624147210},{"_id":"themes/next/.javascript_ignore","hash":"8a224b381155f10e6eb132a4d815c5b52962a9d1","modified":1519624147211},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1519624147211},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1519624147211},{"_id":"themes/next/.travis.yml","hash":"d60d4a5375fea23d53b2156b764a99b2e56fa660","modified":1519624147211},{"_id":"themes/next/LICENSE","hash":"f293bcfcdc06c0b77ba13570bb8af55eb5c059fd","modified":1519624147211},{"_id":"themes/next/README.cn.md","hash":"5d8af3d8de8d3926126a738519e97c8442b0effe","modified":1519624147212},{"_id":"themes/next/README.md","hash":"44b28d995681a7c48bfe3d0577d6203812d07e59","modified":1519624147212},{"_id":"themes/next/_config.yml","hash":"b6e54c66e7c3504277cbc8776b548098b3dd886c","modified":1519793631323},{"_id":"themes/next/bower.json","hash":"0674f11d3d514e087a176da0e1d85c2286aa5fba","modified":1519624147214},{"_id":"themes/next/gulpfile.coffee","hash":"031bffc483e417b20e90eceb6cf358e7596d2e69","modified":1519624147214},{"_id":"themes/next/package.json","hash":"036d3a1346203d2f1a3958024df7f74e7ac07bfe","modified":1519624147243},{"_id":"source/_posts/Django2-0-1搭建电影网站.md","hash":"9ccb572e15d1f52614d6ca6dfb07f4dc390ffb0e","modified":1519713719694},{"_id":"source/_posts/Github多分支管理Hexo-Blog项目.md","hash":"005d4f3688fd9edb4a3712f675f81d44c1c24dcc","modified":1519713656801},{"_id":"source/_posts/Python3下使用Selenium-PhantomJS爬火影忍者漫画.md","hash":"b8499588434ae4c057d9a5b6549c687299adfa80","modified":1519751037896},{"_id":"source/_posts/hello-world.md","hash":"b8cefcae6e00fb9fc954a93587aa026a34878325","modified":1519713823774},{"_id":"source/_posts/一种Git保留两个repo的commit信息进行合并的方法.md","hash":"483550b7fff39bb7aca857887ba23a3637839aee","modified":1519715982036},{"_id":"source/_posts/从零开始学爬虫-01.md","hash":"d30857c41ef0b23aa34919b95a6896069d3670f0","modified":1519751080535},{"_id":"source/_posts/从零开始学爬虫-02.md","hash":"82c11b9d4650984af091d6e580e5ef3dfd0f4dd8","modified":1519751082158},{"_id":"source/_posts/从零开始学爬虫-03.md","hash":"6714ad7f64bdfaa8d5a899adb3adc285b0d589c1","modified":1519751083829},{"_id":"source/_posts/从零开始学爬虫-04.md","hash":"df009a65165b62f9cfb15c801f91270f8055493a","modified":1519751905138},{"_id":"source/_posts/从零开始学爬虫-05.md","hash":"616044ff7f3ef6558b80c583598de43bc0ad8d20","modified":1519752171289},{"_id":"source/_posts/简谈爬虫攻与防.md","hash":"43863ed769a424ecb82a6c981359dc67da39932e","modified":1519751077804},{"_id":"source/about/index.md","hash":"a413a7fd056cc4f8093fcaeb1497ce0ef5473a6b","modified":1519725856738},{"_id":"source/categories/index.md","hash":"e675535fc5444a91757d1878ea74869cfba250dc","modified":1519728606227},{"_id":"source/tags/index.md","hash":"c246461b696977ca1c1d9f018c95fe08579b9fbc","modified":1519728623228},{"_id":"themes/next/.git/FETCH_HEAD","hash":"d67b7cbcc31216dec8e0329efb4eed8c0eec4cdb","modified":1519726468734},{"_id":"themes/next/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1519624147194},{"_id":"themes/next/.git/ORIG_HEAD","hash":"f4d9f6f8bc79e9bc071cf29324a74a1d78158ab9","modified":1519726468756},{"_id":"themes/next/.git/config","hash":"bf7d1df65cf34d0f25a7184a58c37a09f72e4be7","modified":1519726463025},{"_id":"themes/next/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1519624135722},{"_id":"themes/next/.git/index","hash":"4040cf935f10a7ba374f6c7adfe534e71a76b6b7","modified":1519640275157},{"_id":"themes/next/.git/packed-refs","hash":"339779e225d913a344c5e6210617badd049c4434","modified":1519624147186},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"3b5eafd32abb718e56ccf8d1cee0607ad8ce611d","modified":1519624147209},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"352093a1b210c72136687fd2eee649244cee402c","modified":1519624147209},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"902f627155a65099e0a37842ff396a58d0dc306f","modified":1519624147210},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1519624147210},{"_id":"themes/next/languages/de.yml","hash":"057e7df11ddeb1c8c15a5d7c5ff29430d725ec6b","modified":1519624147215},{"_id":"themes/next/languages/default.yml","hash":"44ef3f26917f467459326c2c8be2f73e4d947f35","modified":1519624147215},{"_id":"themes/next/languages/en.yml","hash":"7e680d9bb8f3a3a9d1ba1c9d312b3d257183dded","modified":1519624147216},{"_id":"themes/next/languages/fr-FR.yml","hash":"7e4eb7011b8feee641cfb11c6e73180b0ded1c0f","modified":1519624147216},{"_id":"themes/next/languages/id.yml","hash":"b5de1ea66dd9ef54cac9a1440eaa4e3f5fc011f5","modified":1519624147217},{"_id":"themes/next/languages/it.yml","hash":"aa595f2bda029f73ef7bfa104b4c55c3f4e9fb4c","modified":1519624147217},{"_id":"themes/next/languages/ja.yml","hash":"3c76e16fd19b262864475faa6854b718bc08c4d8","modified":1519624147218},{"_id":"themes/next/languages/ko.yml","hash":"ea5b46056e73ebcee121d5551627af35cbffc900","modified":1519624147218},{"_id":"themes/next/languages/nl-NL.yml","hash":"edca4f3598857dbc3cbf19ed412213329b6edd47","modified":1519624147218},{"_id":"themes/next/languages/pt-BR.yml","hash":"b1694ae766ed90277bcc4daca4b1cfa19cdcb72b","modified":1519624147218},{"_id":"themes/next/languages/pt.yml","hash":"44b61f2d085b827b507909a0b8f8ce31c6ef5d04","modified":1519624147218},{"_id":"themes/next/languages/ru.yml","hash":"98ec6f0b7183282e11cffc7ff586ceb82400dd75","modified":1519624147219},{"_id":"themes/next/languages/vi.yml","hash":"fd08d3c8d2c62965a98ac420fdaf95e54c25d97c","modified":1519624147219},{"_id":"themes/next/languages/zh-Hans.yml","hash":"16ef56d0dea94638de7d200984c90ae56f26b4fe","modified":1519624147219},{"_id":"themes/next/languages/zh-hk.yml","hash":"9396f41ae76e4fef99b257c93c7354e661f6e0fa","modified":1519624147219},{"_id":"themes/next/languages/zh-tw.yml","hash":"50b71abb3ecc0686f9739e179e2f829cd074ecd9","modified":1519624147220},{"_id":"themes/next/layout/_layout.swig","hash":"ab81e9567b1a7570a0296356c8b44839fbb47239","modified":1519794595755},{"_id":"themes/next/layout/archive.swig","hash":"f0a8225feafd971419837cdb4bcfec98a4a59b2f","modified":1519624147240},{"_id":"themes/next/layout/category.swig","hash":"4472255f4a3e3dd6d79201523a9526dcabdfbf18","modified":1519624147241},{"_id":"themes/next/layout/index.swig","hash":"783611349c941848a0e26ee2f1dc44dd14879bd1","modified":1519624147241},{"_id":"themes/next/layout/page.swig","hash":"969caaee05bdea725e99016eb63d810893a73e99","modified":1519624147242},{"_id":"themes/next/layout/post.swig","hash":"b3589a8e46288a10d20e41c7a5985d2493725aec","modified":1519624147242},{"_id":"themes/next/layout/schedule.swig","hash":"d86f8de4e118f8c4d778b285c140474084a271db","modified":1519624147242},{"_id":"themes/next/layout/tag.swig","hash":"7e0a7d7d832883eddb1297483ad22c184e4368de","modified":1519624147243},{"_id":"themes/next/scripts/merge-configs.js","hash":"81e86717ecfb775986b945d17f0a4ba27532ef07","modified":1519624147244},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1519624147245},{"_id":"themes/next/source/.DS_Store","hash":"6266fdba27b0249947350a7214c960c4bc33b9b0","modified":1519703767860},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1519624147345},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1519624147345},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1519624147346},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1519624147279},{"_id":"themes/next/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1519624135725},{"_id":"themes/next/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1519624135723},{"_id":"themes/next/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1519624135726},{"_id":"themes/next/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1519624135726},{"_id":"themes/next/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1519624135724},{"_id":"themes/next/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1519624135726},{"_id":"themes/next/.git/hooks/pre-rebase.sample","hash":"5885a56ab4fca8075a05a562d005e922cde9853b","modified":1519624135724},{"_id":"themes/next/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1519624135725},{"_id":"themes/next/.git/hooks/prepare-commit-msg.sample","hash":"2b6275eda365cad50d167fe3a387c9bc9fedd54f","modified":1519624135726},{"_id":"themes/next/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1519624135727},{"_id":"themes/next/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1519624135722},{"_id":"themes/next/.git/logs/HEAD","hash":"bc2442b4b1e426d613ede948ab87bb34810af65c","modified":1519624147197},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1519624147220},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1519624147221},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1519624147221},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"665a928604f99d2ba7dc4a4a9150178229568cc6","modified":1519624147221},{"_id":"themes/next/layout/_macro/post.swig","hash":"446a35a2cd389f8cfc3aa38973a9b44ad0740134","modified":1519624147222},{"_id":"themes/next/layout/_macro/reward.swig","hash":"56e8d8556cf474c56ae1bef9cb7bbd26554adb07","modified":1519624147222},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"6a54c3c85ff6b19d275827a327abbf4bd99b2ebf","modified":1519624147222},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"39852700e4084ecccffa6d4669168e5cc0514c9e","modified":1519624147222},{"_id":"themes/next/layout/_partials/comments.swig","hash":"4a6f5b1792b2e5262b7fdab9a716b3108e2f09c7","modified":1519624147223},{"_id":"themes/next/layout/_partials/footer.swig","hash":"c4d6181f5d3db5365e622f78714af8cc58d7a45e","modified":1519624147223},{"_id":"themes/next/layout/_partials/head.swig","hash":"6b94fe8f3279daea5623c49ef4bb35917ba57510","modified":1519624147223},{"_id":"themes/next/layout/_partials/header.swig","hash":"ed042be6252848058c90109236ec988e392d91d4","modified":1519624147224},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1519624147225},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1519624147225},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1519624147225},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1519624147236},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1519624147236},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1519624147237},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"5fe0447cc88a5a63b530cf0426f93c4634811876","modified":1519624147237},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1519624147237},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1519624147238},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"1ddb2336a1a19b47af3017047012c01ec5f54529","modified":1519624147238},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1519624147228},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1519624147228},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"a266f96ad06ee87bdeae6e105a4b53cd587bbd04","modified":1519624147229},{"_id":"themes/next/scripts/tags/button.js","hash":"d023f10a00077f47082b0517e2ad666e6e994f60","modified":1519624147245},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1519624147245},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1519624147246},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1519624147246},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1519624147246},{"_id":"themes/next/scripts/tags/label.js","hash":"2f8f41a7316372f0d1ed6b51190dc4acd3e16fff","modified":1519624147246},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"eeeabede68cf263de9e6593ecf682f620da16f0a","modified":1519624147246},{"_id":"themes/next/scripts/tags/note.js","hash":"64de4e9d01cf3b491ffc7d53afdf148ee5ad9779","modified":1519624147246},{"_id":"themes/next/scripts/tags/tabs.js","hash":"5786545d51c38e8ca38d1bfc7dd9e946fc70a316","modified":1519624147247},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1519624147279},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1519624147279},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1519624147280},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1519624147280},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1519624147281},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1519624147281},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1519624147281},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1519624147282},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1519624147282},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1519624147282},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1519624147283},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1519624147283},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1519624147283},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1519624147284},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1519624147284},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1519624147284},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1519624147285},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1519624147285},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1519624147285},{"_id":"themes/next/source/uploads/avatar.jpg","hash":"486f26a392d7783a3e9ac1a81ddecbde060f47e5","modified":1519703513674},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1519624147229},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1519624147229},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1519624147270},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1519624147270},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1519624147270},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1519624147278},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1519624147279},{"_id":"themes/next/.git/refs/heads/master","hash":"f4d9f6f8bc79e9bc071cf29324a74a1d78158ab9","modified":1519624147195},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1519624147224},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1519624147224},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1519624147226},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1519624147226},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1519624147226},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1519624147227},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1519624147227},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1519624147227},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"048fd5e98149469f8c28c21ba3561a7a67952c9b","modified":1519624147228},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1519624147234},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"c316758546dc9ba6c60cb4d852c17ca6bb6d6724","modified":1519624147234},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1519624147235},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"10160daceaa6f1ecf632323d422ebe2caae49ddf","modified":1519624147235},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1519624147235},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"aa0629277d751c55c6d973e7691bf84af9b17a60","modified":1519624147235},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"8a2e393d2e49f7bf560766d8a07cd461bf3fce4f","modified":1519624147236},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"fcabbb241f894c9a6309c44e126cf3e8fea81fd4","modified":1519624147236},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"8b6650f77fe0a824c8075b2659e0403e0c78a705","modified":1519624147236},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"98df9d72e37dd071e882f2d5623c9d817815b139","modified":1519624147230},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1519624147230},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1519624147230},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"18e7bef8923d83ea42df6c97405e515a876cede4","modified":1519624147231},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1519624147231},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1519624147231},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"1cd01c6e92ab1913d48e556a92bb4f28b6dc4996","modified":1519624147231},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1519624147232},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"5e9bb24c750b49513d9a65799e832f07410002ac","modified":1519624147232},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"fc65b9c98a0a8ab43a5e7aabff6c5f03838e09c8","modified":1519624147232},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1519624147232},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1519624147233},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1519624147233},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1519624147238},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"385c066af96bee30be2459dbec8aae1f15d382f5","modified":1519624147239},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1519624147239},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1519624147240},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1519624147228},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1519624147229},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1519624147229},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1519624147269},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1519624147270},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"9ab65361ba0a12a986edd103e56492644c2db0b8","modified":1519624147270},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"82f9055955920ed88a2ab6a20ab02169abb2c634","modified":1519624147270},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"99fbb4686ea9a3e03a4726ed7cf4d8f529034452","modified":1519624147277},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"be087dcc060e8179f7e7f60ab4feb65817bd3d9f","modified":1519624147278},{"_id":"themes/next/source/css/_variables/base.styl","hash":"29c261fa6b4046322559074d75239c6b272fb8a3","modified":1519624147278},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"f29165e36489a87ba32d17dddfd2720d84e3f3ec","modified":1519624147278},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1519624147286},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1519624147286},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1519624147286},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1519624147287},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1519624147287},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1519624147287},{"_id":"themes/next/source/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1519624147287},{"_id":"themes/next/source/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1519624147288},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1519624147288},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1519624147289},{"_id":"themes/next/source/js/src/utils.js","hash":"9b1325801d27213083d1487a12b1a62b539ab6f8","modified":1519624147289},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1519624147299},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1519624147295},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1519624147299},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1519624147300},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1519624147300},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1519624147309},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1519624147309},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1519624147309},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1519624147310},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1519624147310},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1519624147306},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1519624147307},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1519624147307},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1519624147307},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1519624147324},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1519624147326},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1519624147326},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1519624147326},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1519624147327},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1519624147327},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1519624147327},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","hash":"c39d37278c1e178838732af21bd26cd0baeddfe0","modified":1519624147328},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","hash":"3ef0020a1815ca6151ea4886cd0d37421ae3695c","modified":1519624147328},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","hash":"9885fd9bea5e7ebafc5b1de9d17be5e106248d96","modified":1519624147329},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1519624147334},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1519624147334},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1519624147335},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1519624147329},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1519624147329},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1519624147330},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1519624147330},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1519624147330},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1519624147330},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1519624147331},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1519624147331},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1519624147331},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1519624147332},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1519624147332},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1519624147332},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1519624147333},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1519624147333},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1519624147341},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1519624147341},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1519624147344},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1519624147344},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1519624147345},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1519624147325},{"_id":"themes/next/.git/logs/refs/heads/master","hash":"bc2442b4b1e426d613ede948ab87bb34810af65c","modified":1519624147198},{"_id":"themes/next/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1519624147193},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1519624147238},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1519624147238},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"4719ce717962663c5c33ef97b1119a0b3a4ecdc3","modified":1519624147247},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1519624147247},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"7e509c7c28c59f905b847304dd3d14d94b6f3b8e","modified":1519624147248},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1519624147248},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1519624147248},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"c5d48863f332ff8ce7c88dec2c893f709d7331d3","modified":1519624147252},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1519624147261},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1519624147267},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"f7c44b0ee46cf2cf82a4c9455ba8d8b55299976f","modified":1519624147267},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1519624147268},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"47a46583a1f3731157a3f53f80ed1ed5e2753e8e","modified":1519624147268},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1519624147268},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1519624147269},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1519624147269},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"18c3336ee3d09bd2da6a876e1336539f03d5a973","modified":1519624147271},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1519624147271},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1519624147271},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1519624147272},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1519624147272},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"3b25edfa187d1bbbd0d38b50dd013cef54758abf","modified":1519624147272},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1519624147273},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1519624147273},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1519624147274},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1519624147274},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"4aac01962520d60b03b23022ab601ad4bd19c08c","modified":1519624147275},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1519624147275},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1519624147275},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1519624147276},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"5b93958239d3d2bf9aeaede44eced2434d784462","modified":1519624147276},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"215de948be49bcf14f06d500cef9f7035e406a43","modified":1519624147276},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1519624147276},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"9d16fa3c14ed76b71229f022b63a02fd0f580958","modified":1519624147277},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1519624147277},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1519624147288},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1519624147292},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1519624147294},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1519624147294},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1519624147301},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1519624147301},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1519624147301},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1519624147302},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1519624147302},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1519624147302},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1519624147305},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1519624147306},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1519624147306},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1519624147311},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1519624147311},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1519624147311},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1519624147308},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1519624147308},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1519624147340},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1519624147340},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1519624147293},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1519624147323},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1519624147324},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1519624147343},{"_id":"themes/next/.git/logs/refs/remotes/origin/HEAD","hash":"bc2442b4b1e426d613ede948ab87bb34810af65c","modified":1519624147193},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"7905a7f625702b45645d8be1268cb8af3f698c70","modified":1519624147248},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1519624147250},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"25dc25f61a232f03ca72472b7852f882448ec185","modified":1519624147250},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1519624147250},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"f5aa2ba3bfffc15475e7e72a55b5c9d18609fdf5","modified":1519624147251},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1519624147251},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1519624147251},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1519624147252},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1519624147252},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1519624147248},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1519624147249},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1519624147249},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1519624147249},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1519624147249},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"e72a89e0f421444453e149ba32c77a64bd8e44e8","modified":1519624147253},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1519624147253},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1519624147253},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1519624147254},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"535b3b4f8cb1eec2558e094320e7dfb01f94c0e7","modified":1519624147254},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1519624147254},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"aea21141015ca8c409d8b33e3e34ec505f464e93","modified":1519624147255},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1519624147255},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"36332c8a91f089f545f3c3e8ea90d08aa4d6e60c","modified":1519624147255},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1519624147255},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"d5a4e4fc17f1f7e7c3a61b52d8e2e9677e139de7","modified":1519624147256},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1519624147256},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1519624147256},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"e4055a0d2cd2b0ad9dc55928e2f3e7bd4e499da3","modified":1519624147257},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"262debfd4442fa03d9919ceb88b948339df03fb0","modified":1519624147257},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"0a6c0efffdf18bddbc1d1238feaed282b09cd0fe","modified":1519624147257},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1519624147258},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"89dd4f8b1f1cce3ad46cf2256038472712387d02","modified":1519624147259},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1519624147259},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1519624147259},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1519624147259},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"12937cae17c96c74d5c58db6cb29de3b2dfa14a2","modified":1519624147260},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1519624147260},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"50305b6ad7d09d2ffa4854e39f41ec1f4fe984fd","modified":1519624147260},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1519624147261},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1519624147261},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1519624147262},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"37e951e734a252fe8a81f452b963df2ba90bfe90","modified":1519624147262},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1519624147262},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1519624147262},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"ee7528900578ef4753effe05b346381c40de5499","modified":1519624147263},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1519624147263},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1519624147263},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"ead0d0f2321dc71505788c7f689f92257cf14947","modified":1519624147264},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1519624147264},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1519624147264},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1519624147265},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1519624147265},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"34935b40237c074be5f5e8818c14ccfd802b7439","modified":1519624147265},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1519624147265},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1519624147266},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1519624147266},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"a5e3e6b4b4b814a9fe40b34d784fed67d6d977fa","modified":1519624147266},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"1ccfbd4d0f5754b2dc2719a91245c95f547a7652","modified":1519624147266},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1519624147273},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1519624147274},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1519624147275},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1519624147290},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1519624147290},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1519624147291},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1519624147291},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1519624147292},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1519624147303},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1519624147303},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1519624147304},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1519624147304},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1519624147304},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1519624147305},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1519624147313},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1519624147316},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1519624147322},{"_id":"themes/next/.git/objects/pack/pack-b8112ba9b2c5bcb1cb04d783dc033a5ffee52826.idx","hash":"40b3e03b5059218f1654f42fa247459300719d43","modified":1519624147160},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1519624147299},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1519624147339},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1519624147320},{"_id":"themes/next/.git/objects/pack/pack-b8112ba9b2c5bcb1cb04d783dc033a5ffee52826.pack","hash":"e3ce8bde10cc496dd49a3dbb6e8adf7035e5faad","modified":1519624147155}],"Category":[{"name":"Django","_id":"cje6mdfcx0002rcot39gjawb6"},{"name":"Hexo","_id":"cje6mdfde0004rcoti40x7z8p"},{"name":"Spider","_id":"cje6mdfzi000ircotguhvdv78"},{"name":"Git","_id":"cje6mdg0b000prcote0n1s5wa"},{"name":"Urllib","parent":"cje6mdfzi000ircotguhvdv78","_id":"cje6mdg0j000trcoto8xi1zlo"},{"name":"Selenium","parent":"cje6mdfzi000ircotguhvdv78","_id":"cje6mdg0v000yrcotgf6tkvgw"},{"name":"Scrapy","parent":"cje6mdfzi000ircotguhvdv78","_id":"cje6mdg1h001lrcotoyqxlvgz"}],"Data":[],"Page":[{"title":"about","date":"2018-02-27T10:04:16.000Z","_content":"","source":"about/index.md","raw":"---\ntitle: about\ndate: 2018-02-27 18:04:16\n---\n","updated":"2018-02-27T10:04:16.738Z","path":"about/index.html","comments":1,"layout":"page","_id":"cje6mdfz4000frcot901jdgnn","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"分类","date":"2018-02-26T07:16:43.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: 分类\ndate: 2018-02-26 15:16:43\ntype: \"categories\"\n---\n","updated":"2018-02-27T10:50:06.227Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cje6mdfzd000hrcotchcwfsuq","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"标签","date":"2018-02-26T07:13:29.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: 标签\ndate: 2018-02-26 15:13:29\ntype: \"tags\"\n---\n","updated":"2018-02-27T10:50:23.228Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cje6mdfzm000lrcot7j0l2ajk","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"Django2.0.1搭建电影网站","date":"2018-02-27T05:48:44.000Z","_content":"![首页](https://school.thoughtworks.cn/bbs/assets/uploads/files/1517985555955-fireshot-capture-8-cinema-http___127.0.0.1_8000_.png)\n\n本项目已经部署到服务器，可以通过该IP查看\nhttp://59.110.221.56/\n[GitHub源代码](https://github.com/ByiProX/ThoughtWorks-Cinema)\n<!--more-->\n\n## 技术栈\n> Bootstrap 3 + Django 2.0.1 + MySQL 5.7.17 + Nginx + locust\n\n\n## 本地服务运行方法\n\n终端在venv文件夹路径下开启虚拟环境(根据自己的shell进行选择)：\n\n```python\nsource ../venv/bin/activate.fish\nsource ../venv/bin/activate\nsource ../venv/bin/activate.csh\n```\n然后执行：\n```python\npython3 TWS_Cinema/manage.py runserver\n```\n\n**如果报错**，终端进入requirements.txt所在目录，运行命令：\n```python3\npip3 install -r requirements.txt\n```\n\n然后执行：\n```python\npython3 TWS_Cinema/manage.py runserver\n```\n\n## 单元测试运行方法 ##\n在manage.py路径下终端运行\n\n```python\npython3 manage.py test\n```\n\n## 网站功能描述\n\n- 实现导航栏搜索电影，支持按年份搜索和类型搜索\n    -- 显示分类列表\n    -- 点击分类显示符合分类要求的电影\n\n- 实现搜索功能，支持按电影名称模糊搜索\n\n- 实现电影详细信息查看功能\n    -- 显示电影详细信息\n    -- 显示豆瓣 Top 5 影评\n    -- 在电影详细页面显示相似电影推荐\n    -- 增加电影观看链接\n\n## API\n\n- 按电影id搜索 —— api/movie/id/        # 例如：api/movie/id/1291545\n\n- 按电影名搜索 —— api/movie/title/     # 例如：api/movie/title/大鱼\n\n- 按电影原始名搜索 —— api/movie/original_title/     # 例如：api/movie/original_title/Big Fish\n\n- 按电影类型搜索 —— api/movie/genre/   # 例如：api/movie/genre/剧情\n\n- 按电影年份搜索 —— api/movie/year/    # 例如：api/movie/year/2003\n\n\n## 网站性能测试结果\n\n在文件locustfile.py路径下运行\n```python3\nlocust --host=http://59.110.221.56\n```\n\n### 压力测试\n* 采取的框架：**locust**\n* 服务器性能：\n    * CPU：1核\n    * 内存：2 GB (I/O优化)\n    * 带宽：1Mbps\n* 测试结果：\n    * 500人：100%正确\n    * 1000人：40%出错率\n* 测试截图\n\n\n\n![Screen Shot 2018-02-08 at 16.15.49.png](http://upload-images.jianshu.io/upload_images/2952111-4c41c64c40130ebe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n![Screen Shot 2018-02-08 at 16.14.10.png](http://upload-images.jianshu.io/upload_images/2952111-c2c542dbf0ce9e58.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n![Screen Shot 2018-02-08 at 16.13.49.png](http://upload-images.jianshu.io/upload_images/2952111-f5e4ace67f22ddac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n## 电影网站的其他截图\n\n![list](https://school.thoughtworks.cn/bbs/assets/uploads/files/1517985569003-fireshot-capture-9-%E9%A6%96%E9%A1%B5-http___127.0.0.1_8000_movie_display_.png)\n![detail](https://school.thoughtworks.cn/bbs/assets/uploads/files/1517985586166-fireshot-capture-10-%E7%94%B5%E5%BD%B1%E8%AF%A6%E6%83%85-http___127.0.0.1_8000_movie_id_1291545_.png)\n\n\n## Reference\n\n[Locust 简介以及使用](https://my.oschina.net/u/1447352/blog/1499428/)\n","source":"_posts/Django2-0-1搭建电影网站.md","raw":"---\ntitle: Django2.0.1搭建电影网站\ndate: 2018-02-27 13:48:44\ntags:\n  - Django\n  - Python3\ncategories:\n  - Django\n---\n![首页](https://school.thoughtworks.cn/bbs/assets/uploads/files/1517985555955-fireshot-capture-8-cinema-http___127.0.0.1_8000_.png)\n\n本项目已经部署到服务器，可以通过该IP查看\nhttp://59.110.221.56/\n[GitHub源代码](https://github.com/ByiProX/ThoughtWorks-Cinema)\n<!--more-->\n\n## 技术栈\n> Bootstrap 3 + Django 2.0.1 + MySQL 5.7.17 + Nginx + locust\n\n\n## 本地服务运行方法\n\n终端在venv文件夹路径下开启虚拟环境(根据自己的shell进行选择)：\n\n```python\nsource ../venv/bin/activate.fish\nsource ../venv/bin/activate\nsource ../venv/bin/activate.csh\n```\n然后执行：\n```python\npython3 TWS_Cinema/manage.py runserver\n```\n\n**如果报错**，终端进入requirements.txt所在目录，运行命令：\n```python3\npip3 install -r requirements.txt\n```\n\n然后执行：\n```python\npython3 TWS_Cinema/manage.py runserver\n```\n\n## 单元测试运行方法 ##\n在manage.py路径下终端运行\n\n```python\npython3 manage.py test\n```\n\n## 网站功能描述\n\n- 实现导航栏搜索电影，支持按年份搜索和类型搜索\n    -- 显示分类列表\n    -- 点击分类显示符合分类要求的电影\n\n- 实现搜索功能，支持按电影名称模糊搜索\n\n- 实现电影详细信息查看功能\n    -- 显示电影详细信息\n    -- 显示豆瓣 Top 5 影评\n    -- 在电影详细页面显示相似电影推荐\n    -- 增加电影观看链接\n\n## API\n\n- 按电影id搜索 —— api/movie/id/        # 例如：api/movie/id/1291545\n\n- 按电影名搜索 —— api/movie/title/     # 例如：api/movie/title/大鱼\n\n- 按电影原始名搜索 —— api/movie/original_title/     # 例如：api/movie/original_title/Big Fish\n\n- 按电影类型搜索 —— api/movie/genre/   # 例如：api/movie/genre/剧情\n\n- 按电影年份搜索 —— api/movie/year/    # 例如：api/movie/year/2003\n\n\n## 网站性能测试结果\n\n在文件locustfile.py路径下运行\n```python3\nlocust --host=http://59.110.221.56\n```\n\n### 压力测试\n* 采取的框架：**locust**\n* 服务器性能：\n    * CPU：1核\n    * 内存：2 GB (I/O优化)\n    * 带宽：1Mbps\n* 测试结果：\n    * 500人：100%正确\n    * 1000人：40%出错率\n* 测试截图\n\n\n\n![Screen Shot 2018-02-08 at 16.15.49.png](http://upload-images.jianshu.io/upload_images/2952111-4c41c64c40130ebe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n![Screen Shot 2018-02-08 at 16.14.10.png](http://upload-images.jianshu.io/upload_images/2952111-c2c542dbf0ce9e58.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n![Screen Shot 2018-02-08 at 16.13.49.png](http://upload-images.jianshu.io/upload_images/2952111-f5e4ace67f22ddac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n## 电影网站的其他截图\n\n![list](https://school.thoughtworks.cn/bbs/assets/uploads/files/1517985569003-fireshot-capture-9-%E9%A6%96%E9%A1%B5-http___127.0.0.1_8000_movie_display_.png)\n![detail](https://school.thoughtworks.cn/bbs/assets/uploads/files/1517985586166-fireshot-capture-10-%E7%94%B5%E5%BD%B1%E8%AF%A6%E6%83%85-http___127.0.0.1_8000_movie_id_1291545_.png)\n\n\n## Reference\n\n[Locust 简介以及使用](https://my.oschina.net/u/1447352/blog/1499428/)\n","slug":"Django2-0-1搭建电影网站","published":1,"updated":"2018-02-27T06:41:59.694Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cje6mdfbj0000rcotmuv0oxec","content":"<p><img src=\"https://school.thoughtworks.cn/bbs/assets/uploads/files/1517985555955-fireshot-capture-8-cinema-http___127.0.0.1_8000_.png\" alt=\"首页\"></p>\n<p>本项目已经部署到服务器，可以通过该IP查看<br><a href=\"http://59.110.221.56/\" target=\"_blank\" rel=\"noopener\">http://59.110.221.56/</a><br><a href=\"https://github.com/ByiProX/ThoughtWorks-Cinema\" target=\"_blank\" rel=\"noopener\">GitHub源代码</a><br><a id=\"more\"></a></p>\n<h2 id=\"技术栈\"><a href=\"#技术栈\" class=\"headerlink\" title=\"技术栈\"></a>技术栈</h2><blockquote>\n<p>Bootstrap 3 + Django 2.0.1 + MySQL 5.7.17 + Nginx + locust</p>\n</blockquote>\n<h2 id=\"本地服务运行方法\"><a href=\"#本地服务运行方法\" class=\"headerlink\" title=\"本地服务运行方法\"></a>本地服务运行方法</h2><p>终端在venv文件夹路径下开启虚拟环境(根据自己的shell进行选择)：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">source ../venv/bin/activate.fish</span><br><span class=\"line\">source ../venv/bin/activate</span><br><span class=\"line\">source ../venv/bin/activate.csh</span><br></pre></td></tr></table></figure>\n<p>然后执行：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 TWS_Cinema/manage.py runserver</span><br></pre></td></tr></table></figure></p>\n<p><strong>如果报错</strong>，终端进入requirements.txt所在目录，运行命令：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip3 install -r requirements.txt</span><br></pre></td></tr></table></figure></p>\n<p>然后执行：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 TWS_Cinema/manage.py runserver</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"单元测试运行方法\"><a href=\"#单元测试运行方法\" class=\"headerlink\" title=\"单元测试运行方法\"></a>单元测试运行方法</h2><p>在manage.py路径下终端运行</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 manage.py test</span><br></pre></td></tr></table></figure>\n<h2 id=\"网站功能描述\"><a href=\"#网站功能描述\" class=\"headerlink\" title=\"网站功能描述\"></a>网站功能描述</h2><ul>\n<li><p>实现导航栏搜索电影，支持按年份搜索和类型搜索<br>  – 显示分类列表<br>  – 点击分类显示符合分类要求的电影</p>\n</li>\n<li><p>实现搜索功能，支持按电影名称模糊搜索</p>\n</li>\n<li><p>实现电影详细信息查看功能<br>  – 显示电影详细信息<br>  – 显示豆瓣 Top 5 影评<br>  – 在电影详细页面显示相似电影推荐<br>  – 增加电影观看链接</p>\n</li>\n</ul>\n<h2 id=\"API\"><a href=\"#API\" class=\"headerlink\" title=\"API\"></a>API</h2><ul>\n<li><p>按电影id搜索 —— api/movie/id/        # 例如：api/movie/id/1291545</p>\n</li>\n<li><p>按电影名搜索 —— api/movie/title/     # 例如：api/movie/title/大鱼</p>\n</li>\n<li><p>按电影原始名搜索 —— api/movie/original_title/     # 例如：api/movie/original_title/Big Fish</p>\n</li>\n<li><p>按电影类型搜索 —— api/movie/genre/   # 例如：api/movie/genre/剧情</p>\n</li>\n<li><p>按电影年份搜索 —— api/movie/year/    # 例如：api/movie/year/2003</p>\n</li>\n</ul>\n<h2 id=\"网站性能测试结果\"><a href=\"#网站性能测试结果\" class=\"headerlink\" title=\"网站性能测试结果\"></a>网站性能测试结果</h2><p>在文件locustfile.py路径下运行<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">locust --host=http://59.110.221.56</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"压力测试\"><a href=\"#压力测试\" class=\"headerlink\" title=\"压力测试\"></a>压力测试</h3><ul>\n<li>采取的框架：<strong>locust</strong></li>\n<li>服务器性能：<ul>\n<li>CPU：1核</li>\n<li>内存：2 GB (I/O优化)</li>\n<li>带宽：1Mbps</li>\n</ul>\n</li>\n<li>测试结果：<ul>\n<li>500人：100%正确</li>\n<li>1000人：40%出错率</li>\n</ul>\n</li>\n<li>测试截图</li>\n</ul>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/2952111-4c41c64c40130ebe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Screen Shot 2018-02-08 at 16.15.49.png\"></p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/2952111-c2c542dbf0ce9e58.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Screen Shot 2018-02-08 at 16.14.10.png\"></p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/2952111-f5e4ace67f22ddac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Screen Shot 2018-02-08 at 16.13.49.png\"></p>\n<h2 id=\"电影网站的其他截图\"><a href=\"#电影网站的其他截图\" class=\"headerlink\" title=\"电影网站的其他截图\"></a>电影网站的其他截图</h2><p><img src=\"https://school.thoughtworks.cn/bbs/assets/uploads/files/1517985569003-fireshot-capture-9-%E9%A6%96%E9%A1%B5-http___127.0.0.1_8000_movie_display_.png\" alt=\"list\"><br><img src=\"https://school.thoughtworks.cn/bbs/assets/uploads/files/1517985586166-fireshot-capture-10-%E7%94%B5%E5%BD%B1%E8%AF%A6%E6%83%85-http___127.0.0.1_8000_movie_id_1291545_.png\" alt=\"detail\"></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><p><a href=\"https://my.oschina.net/u/1447352/blog/1499428/\" target=\"_blank\" rel=\"noopener\">Locust 简介以及使用</a></p>\n","site":{"data":{}},"excerpt":"<p><img src=\"https://school.thoughtworks.cn/bbs/assets/uploads/files/1517985555955-fireshot-capture-8-cinema-http___127.0.0.1_8000_.png\" alt=\"首页\"></p>\n<p>本项目已经部署到服务器，可以通过该IP查看<br><a href=\"http://59.110.221.56/\" target=\"_blank\" rel=\"noopener\">http://59.110.221.56/</a><br><a href=\"https://github.com/ByiProX/ThoughtWorks-Cinema\" target=\"_blank\" rel=\"noopener\">GitHub源代码</a><br>","more":"</p>\n<h2 id=\"技术栈\"><a href=\"#技术栈\" class=\"headerlink\" title=\"技术栈\"></a>技术栈</h2><blockquote>\n<p>Bootstrap 3 + Django 2.0.1 + MySQL 5.7.17 + Nginx + locust</p>\n</blockquote>\n<h2 id=\"本地服务运行方法\"><a href=\"#本地服务运行方法\" class=\"headerlink\" title=\"本地服务运行方法\"></a>本地服务运行方法</h2><p>终端在venv文件夹路径下开启虚拟环境(根据自己的shell进行选择)：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">source ../venv/bin/activate.fish</span><br><span class=\"line\">source ../venv/bin/activate</span><br><span class=\"line\">source ../venv/bin/activate.csh</span><br></pre></td></tr></table></figure>\n<p>然后执行：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 TWS_Cinema/manage.py runserver</span><br></pre></td></tr></table></figure></p>\n<p><strong>如果报错</strong>，终端进入requirements.txt所在目录，运行命令：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip3 install -r requirements.txt</span><br></pre></td></tr></table></figure></p>\n<p>然后执行：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 TWS_Cinema/manage.py runserver</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"单元测试运行方法\"><a href=\"#单元测试运行方法\" class=\"headerlink\" title=\"单元测试运行方法\"></a>单元测试运行方法</h2><p>在manage.py路径下终端运行</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 manage.py test</span><br></pre></td></tr></table></figure>\n<h2 id=\"网站功能描述\"><a href=\"#网站功能描述\" class=\"headerlink\" title=\"网站功能描述\"></a>网站功能描述</h2><ul>\n<li><p>实现导航栏搜索电影，支持按年份搜索和类型搜索<br>  – 显示分类列表<br>  – 点击分类显示符合分类要求的电影</p>\n</li>\n<li><p>实现搜索功能，支持按电影名称模糊搜索</p>\n</li>\n<li><p>实现电影详细信息查看功能<br>  – 显示电影详细信息<br>  – 显示豆瓣 Top 5 影评<br>  – 在电影详细页面显示相似电影推荐<br>  – 增加电影观看链接</p>\n</li>\n</ul>\n<h2 id=\"API\"><a href=\"#API\" class=\"headerlink\" title=\"API\"></a>API</h2><ul>\n<li><p>按电影id搜索 —— api/movie/id/        # 例如：api/movie/id/1291545</p>\n</li>\n<li><p>按电影名搜索 —— api/movie/title/     # 例如：api/movie/title/大鱼</p>\n</li>\n<li><p>按电影原始名搜索 —— api/movie/original_title/     # 例如：api/movie/original_title/Big Fish</p>\n</li>\n<li><p>按电影类型搜索 —— api/movie/genre/   # 例如：api/movie/genre/剧情</p>\n</li>\n<li><p>按电影年份搜索 —— api/movie/year/    # 例如：api/movie/year/2003</p>\n</li>\n</ul>\n<h2 id=\"网站性能测试结果\"><a href=\"#网站性能测试结果\" class=\"headerlink\" title=\"网站性能测试结果\"></a>网站性能测试结果</h2><p>在文件locustfile.py路径下运行<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">locust --host=http://59.110.221.56</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"压力测试\"><a href=\"#压力测试\" class=\"headerlink\" title=\"压力测试\"></a>压力测试</h3><ul>\n<li>采取的框架：<strong>locust</strong></li>\n<li>服务器性能：<ul>\n<li>CPU：1核</li>\n<li>内存：2 GB (I/O优化)</li>\n<li>带宽：1Mbps</li>\n</ul>\n</li>\n<li>测试结果：<ul>\n<li>500人：100%正确</li>\n<li>1000人：40%出错率</li>\n</ul>\n</li>\n<li>测试截图</li>\n</ul>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/2952111-4c41c64c40130ebe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Screen Shot 2018-02-08 at 16.15.49.png\"></p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/2952111-c2c542dbf0ce9e58.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Screen Shot 2018-02-08 at 16.14.10.png\"></p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/2952111-f5e4ace67f22ddac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Screen Shot 2018-02-08 at 16.13.49.png\"></p>\n<h2 id=\"电影网站的其他截图\"><a href=\"#电影网站的其他截图\" class=\"headerlink\" title=\"电影网站的其他截图\"></a>电影网站的其他截图</h2><p><img src=\"https://school.thoughtworks.cn/bbs/assets/uploads/files/1517985569003-fireshot-capture-9-%E9%A6%96%E9%A1%B5-http___127.0.0.1_8000_movie_display_.png\" alt=\"list\"><br><img src=\"https://school.thoughtworks.cn/bbs/assets/uploads/files/1517985586166-fireshot-capture-10-%E7%94%B5%E5%BD%B1%E8%AF%A6%E6%83%85-http___127.0.0.1_8000_movie_id_1291545_.png\" alt=\"detail\"></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><p><a href=\"https://my.oschina.net/u/1447352/blog/1499428/\" target=\"_blank\" rel=\"noopener\">Locust 简介以及使用</a></p>"},{"title":"Github多分支管理Hexo-Blog项目","date":"2018-02-26T16:23:41.000Z","_content":"\nHexo在部署之后在github的仓库中我们只能找到生成的静态文件。然而博客的源文件：主题、文章、配置等文件都还在本地，并没有备份。对于多台终端设备的用户不够友好，而且存在一定的风险，万一那天电脑坏了或者是出现一些其他问题，就得从头再来。为了解决上述问题，我们可以利用github的分支思想来备份我们的源文件。\n\n<!--more-->\n\n备份之前，需要了解博客根目录下面的文件以及文件夹作用：\n```python\n.deploy_git/        网站静态文件(git)\nnode_modules/       插件\npublic/             网站静态文件\nscaffolds/          文章模板\nsource/             博文等\nthemes/             主题\n_config.yml         网站配置文件\npackage.json        Hexo信息\ndb.json             数据文件\n```\n## 备份的思路\n`master分支存放部署生成的静态文件，Hexo-Bog分支存放我们要备份项目源文件。`实际备份中，.deploy_git、public文件夹和我们的master分支内容重复，所以略过。因此，我们在根目录下面建一个**.gitignore**文件来建立“黑名单”，禁止备份。\n\n## 编辑**.gitignore**过滤文件\n文件内容如下：\n```python\n.DS_Store\npublic/\n.deploy*/\n```\n## 关于备份\n终端中在项目的根目录下执行，对于作者自己的项目，命令执行的路径为ByiProX/下：\n```Bash\n$ git init\n$ git remote add origin git@github.com:username/username.github.io.git\t\t\n# username为博客项目的名称，也就是git的用户名\n$ git add .\n$ git commit -m \"ready for backup of the project\"\n$ git push origin master:Hexo-Blog\n```\n\n执行完毕后会发现github博客仓库已经有了一个新分支Hexo-Blog，于是备份工作完成。\n以后，开始写博文时，即终端运行\n```Bash\n$ hexo new [layout] <title>\n```\n完成文章后,对编辑后的文章进行备份保存，即终端运行,为下面的部署做准备\n```Bash\n    $ git add .\n    $ git commit -m \"add one article\"\n    $ git push origin master:Hexo-Blog\n```\n\n## 部署\n运行一下命令进行仓库master分支静态文件部署\n```Bash\n$ hexo clean\n$ hexo generate\n$ hexo deploy\n```\n\n以上完成项目源文件以及静态文件的Git管理\n\n## 参考文献及进阶\n[Hexo+github搭建个人博客并实现多终端管理](https://mrlrf.github.io/2017/05/05/Hexo-github%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/)\n[如何在github上面备份Hexo](https://blog.zaihua.me/post/blog_github_backup.html)\n[Hexo的版本控制与持续集成](https://formulahendry.github.io/2016/12/04/hexo-ci/)\n[使用hexo，如果换了电脑怎么更新博客](https://www.zhihu.com/question/21193762)\n","source":"_posts/Github多分支管理Hexo-Blog项目.md","raw":"---\ntitle: Github多分支管理Hexo-Blog项目\ndate: 2018-02-27 00:23:41\ntags:\n  - Hexo\n  - Git\ncategories:\n  - Hexo\n---\n\nHexo在部署之后在github的仓库中我们只能找到生成的静态文件。然而博客的源文件：主题、文章、配置等文件都还在本地，并没有备份。对于多台终端设备的用户不够友好，而且存在一定的风险，万一那天电脑坏了或者是出现一些其他问题，就得从头再来。为了解决上述问题，我们可以利用github的分支思想来备份我们的源文件。\n\n<!--more-->\n\n备份之前，需要了解博客根目录下面的文件以及文件夹作用：\n```python\n.deploy_git/        网站静态文件(git)\nnode_modules/       插件\npublic/             网站静态文件\nscaffolds/          文章模板\nsource/             博文等\nthemes/             主题\n_config.yml         网站配置文件\npackage.json        Hexo信息\ndb.json             数据文件\n```\n## 备份的思路\n`master分支存放部署生成的静态文件，Hexo-Bog分支存放我们要备份项目源文件。`实际备份中，.deploy_git、public文件夹和我们的master分支内容重复，所以略过。因此，我们在根目录下面建一个**.gitignore**文件来建立“黑名单”，禁止备份。\n\n## 编辑**.gitignore**过滤文件\n文件内容如下：\n```python\n.DS_Store\npublic/\n.deploy*/\n```\n## 关于备份\n终端中在项目的根目录下执行，对于作者自己的项目，命令执行的路径为ByiProX/下：\n```Bash\n$ git init\n$ git remote add origin git@github.com:username/username.github.io.git\t\t\n# username为博客项目的名称，也就是git的用户名\n$ git add .\n$ git commit -m \"ready for backup of the project\"\n$ git push origin master:Hexo-Blog\n```\n\n执行完毕后会发现github博客仓库已经有了一个新分支Hexo-Blog，于是备份工作完成。\n以后，开始写博文时，即终端运行\n```Bash\n$ hexo new [layout] <title>\n```\n完成文章后,对编辑后的文章进行备份保存，即终端运行,为下面的部署做准备\n```Bash\n    $ git add .\n    $ git commit -m \"add one article\"\n    $ git push origin master:Hexo-Blog\n```\n\n## 部署\n运行一下命令进行仓库master分支静态文件部署\n```Bash\n$ hexo clean\n$ hexo generate\n$ hexo deploy\n```\n\n以上完成项目源文件以及静态文件的Git管理\n\n## 参考文献及进阶\n[Hexo+github搭建个人博客并实现多终端管理](https://mrlrf.github.io/2017/05/05/Hexo-github%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/)\n[如何在github上面备份Hexo](https://blog.zaihua.me/post/blog_github_backup.html)\n[Hexo的版本控制与持续集成](https://formulahendry.github.io/2016/12/04/hexo-ci/)\n[使用hexo，如果换了电脑怎么更新博客](https://www.zhihu.com/question/21193762)\n","slug":"Github多分支管理Hexo-Blog项目","published":1,"updated":"2018-02-27T06:40:56.801Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cje6mdfce0001rcotybvv7qma","content":"<p>Hexo在部署之后在github的仓库中我们只能找到生成的静态文件。然而博客的源文件：主题、文章、配置等文件都还在本地，并没有备份。对于多台终端设备的用户不够友好，而且存在一定的风险，万一那天电脑坏了或者是出现一些其他问题，就得从头再来。为了解决上述问题，我们可以利用github的分支思想来备份我们的源文件。</p>\n<a id=\"more\"></a>\n<p>备份之前，需要了解博客根目录下面的文件以及文件夹作用：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">.deploy_git/        网站静态文件(git)</span><br><span class=\"line\">node_modules/       插件</span><br><span class=\"line\">public/             网站静态文件</span><br><span class=\"line\">scaffolds/          文章模板</span><br><span class=\"line\">source/             博文等</span><br><span class=\"line\">themes/             主题</span><br><span class=\"line\">_config.yml         网站配置文件</span><br><span class=\"line\">package.json        Hexo信息</span><br><span class=\"line\">db.json             数据文件</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"备份的思路\"><a href=\"#备份的思路\" class=\"headerlink\" title=\"备份的思路\"></a>备份的思路</h2><p><code>master分支存放部署生成的静态文件，Hexo-Bog分支存放我们要备份项目源文件。</code>实际备份中，.deploy_git、public文件夹和我们的master分支内容重复，所以略过。因此，我们在根目录下面建一个<strong>.gitignore</strong>文件来建立“黑名单”，禁止备份。</p>\n<h2 id=\"编辑-gitignore过滤文件\"><a href=\"#编辑-gitignore过滤文件\" class=\"headerlink\" title=\"编辑.gitignore过滤文件\"></a>编辑<strong>.gitignore</strong>过滤文件</h2><p>文件内容如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">.DS_Store</span><br><span class=\"line\">public/</span><br><span class=\"line\">.deploy*/</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"关于备份\"><a href=\"#关于备份\" class=\"headerlink\" title=\"关于备份\"></a>关于备份</h2><p>终端中在项目的根目录下执行，对于作者自己的项目，命令执行的路径为ByiProX/下：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git init</span><br><span class=\"line\">$ git remote add origin git@github.com:username/username.github.io.git\t\t</span><br><span class=\"line\"><span class=\"comment\"># username为博客项目的名称，也就是git的用户名</span></span><br><span class=\"line\">$ git add .</span><br><span class=\"line\">$ git commit -m <span class=\"string\">\"ready for backup of the project\"</span></span><br><span class=\"line\">$ git push origin master:Hexo-Blog</span><br></pre></td></tr></table></figure></p>\n<p>执行完毕后会发现github博客仓库已经有了一个新分支Hexo-Blog，于是备份工作完成。<br>以后，开始写博文时，即终端运行<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new [layout] &lt;title&gt;</span><br></pre></td></tr></table></figure></p>\n<p>完成文章后,对编辑后的文章进行备份保存，即终端运行,为下面的部署做准备<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git add .</span><br><span class=\"line\">$ git commit -m <span class=\"string\">\"add one article\"</span></span><br><span class=\"line\">$ git push origin master:Hexo-Blog</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"部署\"><a href=\"#部署\" class=\"headerlink\" title=\"部署\"></a>部署</h2><p>运行一下命令进行仓库master分支静态文件部署<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo clean</span><br><span class=\"line\">$ hexo generate</span><br><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure></p>\n<p>以上完成项目源文件以及静态文件的Git管理</p>\n<h2 id=\"参考文献及进阶\"><a href=\"#参考文献及进阶\" class=\"headerlink\" title=\"参考文献及进阶\"></a>参考文献及进阶</h2><p><a href=\"https://mrlrf.github.io/2017/05/05/Hexo-github%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/\" target=\"_blank\" rel=\"noopener\">Hexo+github搭建个人博客并实现多终端管理</a><br><a href=\"https://blog.zaihua.me/post/blog_github_backup.html\" target=\"_blank\" rel=\"noopener\">如何在github上面备份Hexo</a><br><a href=\"https://formulahendry.github.io/2016/12/04/hexo-ci/\" target=\"_blank\" rel=\"noopener\">Hexo的版本控制与持续集成</a><br><a href=\"https://www.zhihu.com/question/21193762\" target=\"_blank\" rel=\"noopener\">使用hexo，如果换了电脑怎么更新博客</a></p>\n","site":{"data":{}},"excerpt":"<p>Hexo在部署之后在github的仓库中我们只能找到生成的静态文件。然而博客的源文件：主题、文章、配置等文件都还在本地，并没有备份。对于多台终端设备的用户不够友好，而且存在一定的风险，万一那天电脑坏了或者是出现一些其他问题，就得从头再来。为了解决上述问题，我们可以利用github的分支思想来备份我们的源文件。</p>","more":"<p>备份之前，需要了解博客根目录下面的文件以及文件夹作用：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">.deploy_git/        网站静态文件(git)</span><br><span class=\"line\">node_modules/       插件</span><br><span class=\"line\">public/             网站静态文件</span><br><span class=\"line\">scaffolds/          文章模板</span><br><span class=\"line\">source/             博文等</span><br><span class=\"line\">themes/             主题</span><br><span class=\"line\">_config.yml         网站配置文件</span><br><span class=\"line\">package.json        Hexo信息</span><br><span class=\"line\">db.json             数据文件</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"备份的思路\"><a href=\"#备份的思路\" class=\"headerlink\" title=\"备份的思路\"></a>备份的思路</h2><p><code>master分支存放部署生成的静态文件，Hexo-Bog分支存放我们要备份项目源文件。</code>实际备份中，.deploy_git、public文件夹和我们的master分支内容重复，所以略过。因此，我们在根目录下面建一个<strong>.gitignore</strong>文件来建立“黑名单”，禁止备份。</p>\n<h2 id=\"编辑-gitignore过滤文件\"><a href=\"#编辑-gitignore过滤文件\" class=\"headerlink\" title=\"编辑.gitignore过滤文件\"></a>编辑<strong>.gitignore</strong>过滤文件</h2><p>文件内容如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">.DS_Store</span><br><span class=\"line\">public/</span><br><span class=\"line\">.deploy*/</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"关于备份\"><a href=\"#关于备份\" class=\"headerlink\" title=\"关于备份\"></a>关于备份</h2><p>终端中在项目的根目录下执行，对于作者自己的项目，命令执行的路径为ByiProX/下：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git init</span><br><span class=\"line\">$ git remote add origin git@github.com:username/username.github.io.git\t\t</span><br><span class=\"line\"><span class=\"comment\"># username为博客项目的名称，也就是git的用户名</span></span><br><span class=\"line\">$ git add .</span><br><span class=\"line\">$ git commit -m <span class=\"string\">\"ready for backup of the project\"</span></span><br><span class=\"line\">$ git push origin master:Hexo-Blog</span><br></pre></td></tr></table></figure></p>\n<p>执行完毕后会发现github博客仓库已经有了一个新分支Hexo-Blog，于是备份工作完成。<br>以后，开始写博文时，即终端运行<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new [layout] &lt;title&gt;</span><br></pre></td></tr></table></figure></p>\n<p>完成文章后,对编辑后的文章进行备份保存，即终端运行,为下面的部署做准备<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git add .</span><br><span class=\"line\">$ git commit -m <span class=\"string\">\"add one article\"</span></span><br><span class=\"line\">$ git push origin master:Hexo-Blog</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"部署\"><a href=\"#部署\" class=\"headerlink\" title=\"部署\"></a>部署</h2><p>运行一下命令进行仓库master分支静态文件部署<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo clean</span><br><span class=\"line\">$ hexo generate</span><br><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure></p>\n<p>以上完成项目源文件以及静态文件的Git管理</p>\n<h2 id=\"参考文献及进阶\"><a href=\"#参考文献及进阶\" class=\"headerlink\" title=\"参考文献及进阶\"></a>参考文献及进阶</h2><p><a href=\"https://mrlrf.github.io/2017/05/05/Hexo-github%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/\" target=\"_blank\" rel=\"noopener\">Hexo+github搭建个人博客并实现多终端管理</a><br><a href=\"https://blog.zaihua.me/post/blog_github_backup.html\" target=\"_blank\" rel=\"noopener\">如何在github上面备份Hexo</a><br><a href=\"https://formulahendry.github.io/2016/12/04/hexo-ci/\" target=\"_blank\" rel=\"noopener\">Hexo的版本控制与持续集成</a><br><a href=\"https://www.zhihu.com/question/21193762\" target=\"_blank\" rel=\"noopener\">使用hexo，如果换了电脑怎么更新博客</a></p>"},{"title":"Python3下使用Selenium&PhantomJS爬火影忍者漫画","date":"2018-02-27T06:16:38.000Z","_content":"\n近期学习爬虫，发现懂的越多，不懂的知识点越多（所以当个傻子还是很幸福的）。好记性不如烂笔头，之前都是把看到的资料链接直接挂到一些平台，比如知乎、简书、Github等。今天有点时间，就好好码一下字，排排版，方便以后查阅。\n\nSelenium用来模拟浏览器的行为，比如点击、最大化、滚动窗口等；PhantomJS是一种浏览器，不过这种浏览器没有UI界面，感觉就像是专门为爬虫设计，优点很明显，可以有效减小内存的使用。\n\n<!--more-->\n\n## 爬虫使用到的模块\n```Python\nfrom selenium import webdriver\nfrom myLogging import MyLogging\nimport os\nimport time\nimport re\n```\n\nmyLogging模块是自己配置的日志包，想要的可以点击**_[这里](http://link.zhihu.com/?target=https%3A//github.com/ByiProX/DownloadPicsBySeleniumAndPhantomJS)_**自己看\n\n爬虫很关键的一点就是能够看懂网页的源代码，记得当初刚刚真正开始接触编程的时候，有很长的一段时间在看HTML、CSS、JS的一些知识，虽然忘得很多，但是印象还是有的，对于后面看网页源代码很有帮助。学习爬虫，除了会基本的python知识以外，还要会网页的一些知识。\n\n## 爬取图片思路：\n\n  * 已知连接，分析网页的代码结构，看所需的数据是否需要切换frame，并定位所需数据的位于哪个标签之下\n  * 采用不同的模块有不同的保存图片方式，如果采用request模块，保存图片方式是可以采用**requests.get(comicUrl).content**方法，使用该方法需要确定网页的地址。该项目中没有涉及request的使用，所以此后不再表述。对于selenium可以使用 **get_screenshot_as_file()** 方法，使用该方法强烈建议使用phantomjs，如果使用chrome浏览器，图片尺寸太大的话，会出现截图不完整，对比如下：\n  ![对比](http://upload-images.jianshu.io/upload_images/2952111-2323c462a546dcc3.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n  * 找到下一张图片的连接位置并点击更新网页，一般来讲新网页与之前网页结构相同。\n  * 在新网页的基础上保存图片，设置循环如此反复。\n\n爬取网页的URL为：[爬取火影漫画第一话](http://link.zhihu.com/?target=http%3A//comic.kukudm.com/comiclist/3/3/1.htm)\n\n## 代码\n\n```Python\nclass DownloadPics(object):\n\n    def __init__(self, url):\n        self.url = url\n        self.log = MyLogging()\n        self.browser = self.get_browser()\n        self.save_pics(self.browser)\n\n    def get_browser(self):\n        browser = webdriver.PhantomJS()\n        try:\n            browser.get(self.url)\n        except:\n            MyLogging.error('open the url %s failed' % self.url)\n        browser.implicitly_wait(20)\n        return browser\n\n    def save_pics(self, browser):\n        pics_title = browser.title.split('_')[0]\n        self.create_dir(pics_title)\n        os.chdir(pics_title)\n        sum_page = self.find_total_page_num(browser)\n        i = 1\n        while i < sum_page:\n            image_name = str(i) + '.png'\n            browser.get_screenshot_as_file(image_name)  \n            # 使用PhantomJS避免了截图的不完整，可以与Chrome比较\n            self.log.info('saving image %s' % image_name)\n            i += 1\n            css_selector = \"a[href='/comiclist/3/3/%s.htm']\" % i  \n            # 该方法感觉还不错呢，不过这个网站确实挺差劲的\n            next_page = browser.find_element_by_css_selector(css_selector)\n            next_page.click()\n            time.sleep(2)\n            # browser.implicitly_wait(20)\n\n    def find_total_page_num(self, browser):\n        page_element = browser.find_element_by_css_selector(\"table[cellspacing='1']\")\n        num = re.search(r'共\\d+页', page_element.text).group()[1:-1]  \n        return int(num)\n\n    def create_dir(self, dir_name):\n        if os.path.exists(dir_name):\n            self.log.error('create directory %s failed cause a same directory exists' % dir_name)\n        else:\n            try:\n                os.makedirs(dir_name)\n            except:\n                self.log.error('create directory %s failed' % dir_name)\n            else:\n                self.log.info('create directory %s success' % dir_name)\n\nif __name__ == '__main__':\n    start_url = 'http://comic.kukudm.com/comiclist/3/3/1.htm'\n    DL = DownloadPics(start_url)\n```\n\n## 运行结果\n\n\n![gif](http://upload-images.jianshu.io/upload_images/2952111-e7e2cf39116b5fea.gif?imageMogr2/auto-orient/strip)\n","source":"_posts/Python3下使用Selenium-PhantomJS爬火影忍者漫画.md","raw":"---\ntitle: Python3下使用Selenium&PhantomJS爬火影忍者漫画\ndate: 2018-02-27 14:16:38\ntags:\n  - Spider\n  - Selenium\n  - PhantomJS\n  - Python3\ncategories:\n  - Spider\n  - Selenium\n\n---\n\n近期学习爬虫，发现懂的越多，不懂的知识点越多（所以当个傻子还是很幸福的）。好记性不如烂笔头，之前都是把看到的资料链接直接挂到一些平台，比如知乎、简书、Github等。今天有点时间，就好好码一下字，排排版，方便以后查阅。\n\nSelenium用来模拟浏览器的行为，比如点击、最大化、滚动窗口等；PhantomJS是一种浏览器，不过这种浏览器没有UI界面，感觉就像是专门为爬虫设计，优点很明显，可以有效减小内存的使用。\n\n<!--more-->\n\n## 爬虫使用到的模块\n```Python\nfrom selenium import webdriver\nfrom myLogging import MyLogging\nimport os\nimport time\nimport re\n```\n\nmyLogging模块是自己配置的日志包，想要的可以点击**_[这里](http://link.zhihu.com/?target=https%3A//github.com/ByiProX/DownloadPicsBySeleniumAndPhantomJS)_**自己看\n\n爬虫很关键的一点就是能够看懂网页的源代码，记得当初刚刚真正开始接触编程的时候，有很长的一段时间在看HTML、CSS、JS的一些知识，虽然忘得很多，但是印象还是有的，对于后面看网页源代码很有帮助。学习爬虫，除了会基本的python知识以外，还要会网页的一些知识。\n\n## 爬取图片思路：\n\n  * 已知连接，分析网页的代码结构，看所需的数据是否需要切换frame，并定位所需数据的位于哪个标签之下\n  * 采用不同的模块有不同的保存图片方式，如果采用request模块，保存图片方式是可以采用**requests.get(comicUrl).content**方法，使用该方法需要确定网页的地址。该项目中没有涉及request的使用，所以此后不再表述。对于selenium可以使用 **get_screenshot_as_file()** 方法，使用该方法强烈建议使用phantomjs，如果使用chrome浏览器，图片尺寸太大的话，会出现截图不完整，对比如下：\n  ![对比](http://upload-images.jianshu.io/upload_images/2952111-2323c462a546dcc3.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n  * 找到下一张图片的连接位置并点击更新网页，一般来讲新网页与之前网页结构相同。\n  * 在新网页的基础上保存图片，设置循环如此反复。\n\n爬取网页的URL为：[爬取火影漫画第一话](http://link.zhihu.com/?target=http%3A//comic.kukudm.com/comiclist/3/3/1.htm)\n\n## 代码\n\n```Python\nclass DownloadPics(object):\n\n    def __init__(self, url):\n        self.url = url\n        self.log = MyLogging()\n        self.browser = self.get_browser()\n        self.save_pics(self.browser)\n\n    def get_browser(self):\n        browser = webdriver.PhantomJS()\n        try:\n            browser.get(self.url)\n        except:\n            MyLogging.error('open the url %s failed' % self.url)\n        browser.implicitly_wait(20)\n        return browser\n\n    def save_pics(self, browser):\n        pics_title = browser.title.split('_')[0]\n        self.create_dir(pics_title)\n        os.chdir(pics_title)\n        sum_page = self.find_total_page_num(browser)\n        i = 1\n        while i < sum_page:\n            image_name = str(i) + '.png'\n            browser.get_screenshot_as_file(image_name)  \n            # 使用PhantomJS避免了截图的不完整，可以与Chrome比较\n            self.log.info('saving image %s' % image_name)\n            i += 1\n            css_selector = \"a[href='/comiclist/3/3/%s.htm']\" % i  \n            # 该方法感觉还不错呢，不过这个网站确实挺差劲的\n            next_page = browser.find_element_by_css_selector(css_selector)\n            next_page.click()\n            time.sleep(2)\n            # browser.implicitly_wait(20)\n\n    def find_total_page_num(self, browser):\n        page_element = browser.find_element_by_css_selector(\"table[cellspacing='1']\")\n        num = re.search(r'共\\d+页', page_element.text).group()[1:-1]  \n        return int(num)\n\n    def create_dir(self, dir_name):\n        if os.path.exists(dir_name):\n            self.log.error('create directory %s failed cause a same directory exists' % dir_name)\n        else:\n            try:\n                os.makedirs(dir_name)\n            except:\n                self.log.error('create directory %s failed' % dir_name)\n            else:\n                self.log.info('create directory %s success' % dir_name)\n\nif __name__ == '__main__':\n    start_url = 'http://comic.kukudm.com/comiclist/3/3/1.htm'\n    DL = DownloadPics(start_url)\n```\n\n## 运行结果\n\n\n![gif](http://upload-images.jianshu.io/upload_images/2952111-e7e2cf39116b5fea.gif?imageMogr2/auto-orient/strip)\n","slug":"Python3下使用Selenium-PhantomJS爬火影忍者漫画","published":1,"updated":"2018-02-27T17:03:57.896Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cje6mdfz1000ercot5f59q97d","content":"<p>近期学习爬虫，发现懂的越多，不懂的知识点越多（所以当个傻子还是很幸福的）。好记性不如烂笔头，之前都是把看到的资料链接直接挂到一些平台，比如知乎、简书、Github等。今天有点时间，就好好码一下字，排排版，方便以后查阅。</p>\n<p>Selenium用来模拟浏览器的行为，比如点击、最大化、滚动窗口等；PhantomJS是一种浏览器，不过这种浏览器没有UI界面，感觉就像是专门为爬虫设计，优点很明显，可以有效减小内存的使用。</p>\n<a id=\"more\"></a>\n<h2 id=\"爬虫使用到的模块\"><a href=\"#爬虫使用到的模块\" class=\"headerlink\" title=\"爬虫使用到的模块\"></a>爬虫使用到的模块</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> selenium <span class=\"keyword\">import</span> webdriver</span><br><span class=\"line\"><span class=\"keyword\">from</span> myLogging <span class=\"keyword\">import</span> MyLogging</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> time</span><br><span class=\"line\"><span class=\"keyword\">import</span> re</span><br></pre></td></tr></table></figure>\n<p>myLogging模块是自己配置的日志包，想要的可以点击<strong><em><a href=\"http://link.zhihu.com/?target=https%3A//github.com/ByiProX/DownloadPicsBySeleniumAndPhantomJS\" target=\"_blank\" rel=\"noopener\">这里</a></em></strong>自己看</p>\n<p>爬虫很关键的一点就是能够看懂网页的源代码，记得当初刚刚真正开始接触编程的时候，有很长的一段时间在看HTML、CSS、JS的一些知识，虽然忘得很多，但是印象还是有的，对于后面看网页源代码很有帮助。学习爬虫，除了会基本的python知识以外，还要会网页的一些知识。</p>\n<h2 id=\"爬取图片思路：\"><a href=\"#爬取图片思路：\" class=\"headerlink\" title=\"爬取图片思路：\"></a>爬取图片思路：</h2><ul>\n<li>已知连接，分析网页的代码结构，看所需的数据是否需要切换frame，并定位所需数据的位于哪个标签之下</li>\n<li>采用不同的模块有不同的保存图片方式，如果采用request模块，保存图片方式是可以采用<strong>requests.get(comicUrl).content</strong>方法，使用该方法需要确定网页的地址。该项目中没有涉及request的使用，所以此后不再表述。对于selenium可以使用 <strong>get_screenshot_as_file()</strong> 方法，使用该方法强烈建议使用phantomjs，如果使用chrome浏览器，图片尺寸太大的话，会出现截图不完整，对比如下：<br><img src=\"http://upload-images.jianshu.io/upload_images/2952111-2323c462a546dcc3.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"对比\"></li>\n<li>找到下一张图片的连接位置并点击更新网页，一般来讲新网页与之前网页结构相同。</li>\n<li>在新网页的基础上保存图片，设置循环如此反复。</li>\n</ul>\n<p>爬取网页的URL为：<a href=\"http://link.zhihu.com/?target=http%3A//comic.kukudm.com/comiclist/3/3/1.htm\" target=\"_blank\" rel=\"noopener\">爬取火影漫画第一话</a></p>\n<h2 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DownloadPics</span><span class=\"params\">(object)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, url)</span>:</span></span><br><span class=\"line\">        self.url = url</span><br><span class=\"line\">        self.log = MyLogging()</span><br><span class=\"line\">        self.browser = self.get_browser()</span><br><span class=\"line\">        self.save_pics(self.browser)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_browser</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        browser = webdriver.PhantomJS()</span><br><span class=\"line\">        <span class=\"keyword\">try</span>:</span><br><span class=\"line\">            browser.get(self.url)</span><br><span class=\"line\">        <span class=\"keyword\">except</span>:</span><br><span class=\"line\">            MyLogging.error(<span class=\"string\">'open the url %s failed'</span> % self.url)</span><br><span class=\"line\">        browser.implicitly_wait(<span class=\"number\">20</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> browser</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">save_pics</span><span class=\"params\">(self, browser)</span>:</span></span><br><span class=\"line\">        pics_title = browser.title.split(<span class=\"string\">'_'</span>)[<span class=\"number\">0</span>]</span><br><span class=\"line\">        self.create_dir(pics_title)</span><br><span class=\"line\">        os.chdir(pics_title)</span><br><span class=\"line\">        sum_page = self.find_total_page_num(browser)</span><br><span class=\"line\">        i = <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> i &lt; sum_page:</span><br><span class=\"line\">            image_name = str(i) + <span class=\"string\">'.png'</span></span><br><span class=\"line\">            browser.get_screenshot_as_file(image_name)  </span><br><span class=\"line\">            <span class=\"comment\"># 使用PhantomJS避免了截图的不完整，可以与Chrome比较</span></span><br><span class=\"line\">            self.log.info(<span class=\"string\">'saving image %s'</span> % image_name)</span><br><span class=\"line\">            i += <span class=\"number\">1</span></span><br><span class=\"line\">            css_selector = <span class=\"string\">\"a[href='/comiclist/3/3/%s.htm']\"</span> % i  </span><br><span class=\"line\">            <span class=\"comment\"># 该方法感觉还不错呢，不过这个网站确实挺差劲的</span></span><br><span class=\"line\">            next_page = browser.find_element_by_css_selector(css_selector)</span><br><span class=\"line\">            next_page.click()</span><br><span class=\"line\">            time.sleep(<span class=\"number\">2</span>)</span><br><span class=\"line\">            <span class=\"comment\"># browser.implicitly_wait(20)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">find_total_page_num</span><span class=\"params\">(self, browser)</span>:</span></span><br><span class=\"line\">        page_element = browser.find_element_by_css_selector(<span class=\"string\">\"table[cellspacing='1']\"</span>)</span><br><span class=\"line\">        num = re.search(<span class=\"string\">r'共\\d+页'</span>, page_element.text).group()[<span class=\"number\">1</span>:<span class=\"number\">-1</span>]  </span><br><span class=\"line\">        <span class=\"keyword\">return</span> int(num)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">create_dir</span><span class=\"params\">(self, dir_name)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> os.path.exists(dir_name):</span><br><span class=\"line\">            self.log.error(<span class=\"string\">'create directory %s failed cause a same directory exists'</span> % dir_name)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">try</span>:</span><br><span class=\"line\">                os.makedirs(dir_name)</span><br><span class=\"line\">            <span class=\"keyword\">except</span>:</span><br><span class=\"line\">                self.log.error(<span class=\"string\">'create directory %s failed'</span> % dir_name)</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                self.log.info(<span class=\"string\">'create directory %s success'</span> % dir_name)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    start_url = <span class=\"string\">'http://comic.kukudm.com/comiclist/3/3/1.htm'</span></span><br><span class=\"line\">    DL = DownloadPics(start_url)</span><br></pre></td></tr></table></figure>\n<h2 id=\"运行结果\"><a href=\"#运行结果\" class=\"headerlink\" title=\"运行结果\"></a>运行结果</h2><p><img src=\"http://upload-images.jianshu.io/upload_images/2952111-e7e2cf39116b5fea.gif?imageMogr2/auto-orient/strip\" alt=\"gif\"></p>\n","site":{"data":{}},"excerpt":"<p>近期学习爬虫，发现懂的越多，不懂的知识点越多（所以当个傻子还是很幸福的）。好记性不如烂笔头，之前都是把看到的资料链接直接挂到一些平台，比如知乎、简书、Github等。今天有点时间，就好好码一下字，排排版，方便以后查阅。</p>\n<p>Selenium用来模拟浏览器的行为，比如点击、最大化、滚动窗口等；PhantomJS是一种浏览器，不过这种浏览器没有UI界面，感觉就像是专门为爬虫设计，优点很明显，可以有效减小内存的使用。</p>","more":"<h2 id=\"爬虫使用到的模块\"><a href=\"#爬虫使用到的模块\" class=\"headerlink\" title=\"爬虫使用到的模块\"></a>爬虫使用到的模块</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> selenium <span class=\"keyword\">import</span> webdriver</span><br><span class=\"line\"><span class=\"keyword\">from</span> myLogging <span class=\"keyword\">import</span> MyLogging</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> time</span><br><span class=\"line\"><span class=\"keyword\">import</span> re</span><br></pre></td></tr></table></figure>\n<p>myLogging模块是自己配置的日志包，想要的可以点击<strong><em><a href=\"http://link.zhihu.com/?target=https%3A//github.com/ByiProX/DownloadPicsBySeleniumAndPhantomJS\" target=\"_blank\" rel=\"noopener\">这里</a></em></strong>自己看</p>\n<p>爬虫很关键的一点就是能够看懂网页的源代码，记得当初刚刚真正开始接触编程的时候，有很长的一段时间在看HTML、CSS、JS的一些知识，虽然忘得很多，但是印象还是有的，对于后面看网页源代码很有帮助。学习爬虫，除了会基本的python知识以外，还要会网页的一些知识。</p>\n<h2 id=\"爬取图片思路：\"><a href=\"#爬取图片思路：\" class=\"headerlink\" title=\"爬取图片思路：\"></a>爬取图片思路：</h2><ul>\n<li>已知连接，分析网页的代码结构，看所需的数据是否需要切换frame，并定位所需数据的位于哪个标签之下</li>\n<li>采用不同的模块有不同的保存图片方式，如果采用request模块，保存图片方式是可以采用<strong>requests.get(comicUrl).content</strong>方法，使用该方法需要确定网页的地址。该项目中没有涉及request的使用，所以此后不再表述。对于selenium可以使用 <strong>get_screenshot_as_file()</strong> 方法，使用该方法强烈建议使用phantomjs，如果使用chrome浏览器，图片尺寸太大的话，会出现截图不完整，对比如下：<br><img src=\"http://upload-images.jianshu.io/upload_images/2952111-2323c462a546dcc3.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"对比\"></li>\n<li>找到下一张图片的连接位置并点击更新网页，一般来讲新网页与之前网页结构相同。</li>\n<li>在新网页的基础上保存图片，设置循环如此反复。</li>\n</ul>\n<p>爬取网页的URL为：<a href=\"http://link.zhihu.com/?target=http%3A//comic.kukudm.com/comiclist/3/3/1.htm\" target=\"_blank\" rel=\"noopener\">爬取火影漫画第一话</a></p>\n<h2 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DownloadPics</span><span class=\"params\">(object)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, url)</span>:</span></span><br><span class=\"line\">        self.url = url</span><br><span class=\"line\">        self.log = MyLogging()</span><br><span class=\"line\">        self.browser = self.get_browser()</span><br><span class=\"line\">        self.save_pics(self.browser)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_browser</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        browser = webdriver.PhantomJS()</span><br><span class=\"line\">        <span class=\"keyword\">try</span>:</span><br><span class=\"line\">            browser.get(self.url)</span><br><span class=\"line\">        <span class=\"keyword\">except</span>:</span><br><span class=\"line\">            MyLogging.error(<span class=\"string\">'open the url %s failed'</span> % self.url)</span><br><span class=\"line\">        browser.implicitly_wait(<span class=\"number\">20</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> browser</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">save_pics</span><span class=\"params\">(self, browser)</span>:</span></span><br><span class=\"line\">        pics_title = browser.title.split(<span class=\"string\">'_'</span>)[<span class=\"number\">0</span>]</span><br><span class=\"line\">        self.create_dir(pics_title)</span><br><span class=\"line\">        os.chdir(pics_title)</span><br><span class=\"line\">        sum_page = self.find_total_page_num(browser)</span><br><span class=\"line\">        i = <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> i &lt; sum_page:</span><br><span class=\"line\">            image_name = str(i) + <span class=\"string\">'.png'</span></span><br><span class=\"line\">            browser.get_screenshot_as_file(image_name)  </span><br><span class=\"line\">            <span class=\"comment\"># 使用PhantomJS避免了截图的不完整，可以与Chrome比较</span></span><br><span class=\"line\">            self.log.info(<span class=\"string\">'saving image %s'</span> % image_name)</span><br><span class=\"line\">            i += <span class=\"number\">1</span></span><br><span class=\"line\">            css_selector = <span class=\"string\">\"a[href='/comiclist/3/3/%s.htm']\"</span> % i  </span><br><span class=\"line\">            <span class=\"comment\"># 该方法感觉还不错呢，不过这个网站确实挺差劲的</span></span><br><span class=\"line\">            next_page = browser.find_element_by_css_selector(css_selector)</span><br><span class=\"line\">            next_page.click()</span><br><span class=\"line\">            time.sleep(<span class=\"number\">2</span>)</span><br><span class=\"line\">            <span class=\"comment\"># browser.implicitly_wait(20)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">find_total_page_num</span><span class=\"params\">(self, browser)</span>:</span></span><br><span class=\"line\">        page_element = browser.find_element_by_css_selector(<span class=\"string\">\"table[cellspacing='1']\"</span>)</span><br><span class=\"line\">        num = re.search(<span class=\"string\">r'共\\d+页'</span>, page_element.text).group()[<span class=\"number\">1</span>:<span class=\"number\">-1</span>]  </span><br><span class=\"line\">        <span class=\"keyword\">return</span> int(num)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">create_dir</span><span class=\"params\">(self, dir_name)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> os.path.exists(dir_name):</span><br><span class=\"line\">            self.log.error(<span class=\"string\">'create directory %s failed cause a same directory exists'</span> % dir_name)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">try</span>:</span><br><span class=\"line\">                os.makedirs(dir_name)</span><br><span class=\"line\">            <span class=\"keyword\">except</span>:</span><br><span class=\"line\">                self.log.error(<span class=\"string\">'create directory %s failed'</span> % dir_name)</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                self.log.info(<span class=\"string\">'create directory %s success'</span> % dir_name)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    start_url = <span class=\"string\">'http://comic.kukudm.com/comiclist/3/3/1.htm'</span></span><br><span class=\"line\">    DL = DownloadPics(start_url)</span><br></pre></td></tr></table></figure>\n<h2 id=\"运行结果\"><a href=\"#运行结果\" class=\"headerlink\" title=\"运行结果\"></a>运行结果</h2><p><img src=\"http://upload-images.jianshu.io/upload_images/2952111-e7e2cf39116b5fea.gif?imageMogr2/auto-orient/strip\" alt=\"gif\"></p>"},{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n非常好的一篇markdown参考手册\n\n<!--more-->\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n非常好的一篇markdown参考手册\n\n<!--more-->\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","slug":"hello-world","published":1,"date":"2018-02-25T18:47:57.250Z","updated":"2018-02-27T06:43:43.774Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cje6mdfz8000grcot4aaud9n2","content":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</p>\n<p>非常好的一篇markdown参考手册</p>\n<a id=\"more\"></a>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>\n","site":{"data":{}},"excerpt":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</p>\n<p>非常好的一篇markdown参考手册</p>","more":"<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>"},{"title":"一种Git保留两个repo的commit信息进行合并的方法","date":"2018-02-27T06:46:37.000Z","_content":"\n以往的合并时首先要删除repo的.git文件夹，然后重新add-commit-push。带来的问题是会丢失某一个仓库的提交信息，不利于时光倒退。经过摸索终于实现了保留两个仓库提交信息的合并方法。介绍如下：\n\n<!--more-->\n\n比如要将DownloadPicsBySeleniumAndPhantomJS这个项目合并到Web-Spider中，终端中执行：\n```Bash\n$ git remote add other git@github.com:ByiProX/****.git\n$ git fetch other\n$ git checkout -b repo1 other/mster\n$ git checkout master\n$ git merge repo1 --allow-unrelated-histories\n```\n\n![](http://img.blog.csdn.net/20180213030000117)\n\n接下来解决merge冲突即可（可以尝试使用mergetool），如有需要可以删除多余分支和远程连接\n\n\n```Bash\n$ git mergetool # 解决merge冲突  \n```\n```bash\n\n$ git remote rm other # 删除远程连接  \n$ git branch -d repo1 # 删除分支操作  \n```\n","source":"_posts/一种Git保留两个repo的commit信息进行合并的方法.md","raw":"---\ntitle: 一种Git保留两个repo的commit信息进行合并的方法\ndate: 2018-02-27 14:46:37\ntags:\n  - Git\ncategories:\n  - Git\n\n---\n\n以往的合并时首先要删除repo的.git文件夹，然后重新add-commit-push。带来的问题是会丢失某一个仓库的提交信息，不利于时光倒退。经过摸索终于实现了保留两个仓库提交信息的合并方法。介绍如下：\n\n<!--more-->\n\n比如要将DownloadPicsBySeleniumAndPhantomJS这个项目合并到Web-Spider中，终端中执行：\n```Bash\n$ git remote add other git@github.com:ByiProX/****.git\n$ git fetch other\n$ git checkout -b repo1 other/mster\n$ git checkout master\n$ git merge repo1 --allow-unrelated-histories\n```\n\n![](http://img.blog.csdn.net/20180213030000117)\n\n接下来解决merge冲突即可（可以尝试使用mergetool），如有需要可以删除多余分支和远程连接\n\n\n```Bash\n$ git mergetool # 解决merge冲突  \n```\n```bash\n\n$ git remote rm other # 删除远程连接  \n$ git branch -d repo1 # 删除分支操作  \n```\n","slug":"一种Git保留两个repo的commit信息进行合并的方法","published":1,"updated":"2018-02-27T07:19:42.036Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cje6mdfzk000krcotx1hu0fmm","content":"<p>以往的合并时首先要删除repo的.git文件夹，然后重新add-commit-push。带来的问题是会丢失某一个仓库的提交信息，不利于时光倒退。经过摸索终于实现了保留两个仓库提交信息的合并方法。介绍如下：</p>\n<a id=\"more\"></a>\n<p>比如要将DownloadPicsBySeleniumAndPhantomJS这个项目合并到Web-Spider中，终端中执行：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git remote add other git@github.com:ByiProX/****.git</span><br><span class=\"line\">$ git fetch other</span><br><span class=\"line\">$ git checkout -b repo1 other/mster</span><br><span class=\"line\">$ git checkout master</span><br><span class=\"line\">$ git merge repo1 --allow-unrelated-histories</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"http://img.blog.csdn.net/20180213030000117\" alt=\"\"></p>\n<p>接下来解决merge冲突即可（可以尝试使用mergetool），如有需要可以删除多余分支和远程连接</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git mergetool <span class=\"comment\"># 解决merge冲突</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">$ git remote rm other <span class=\"comment\"># 删除远程连接  </span></span><br><span class=\"line\">$ git branch -d repo1 <span class=\"comment\"># 删除分支操作</span></span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"<p>以往的合并时首先要删除repo的.git文件夹，然后重新add-commit-push。带来的问题是会丢失某一个仓库的提交信息，不利于时光倒退。经过摸索终于实现了保留两个仓库提交信息的合并方法。介绍如下：</p>","more":"<p>比如要将DownloadPicsBySeleniumAndPhantomJS这个项目合并到Web-Spider中，终端中执行：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git remote add other git@github.com:ByiProX/****.git</span><br><span class=\"line\">$ git fetch other</span><br><span class=\"line\">$ git checkout -b repo1 other/mster</span><br><span class=\"line\">$ git checkout master</span><br><span class=\"line\">$ git merge repo1 --allow-unrelated-histories</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"http://img.blog.csdn.net/20180213030000117\" alt=\"\"></p>\n<p>接下来解决merge冲突即可（可以尝试使用mergetool），如有需要可以删除多余分支和远程连接</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git mergetool <span class=\"comment\"># 解决merge冲突</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">$ git remote rm other <span class=\"comment\"># 删除远程连接  </span></span><br><span class=\"line\">$ git branch -d repo1 <span class=\"comment\"># 删除分支操作</span></span><br></pre></td></tr></table></figure>"},{"title":"从零开始学爬虫-01","date":"2018-02-27T16:37:37.000Z","_content":"\n### 本节关键字\n*urllib | chardet*\n\n### urllib 简介\n在Python3.x中，我们可以使用urlib这个组件抓取网页，urllib是一个URL处理包，这个包中集合了一些处理URL的模块，如下：\n<!-- more -->\n\n>1.urllib.request模块是用来打开和读取URLs的；\n\n>2.urllib.error模块包含一些有urllib.request产生的错误，可以使用try进行捕捉处理；\n\n>3.urllib.parse模块包含了一些解析URLs的方法；\n\n>4.urllib.robotparser模块用来解析robots.txt文本文件.它提供了一个单独的RobotFileParser类，通过该类提供的can_fetch()方法测试爬虫是否可以下载一个页面。\n\n\n使用urllib.request.urlopen()这个接口函数就可以访问一个网站，读取并打印信息。\nurlopen有一些可选参数，具体信息可以查阅Python自带的documentation。\n\n### urllib 测试\n了解到这些，我们就可以写一个最简单的程序，文件名为urllib_test01.py，感受一个urllib库的魅力：\n```python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\n\nif __name__ == \"__main__\":\n    response = request.urlopen(\"http://fanyi.baidu.com\")\n    html = response.read()\n    print(html)\n```\n\nurllib使用使用request.urlopen()访问和读取URLs信息，返回的对象response如同一个文本对象，我们可以调用read()，进行读取。再通过print()屏幕打印。\n\n浏览器就是作为客户端从服务器端获取信息，然后将信息解析，再展示给我们的。但是显然他们都是二进制的乱码。\n\n我们可以通过简单的decode()命令将网页的信息进行解码，并显示出来，我们新创建一个文件，命名为urllib_test02.py，编写如下代码：\n\n```python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\n\nif __name__ == \"__main__\":\n    response = request.urlopen(\"http://fanyi.baidu.com/\")\n    html = response.read()\n    html = html.decode(\"utf-8\")\n    print(html)\n```\n\n这样我们就可以得到这样的结果，显然解码后的信息看起来工整和舒服多了：\n\n\n当然这个前提是我们已经知道了这个网页是使用utf-8编码的，怎么查看网页的编码方式呢？非常简单的方法是使用使用浏览器审查元素，只需要找到head标签开始位置的chareset，就知道网页是采用何种编码。\n\n这样我们就知道了这个网站的编码方式，但是这需要我们每次都打开浏览器，并找下编码方式，显然有些费事，使用几行代码解决更加省事并且显得酷一些。\n\n我们需要安装第三方库chardet，它是用来判断编码的模块。安装好后，我们就可以使用chardet.detect()方法，判断网页的编码方式了。至此，我们就可以编写一个小程序判断网页的编码方式了，新建文件名为chardet_test01.py：\n```Python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\nimport chardet\n\nif __name__ == \"__main__\":\n    response = request.urlopen(\"http://fanyi.baidu.com\")\n    html = response.read()\n    charset = chardet.detect(html)\n    print(charset)\n```\n","source":"_posts/从零开始学爬虫-01.md","raw":"---\ntitle: 从零开始学爬虫-01\ndate: 2018-02-28 00:37:37\ntags:\n  - Spider\n  - Urllib\n  - Python3\ncategories:\n  - Spider\n  - Urllib\n---\n\n### 本节关键字\n*urllib | chardet*\n\n### urllib 简介\n在Python3.x中，我们可以使用urlib这个组件抓取网页，urllib是一个URL处理包，这个包中集合了一些处理URL的模块，如下：\n<!-- more -->\n\n>1.urllib.request模块是用来打开和读取URLs的；\n\n>2.urllib.error模块包含一些有urllib.request产生的错误，可以使用try进行捕捉处理；\n\n>3.urllib.parse模块包含了一些解析URLs的方法；\n\n>4.urllib.robotparser模块用来解析robots.txt文本文件.它提供了一个单独的RobotFileParser类，通过该类提供的can_fetch()方法测试爬虫是否可以下载一个页面。\n\n\n使用urllib.request.urlopen()这个接口函数就可以访问一个网站，读取并打印信息。\nurlopen有一些可选参数，具体信息可以查阅Python自带的documentation。\n\n### urllib 测试\n了解到这些，我们就可以写一个最简单的程序，文件名为urllib_test01.py，感受一个urllib库的魅力：\n```python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\n\nif __name__ == \"__main__\":\n    response = request.urlopen(\"http://fanyi.baidu.com\")\n    html = response.read()\n    print(html)\n```\n\nurllib使用使用request.urlopen()访问和读取URLs信息，返回的对象response如同一个文本对象，我们可以调用read()，进行读取。再通过print()屏幕打印。\n\n浏览器就是作为客户端从服务器端获取信息，然后将信息解析，再展示给我们的。但是显然他们都是二进制的乱码。\n\n我们可以通过简单的decode()命令将网页的信息进行解码，并显示出来，我们新创建一个文件，命名为urllib_test02.py，编写如下代码：\n\n```python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\n\nif __name__ == \"__main__\":\n    response = request.urlopen(\"http://fanyi.baidu.com/\")\n    html = response.read()\n    html = html.decode(\"utf-8\")\n    print(html)\n```\n\n这样我们就可以得到这样的结果，显然解码后的信息看起来工整和舒服多了：\n\n\n当然这个前提是我们已经知道了这个网页是使用utf-8编码的，怎么查看网页的编码方式呢？非常简单的方法是使用使用浏览器审查元素，只需要找到head标签开始位置的chareset，就知道网页是采用何种编码。\n\n这样我们就知道了这个网站的编码方式，但是这需要我们每次都打开浏览器，并找下编码方式，显然有些费事，使用几行代码解决更加省事并且显得酷一些。\n\n我们需要安装第三方库chardet，它是用来判断编码的模块。安装好后，我们就可以使用chardet.detect()方法，判断网页的编码方式了。至此，我们就可以编写一个小程序判断网页的编码方式了，新建文件名为chardet_test01.py：\n```Python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\nimport chardet\n\nif __name__ == \"__main__\":\n    response = request.urlopen(\"http://fanyi.baidu.com\")\n    html = response.read()\n    charset = chardet.detect(html)\n    print(charset)\n```\n","slug":"从零开始学爬虫-01","published":1,"updated":"2018-02-27T17:04:40.535Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cje6mdfzv000mrcotn6z6n4vo","content":"<h3 id=\"本节关键字\"><a href=\"#本节关键字\" class=\"headerlink\" title=\"本节关键字\"></a>本节关键字</h3><p><em>urllib | chardet</em></p>\n<h3 id=\"urllib-简介\"><a href=\"#urllib-简介\" class=\"headerlink\" title=\"urllib 简介\"></a>urllib 简介</h3><p>在Python3.x中，我们可以使用urlib这个组件抓取网页，urllib是一个URL处理包，这个包中集合了一些处理URL的模块，如下：<br><a id=\"more\"></a></p>\n<blockquote>\n<p>1.urllib.request模块是用来打开和读取URLs的；</p>\n</blockquote>\n<blockquote>\n<p>2.urllib.error模块包含一些有urllib.request产生的错误，可以使用try进行捕捉处理；</p>\n</blockquote>\n<blockquote>\n<p>3.urllib.parse模块包含了一些解析URLs的方法；</p>\n</blockquote>\n<blockquote>\n<p>4.urllib.robotparser模块用来解析robots.txt文本文件.它提供了一个单独的RobotFileParser类，通过该类提供的can_fetch()方法测试爬虫是否可以下载一个页面。</p>\n</blockquote>\n<p>使用urllib.request.urlopen()这个接口函数就可以访问一个网站，读取并打印信息。<br>urlopen有一些可选参数，具体信息可以查阅Python自带的documentation。</p>\n<h3 id=\"urllib-测试\"><a href=\"#urllib-测试\" class=\"headerlink\" title=\"urllib 测试\"></a>urllib 测试</h3><p>了解到这些，我们就可以写一个最简单的程序，文件名为urllib_test01.py，感受一个urllib库的魅力：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    response = request.urlopen(<span class=\"string\">\"http://fanyi.baidu.com\"</span>)</span><br><span class=\"line\">    html = response.read()</span><br><span class=\"line\">    print(html)</span><br></pre></td></tr></table></figure></p>\n<p>urllib使用使用request.urlopen()访问和读取URLs信息，返回的对象response如同一个文本对象，我们可以调用read()，进行读取。再通过print()屏幕打印。</p>\n<p>浏览器就是作为客户端从服务器端获取信息，然后将信息解析，再展示给我们的。但是显然他们都是二进制的乱码。</p>\n<p>我们可以通过简单的decode()命令将网页的信息进行解码，并显示出来，我们新创建一个文件，命名为urllib_test02.py，编写如下代码：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    response = request.urlopen(<span class=\"string\">\"http://fanyi.baidu.com/\"</span>)</span><br><span class=\"line\">    html = response.read()</span><br><span class=\"line\">    html = html.decode(<span class=\"string\">\"utf-8\"</span>)</span><br><span class=\"line\">    print(html)</span><br></pre></td></tr></table></figure>\n<p>这样我们就可以得到这样的结果，显然解码后的信息看起来工整和舒服多了：</p>\n<p>当然这个前提是我们已经知道了这个网页是使用utf-8编码的，怎么查看网页的编码方式呢？非常简单的方法是使用使用浏览器审查元素，只需要找到head标签开始位置的chareset，就知道网页是采用何种编码。</p>\n<p>这样我们就知道了这个网站的编码方式，但是这需要我们每次都打开浏览器，并找下编码方式，显然有些费事，使用几行代码解决更加省事并且显得酷一些。</p>\n<p>我们需要安装第三方库chardet，它是用来判断编码的模块。安装好后，我们就可以使用chardet.detect()方法，判断网页的编码方式了。至此，我们就可以编写一个小程序判断网页的编码方式了，新建文件名为chardet_test01.py：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"><span class=\"keyword\">import</span> chardet</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    response = request.urlopen(<span class=\"string\">\"http://fanyi.baidu.com\"</span>)</span><br><span class=\"line\">    html = response.read()</span><br><span class=\"line\">    charset = chardet.detect(html)</span><br><span class=\"line\">    print(charset)</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"<h3 id=\"本节关键字\"><a href=\"#本节关键字\" class=\"headerlink\" title=\"本节关键字\"></a>本节关键字</h3><p><em>urllib | chardet</em></p>\n<h3 id=\"urllib-简介\"><a href=\"#urllib-简介\" class=\"headerlink\" title=\"urllib 简介\"></a>urllib 简介</h3><p>在Python3.x中，我们可以使用urlib这个组件抓取网页，urllib是一个URL处理包，这个包中集合了一些处理URL的模块，如下：<br>","more":"</p>\n<blockquote>\n<p>1.urllib.request模块是用来打开和读取URLs的；</p>\n</blockquote>\n<blockquote>\n<p>2.urllib.error模块包含一些有urllib.request产生的错误，可以使用try进行捕捉处理；</p>\n</blockquote>\n<blockquote>\n<p>3.urllib.parse模块包含了一些解析URLs的方法；</p>\n</blockquote>\n<blockquote>\n<p>4.urllib.robotparser模块用来解析robots.txt文本文件.它提供了一个单独的RobotFileParser类，通过该类提供的can_fetch()方法测试爬虫是否可以下载一个页面。</p>\n</blockquote>\n<p>使用urllib.request.urlopen()这个接口函数就可以访问一个网站，读取并打印信息。<br>urlopen有一些可选参数，具体信息可以查阅Python自带的documentation。</p>\n<h3 id=\"urllib-测试\"><a href=\"#urllib-测试\" class=\"headerlink\" title=\"urllib 测试\"></a>urllib 测试</h3><p>了解到这些，我们就可以写一个最简单的程序，文件名为urllib_test01.py，感受一个urllib库的魅力：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    response = request.urlopen(<span class=\"string\">\"http://fanyi.baidu.com\"</span>)</span><br><span class=\"line\">    html = response.read()</span><br><span class=\"line\">    print(html)</span><br></pre></td></tr></table></figure></p>\n<p>urllib使用使用request.urlopen()访问和读取URLs信息，返回的对象response如同一个文本对象，我们可以调用read()，进行读取。再通过print()屏幕打印。</p>\n<p>浏览器就是作为客户端从服务器端获取信息，然后将信息解析，再展示给我们的。但是显然他们都是二进制的乱码。</p>\n<p>我们可以通过简单的decode()命令将网页的信息进行解码，并显示出来，我们新创建一个文件，命名为urllib_test02.py，编写如下代码：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    response = request.urlopen(<span class=\"string\">\"http://fanyi.baidu.com/\"</span>)</span><br><span class=\"line\">    html = response.read()</span><br><span class=\"line\">    html = html.decode(<span class=\"string\">\"utf-8\"</span>)</span><br><span class=\"line\">    print(html)</span><br></pre></td></tr></table></figure>\n<p>这样我们就可以得到这样的结果，显然解码后的信息看起来工整和舒服多了：</p>\n<p>当然这个前提是我们已经知道了这个网页是使用utf-8编码的，怎么查看网页的编码方式呢？非常简单的方法是使用使用浏览器审查元素，只需要找到head标签开始位置的chareset，就知道网页是采用何种编码。</p>\n<p>这样我们就知道了这个网站的编码方式，但是这需要我们每次都打开浏览器，并找下编码方式，显然有些费事，使用几行代码解决更加省事并且显得酷一些。</p>\n<p>我们需要安装第三方库chardet，它是用来判断编码的模块。安装好后，我们就可以使用chardet.detect()方法，判断网页的编码方式了。至此，我们就可以编写一个小程序判断网页的编码方式了，新建文件名为chardet_test01.py：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"><span class=\"keyword\">import</span> chardet</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    response = request.urlopen(<span class=\"string\">\"http://fanyi.baidu.com\"</span>)</span><br><span class=\"line\">    html = response.read()</span><br><span class=\"line\">    charset = chardet.detect(html)</span><br><span class=\"line\">    print(charset)</span><br></pre></td></tr></table></figure></p>"},{"title":"从零开始学爬虫-02","date":"2018-02-27T16:48:24.000Z","_content":"\n### 一个疑问尚未解决疑问，小弟在此跪求大牛解答一下\n为什么把url里的 \"_o\" 删掉后就可以正常爬取呢？\n<!-- more -->\n### urlopen的url参数 Agent\n\n\nurl不仅可以是一个字符串，例如:http://www.baidu.com。\n\nurl也可以是一个Request对象，这就需要我们先定义一个Request对象，然后将这个Request对象作为urlopen的参数使用，方法如下：\n\n```python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\n\nif __name__ == \"__main__\":\n    req = request.Request(\"http://fanyi.baidu.com/\")\n    response = request.urlopen(req)\n    html = response.read()\n    html = html.decode(\"utf-8\")\n    print(html)\n```\n\n同样，运行这段代码同样可以得到网页信息。可以看一下这段代码和上个笔记中代码的不同，对比一下就明白了。\n\nurlopen()返回的对象，可以使用read()进行读取，同样也可以使用geturl()方法、info()方法、getcode()方法。\n\n\n\n- geturl()返回的是一个url的字符串；\n\n- info()返回的是一些meta标记的元信息，包括一些服务器的信息；\n\n- getcode()返回的是HTTP的状态码，如果返回200表示请求成功。\n\n关于META标签和HTTP状态码的内容可以自行百度百科，里面有很详细的介绍。\n\n\n了解到这些，我们就可以进行新一轮的测试，新建文件名urllib_test04.py，编写如下代码：\n```python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\n\nif __name__ == \"__main__\":\n    req = request.Request(\"http://fanyi.baidu.com/\")\n    response = request.urlopen(req)\n    print(\"geturl打印信息：%s\"%(response.geturl()))\n    print('**********************************************')\n    print(\"info打印信息：%s\"%(response.info()))\n    print('**********************************************')\n    print(\"getcode打印信息：%s\"%(response.getcode()))\n\n```\n\n\n### urlopen的data参数\n\n我们可以使用data参数，向服务器发送数据。根据HTTP规范，GET用于信息获取，POST是向服务器提交数据的一种请求，再换句话说：\n\n从客户端向服务器提交数据使用POST；\n\n从服务器获得数据到客户端使用GET(GET也可以提交，暂不考虑)。\n\n如果没有设置urlopen()函数的data参数，HTTP请求采用GET方式，也就是我们从服务器获取信息，如果我们设置data参数，HTTP请求采用POST方式，也就是我们向服务器传递数据。\n\ndata参数有自己的格式，它是一个基于application/x-www.form-urlencoded的格式，具体格式我们不用了解， 因为我们可以使用urllib.parse.urlencode()函数将字符串自动转换成上面所说的格式。\n\n### 发送data实例\n\n向有道翻译发送data，得到翻译结果。\n####  (1).打开有道翻译界面，如下图所示：\n\n ![image](http://upload-images.jianshu.io/upload_images/2952111-31f629ec53534a43?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n####  (2).鼠标右键检查，也就是审查元素，如下图所示：\n\n![image](http://upload-images.jianshu.io/upload_images/2952111-f199c9cbcd80b40f?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n####  (3).选择右侧出现的Network，如下图所示：\n\n ![image](http://upload-images.jianshu.io/upload_images/2952111-4354c17b0169d4b9?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n####  (4).在左侧输入翻译内容，输入Jack，如下图所示：\n\n ![image](http://upload-images.jianshu.io/upload_images/2952111-1a5e4f785e7bbccf?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n####  (5).点击自动翻译按钮，我们就可以看到右侧出现的内容，如下图所示：\n![image](http://upload-images.jianshu.io/upload_images/2952111-37b47520ec88de2e?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n\n####  (6).点击上图红框中的内容，查看它的信息，如下图所示：\n\n ![image](http://upload-images.jianshu.io/upload_images/2952111-9b692ceecf538fd5?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n![image](http://upload-images.jianshu.io/upload_images/2952111-1dc2354ec47dc6ff?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n####  (7).记住这些信息，这是我们一会儿写程序需要用到的。\n\n  新建文件translate_test.py，编写如下代码：\n ```python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\nfrom urllib import parse\nimport json\n\nif __name__ == \"__main__\":\n    #对应上图的Request URL\n    Request_URL = 'http://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule&sessionFrom=null'\n    #创建Form_Data字典，存储上图的Form Data\n    Form_Data = {}\n    Form_Data['type'] = 'AUTO'\n    Form_Data['i'] = 'Jack'\n    Form_Data['doctype'] = 'json'\n    Form_Data['xmlVersion'] = '1.8'\n    Form_Data['keyfrom'] = 'fanyi.web'\n    Form_Data['ue'] = 'ue:UTF-8'\n    Form_Data['action'] = 'FY_BY_CLICKBUTTON'\n    #使用urlencode方法转换标准格式\n    data = parse.urlencode(Form_Data).encode('utf-8')\n    #传递Request对象和转换完格式的数据\n    response = request.urlopen(Request_URL,data)\n    #读取信息并解码\n    html = response.read().decode('utf-8')\n    #使用JSON\n    translate_results = json.loads(html)\n    #找到翻译结果\n    translate_results = translate_results['translateResult'][0][0]['tgt']\n    #打印翻译信息\n    print(\"翻译的结果是：%s\" % translate_results)\n```\n\n运行查看翻译结果\n","source":"_posts/从零开始学爬虫-02.md","raw":"---\ntitle: 从零开始学爬虫-02\ndate: 2018-02-28 00:48:24\ntags:\n  - Spider\n  - Urllib\n  - Python3\ncategories:\n  - Spider\n  - Urllib\n---\n\n### 一个疑问尚未解决疑问，小弟在此跪求大牛解答一下\n为什么把url里的 \"_o\" 删掉后就可以正常爬取呢？\n<!-- more -->\n### urlopen的url参数 Agent\n\n\nurl不仅可以是一个字符串，例如:http://www.baidu.com。\n\nurl也可以是一个Request对象，这就需要我们先定义一个Request对象，然后将这个Request对象作为urlopen的参数使用，方法如下：\n\n```python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\n\nif __name__ == \"__main__\":\n    req = request.Request(\"http://fanyi.baidu.com/\")\n    response = request.urlopen(req)\n    html = response.read()\n    html = html.decode(\"utf-8\")\n    print(html)\n```\n\n同样，运行这段代码同样可以得到网页信息。可以看一下这段代码和上个笔记中代码的不同，对比一下就明白了。\n\nurlopen()返回的对象，可以使用read()进行读取，同样也可以使用geturl()方法、info()方法、getcode()方法。\n\n\n\n- geturl()返回的是一个url的字符串；\n\n- info()返回的是一些meta标记的元信息，包括一些服务器的信息；\n\n- getcode()返回的是HTTP的状态码，如果返回200表示请求成功。\n\n关于META标签和HTTP状态码的内容可以自行百度百科，里面有很详细的介绍。\n\n\n了解到这些，我们就可以进行新一轮的测试，新建文件名urllib_test04.py，编写如下代码：\n```python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\n\nif __name__ == \"__main__\":\n    req = request.Request(\"http://fanyi.baidu.com/\")\n    response = request.urlopen(req)\n    print(\"geturl打印信息：%s\"%(response.geturl()))\n    print('**********************************************')\n    print(\"info打印信息：%s\"%(response.info()))\n    print('**********************************************')\n    print(\"getcode打印信息：%s\"%(response.getcode()))\n\n```\n\n\n### urlopen的data参数\n\n我们可以使用data参数，向服务器发送数据。根据HTTP规范，GET用于信息获取，POST是向服务器提交数据的一种请求，再换句话说：\n\n从客户端向服务器提交数据使用POST；\n\n从服务器获得数据到客户端使用GET(GET也可以提交，暂不考虑)。\n\n如果没有设置urlopen()函数的data参数，HTTP请求采用GET方式，也就是我们从服务器获取信息，如果我们设置data参数，HTTP请求采用POST方式，也就是我们向服务器传递数据。\n\ndata参数有自己的格式，它是一个基于application/x-www.form-urlencoded的格式，具体格式我们不用了解， 因为我们可以使用urllib.parse.urlencode()函数将字符串自动转换成上面所说的格式。\n\n### 发送data实例\n\n向有道翻译发送data，得到翻译结果。\n####  (1).打开有道翻译界面，如下图所示：\n\n ![image](http://upload-images.jianshu.io/upload_images/2952111-31f629ec53534a43?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n####  (2).鼠标右键检查，也就是审查元素，如下图所示：\n\n![image](http://upload-images.jianshu.io/upload_images/2952111-f199c9cbcd80b40f?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n####  (3).选择右侧出现的Network，如下图所示：\n\n ![image](http://upload-images.jianshu.io/upload_images/2952111-4354c17b0169d4b9?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n####  (4).在左侧输入翻译内容，输入Jack，如下图所示：\n\n ![image](http://upload-images.jianshu.io/upload_images/2952111-1a5e4f785e7bbccf?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n####  (5).点击自动翻译按钮，我们就可以看到右侧出现的内容，如下图所示：\n![image](http://upload-images.jianshu.io/upload_images/2952111-37b47520ec88de2e?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n\n####  (6).点击上图红框中的内容，查看它的信息，如下图所示：\n\n ![image](http://upload-images.jianshu.io/upload_images/2952111-9b692ceecf538fd5?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n![image](http://upload-images.jianshu.io/upload_images/2952111-1dc2354ec47dc6ff?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n####  (7).记住这些信息，这是我们一会儿写程序需要用到的。\n\n  新建文件translate_test.py，编写如下代码：\n ```python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\nfrom urllib import parse\nimport json\n\nif __name__ == \"__main__\":\n    #对应上图的Request URL\n    Request_URL = 'http://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule&sessionFrom=null'\n    #创建Form_Data字典，存储上图的Form Data\n    Form_Data = {}\n    Form_Data['type'] = 'AUTO'\n    Form_Data['i'] = 'Jack'\n    Form_Data['doctype'] = 'json'\n    Form_Data['xmlVersion'] = '1.8'\n    Form_Data['keyfrom'] = 'fanyi.web'\n    Form_Data['ue'] = 'ue:UTF-8'\n    Form_Data['action'] = 'FY_BY_CLICKBUTTON'\n    #使用urlencode方法转换标准格式\n    data = parse.urlencode(Form_Data).encode('utf-8')\n    #传递Request对象和转换完格式的数据\n    response = request.urlopen(Request_URL,data)\n    #读取信息并解码\n    html = response.read().decode('utf-8')\n    #使用JSON\n    translate_results = json.loads(html)\n    #找到翻译结果\n    translate_results = translate_results['translateResult'][0][0]['tgt']\n    #打印翻译信息\n    print(\"翻译的结果是：%s\" % translate_results)\n```\n\n运行查看翻译结果\n","slug":"从零开始学爬虫-02","published":1,"updated":"2018-02-27T17:04:42.158Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cje6mdg02000orcotrxst3tvt","content":"<h3 id=\"一个疑问尚未解决疑问，小弟在此跪求大牛解答一下\"><a href=\"#一个疑问尚未解决疑问，小弟在此跪求大牛解答一下\" class=\"headerlink\" title=\"一个疑问尚未解决疑问，小弟在此跪求大牛解答一下\"></a>一个疑问尚未解决疑问，小弟在此跪求大牛解答一下</h3><p>为什么把url里的 “_o” 删掉后就可以正常爬取呢？<br><a id=\"more\"></a></p>\n<h3 id=\"urlopen的url参数-Agent\"><a href=\"#urlopen的url参数-Agent\" class=\"headerlink\" title=\"urlopen的url参数 Agent\"></a>urlopen的url参数 Agent</h3><p>url不仅可以是一个字符串，例如:<a href=\"http://www.baidu.com。\" target=\"_blank\" rel=\"noopener\">http://www.baidu.com。</a></p>\n<p>url也可以是一个Request对象，这就需要我们先定义一个Request对象，然后将这个Request对象作为urlopen的参数使用，方法如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    req = request.Request(<span class=\"string\">\"http://fanyi.baidu.com/\"</span>)</span><br><span class=\"line\">    response = request.urlopen(req)</span><br><span class=\"line\">    html = response.read()</span><br><span class=\"line\">    html = html.decode(<span class=\"string\">\"utf-8\"</span>)</span><br><span class=\"line\">    print(html)</span><br></pre></td></tr></table></figure>\n<p>同样，运行这段代码同样可以得到网页信息。可以看一下这段代码和上个笔记中代码的不同，对比一下就明白了。</p>\n<p>urlopen()返回的对象，可以使用read()进行读取，同样也可以使用geturl()方法、info()方法、getcode()方法。</p>\n<ul>\n<li><p>geturl()返回的是一个url的字符串；</p>\n</li>\n<li><p>info()返回的是一些meta标记的元信息，包括一些服务器的信息；</p>\n</li>\n<li><p>getcode()返回的是HTTP的状态码，如果返回200表示请求成功。</p>\n</li>\n</ul>\n<p>关于META标签和HTTP状态码的内容可以自行百度百科，里面有很详细的介绍。</p>\n<p>了解到这些，我们就可以进行新一轮的测试，新建文件名urllib_test04.py，编写如下代码：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    req = request.Request(<span class=\"string\">\"http://fanyi.baidu.com/\"</span>)</span><br><span class=\"line\">    response = request.urlopen(req)</span><br><span class=\"line\">    print(<span class=\"string\">\"geturl打印信息：%s\"</span>%(response.geturl()))</span><br><span class=\"line\">    print(<span class=\"string\">'**********************************************'</span>)</span><br><span class=\"line\">    print(<span class=\"string\">\"info打印信息：%s\"</span>%(response.info()))</span><br><span class=\"line\">    print(<span class=\"string\">'**********************************************'</span>)</span><br><span class=\"line\">    print(<span class=\"string\">\"getcode打印信息：%s\"</span>%(response.getcode()))</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"urlopen的data参数\"><a href=\"#urlopen的data参数\" class=\"headerlink\" title=\"urlopen的data参数\"></a>urlopen的data参数</h3><p>我们可以使用data参数，向服务器发送数据。根据HTTP规范，GET用于信息获取，POST是向服务器提交数据的一种请求，再换句话说：</p>\n<p>从客户端向服务器提交数据使用POST；</p>\n<p>从服务器获得数据到客户端使用GET(GET也可以提交，暂不考虑)。</p>\n<p>如果没有设置urlopen()函数的data参数，HTTP请求采用GET方式，也就是我们从服务器获取信息，如果我们设置data参数，HTTP请求采用POST方式，也就是我们向服务器传递数据。</p>\n<p>data参数有自己的格式，它是一个基于application/x-<a href=\"http://www.form-urlencoded的格式，具体格式我们不用了解，\" target=\"_blank\" rel=\"noopener\">www.form-urlencoded的格式，具体格式我们不用了解，</a> 因为我们可以使用urllib.parse.urlencode()函数将字符串自动转换成上面所说的格式。</p>\n<h3 id=\"发送data实例\"><a href=\"#发送data实例\" class=\"headerlink\" title=\"发送data实例\"></a>发送data实例</h3><p>向有道翻译发送data，得到翻译结果。</p>\n<h4 id=\"1-打开有道翻译界面，如下图所示：\"><a href=\"#1-打开有道翻译界面，如下图所示：\" class=\"headerlink\" title=\"(1).打开有道翻译界面，如下图所示：\"></a>(1).打开有道翻译界面，如下图所示：</h4><p> <img src=\"http://upload-images.jianshu.io/upload_images/2952111-31f629ec53534a43?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"></p>\n<h4 id=\"2-鼠标右键检查，也就是审查元素，如下图所示：\"><a href=\"#2-鼠标右键检查，也就是审查元素，如下图所示：\" class=\"headerlink\" title=\"(2).鼠标右键检查，也就是审查元素，如下图所示：\"></a>(2).鼠标右键检查，也就是审查元素，如下图所示：</h4><p><img src=\"http://upload-images.jianshu.io/upload_images/2952111-f199c9cbcd80b40f?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"></p>\n<h4 id=\"3-选择右侧出现的Network，如下图所示：\"><a href=\"#3-选择右侧出现的Network，如下图所示：\" class=\"headerlink\" title=\"(3).选择右侧出现的Network，如下图所示：\"></a>(3).选择右侧出现的Network，如下图所示：</h4><p> <img src=\"http://upload-images.jianshu.io/upload_images/2952111-4354c17b0169d4b9?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"></p>\n<h4 id=\"4-在左侧输入翻译内容，输入Jack，如下图所示：\"><a href=\"#4-在左侧输入翻译内容，输入Jack，如下图所示：\" class=\"headerlink\" title=\"(4).在左侧输入翻译内容，输入Jack，如下图所示：\"></a>(4).在左侧输入翻译内容，输入Jack，如下图所示：</h4><p> <img src=\"http://upload-images.jianshu.io/upload_images/2952111-1a5e4f785e7bbccf?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"></p>\n<h4 id=\"5-点击自动翻译按钮，我们就可以看到右侧出现的内容，如下图所示：\"><a href=\"#5-点击自动翻译按钮，我们就可以看到右侧出现的内容，如下图所示：\" class=\"headerlink\" title=\"(5).点击自动翻译按钮，我们就可以看到右侧出现的内容，如下图所示：\"></a>(5).点击自动翻译按钮，我们就可以看到右侧出现的内容，如下图所示：</h4><p><img src=\"http://upload-images.jianshu.io/upload_images/2952111-37b47520ec88de2e?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"></p>\n<h4 id=\"6-点击上图红框中的内容，查看它的信息，如下图所示：\"><a href=\"#6-点击上图红框中的内容，查看它的信息，如下图所示：\" class=\"headerlink\" title=\"(6).点击上图红框中的内容，查看它的信息，如下图所示：\"></a>(6).点击上图红框中的内容，查看它的信息，如下图所示：</h4><p> <img src=\"http://upload-images.jianshu.io/upload_images/2952111-9b692ceecf538fd5?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"></p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/2952111-1dc2354ec47dc6ff?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"></p>\n<h4 id=\"7-记住这些信息，这是我们一会儿写程序需要用到的。\"><a href=\"#7-记住这些信息，这是我们一会儿写程序需要用到的。\" class=\"headerlink\" title=\"(7).记住这些信息，这是我们一会儿写程序需要用到的。\"></a>(7).记住这些信息，这是我们一会儿写程序需要用到的。</h4><p>  新建文件translate_test.py，编写如下代码：<br> <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> parse</span><br><span class=\"line\"><span class=\"keyword\">import</span> json</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    <span class=\"comment\">#对应上图的Request URL</span></span><br><span class=\"line\">    Request_URL = <span class=\"string\">'http://fanyi.youdao.com/translate?smartresult=dict&amp;smartresult=rule&amp;sessionFrom=null'</span></span><br><span class=\"line\">    <span class=\"comment\">#创建Form_Data字典，存储上图的Form Data</span></span><br><span class=\"line\">    Form_Data = &#123;&#125;</span><br><span class=\"line\">    Form_Data[<span class=\"string\">'type'</span>] = <span class=\"string\">'AUTO'</span></span><br><span class=\"line\">    Form_Data[<span class=\"string\">'i'</span>] = <span class=\"string\">'Jack'</span></span><br><span class=\"line\">    Form_Data[<span class=\"string\">'doctype'</span>] = <span class=\"string\">'json'</span></span><br><span class=\"line\">    Form_Data[<span class=\"string\">'xmlVersion'</span>] = <span class=\"string\">'1.8'</span></span><br><span class=\"line\">    Form_Data[<span class=\"string\">'keyfrom'</span>] = <span class=\"string\">'fanyi.web'</span></span><br><span class=\"line\">    Form_Data[<span class=\"string\">'ue'</span>] = <span class=\"string\">'ue:UTF-8'</span></span><br><span class=\"line\">    Form_Data[<span class=\"string\">'action'</span>] = <span class=\"string\">'FY_BY_CLICKBUTTON'</span></span><br><span class=\"line\">    <span class=\"comment\">#使用urlencode方法转换标准格式</span></span><br><span class=\"line\">    data = parse.urlencode(Form_Data).encode(<span class=\"string\">'utf-8'</span>)</span><br><span class=\"line\">    <span class=\"comment\">#传递Request对象和转换完格式的数据</span></span><br><span class=\"line\">    response = request.urlopen(Request_URL,data)</span><br><span class=\"line\">    <span class=\"comment\">#读取信息并解码</span></span><br><span class=\"line\">    html = response.read().decode(<span class=\"string\">'utf-8'</span>)</span><br><span class=\"line\">    <span class=\"comment\">#使用JSON</span></span><br><span class=\"line\">    translate_results = json.loads(html)</span><br><span class=\"line\">    <span class=\"comment\">#找到翻译结果</span></span><br><span class=\"line\">    translate_results = translate_results[<span class=\"string\">'translateResult'</span>][<span class=\"number\">0</span>][<span class=\"number\">0</span>][<span class=\"string\">'tgt'</span>]</span><br><span class=\"line\">    <span class=\"comment\">#打印翻译信息</span></span><br><span class=\"line\">    print(<span class=\"string\">\"翻译的结果是：%s\"</span> % translate_results)</span><br></pre></td></tr></table></figure></p>\n<p>运行查看翻译结果</p>\n","site":{"data":{}},"excerpt":"<h3 id=\"一个疑问尚未解决疑问，小弟在此跪求大牛解答一下\"><a href=\"#一个疑问尚未解决疑问，小弟在此跪求大牛解答一下\" class=\"headerlink\" title=\"一个疑问尚未解决疑问，小弟在此跪求大牛解答一下\"></a>一个疑问尚未解决疑问，小弟在此跪求大牛解答一下</h3><p>为什么把url里的 “_o” 删掉后就可以正常爬取呢？<br>","more":"</p>\n<h3 id=\"urlopen的url参数-Agent\"><a href=\"#urlopen的url参数-Agent\" class=\"headerlink\" title=\"urlopen的url参数 Agent\"></a>urlopen的url参数 Agent</h3><p>url不仅可以是一个字符串，例如:<a href=\"http://www.baidu.com。\" target=\"_blank\" rel=\"noopener\">http://www.baidu.com。</a></p>\n<p>url也可以是一个Request对象，这就需要我们先定义一个Request对象，然后将这个Request对象作为urlopen的参数使用，方法如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    req = request.Request(<span class=\"string\">\"http://fanyi.baidu.com/\"</span>)</span><br><span class=\"line\">    response = request.urlopen(req)</span><br><span class=\"line\">    html = response.read()</span><br><span class=\"line\">    html = html.decode(<span class=\"string\">\"utf-8\"</span>)</span><br><span class=\"line\">    print(html)</span><br></pre></td></tr></table></figure>\n<p>同样，运行这段代码同样可以得到网页信息。可以看一下这段代码和上个笔记中代码的不同，对比一下就明白了。</p>\n<p>urlopen()返回的对象，可以使用read()进行读取，同样也可以使用geturl()方法、info()方法、getcode()方法。</p>\n<ul>\n<li><p>geturl()返回的是一个url的字符串；</p>\n</li>\n<li><p>info()返回的是一些meta标记的元信息，包括一些服务器的信息；</p>\n</li>\n<li><p>getcode()返回的是HTTP的状态码，如果返回200表示请求成功。</p>\n</li>\n</ul>\n<p>关于META标签和HTTP状态码的内容可以自行百度百科，里面有很详细的介绍。</p>\n<p>了解到这些，我们就可以进行新一轮的测试，新建文件名urllib_test04.py，编写如下代码：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    req = request.Request(<span class=\"string\">\"http://fanyi.baidu.com/\"</span>)</span><br><span class=\"line\">    response = request.urlopen(req)</span><br><span class=\"line\">    print(<span class=\"string\">\"geturl打印信息：%s\"</span>%(response.geturl()))</span><br><span class=\"line\">    print(<span class=\"string\">'**********************************************'</span>)</span><br><span class=\"line\">    print(<span class=\"string\">\"info打印信息：%s\"</span>%(response.info()))</span><br><span class=\"line\">    print(<span class=\"string\">'**********************************************'</span>)</span><br><span class=\"line\">    print(<span class=\"string\">\"getcode打印信息：%s\"</span>%(response.getcode()))</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"urlopen的data参数\"><a href=\"#urlopen的data参数\" class=\"headerlink\" title=\"urlopen的data参数\"></a>urlopen的data参数</h3><p>我们可以使用data参数，向服务器发送数据。根据HTTP规范，GET用于信息获取，POST是向服务器提交数据的一种请求，再换句话说：</p>\n<p>从客户端向服务器提交数据使用POST；</p>\n<p>从服务器获得数据到客户端使用GET(GET也可以提交，暂不考虑)。</p>\n<p>如果没有设置urlopen()函数的data参数，HTTP请求采用GET方式，也就是我们从服务器获取信息，如果我们设置data参数，HTTP请求采用POST方式，也就是我们向服务器传递数据。</p>\n<p>data参数有自己的格式，它是一个基于application/x-<a href=\"http://www.form-urlencoded的格式，具体格式我们不用了解，\" target=\"_blank\" rel=\"noopener\">www.form-urlencoded的格式，具体格式我们不用了解，</a> 因为我们可以使用urllib.parse.urlencode()函数将字符串自动转换成上面所说的格式。</p>\n<h3 id=\"发送data实例\"><a href=\"#发送data实例\" class=\"headerlink\" title=\"发送data实例\"></a>发送data实例</h3><p>向有道翻译发送data，得到翻译结果。</p>\n<h4 id=\"1-打开有道翻译界面，如下图所示：\"><a href=\"#1-打开有道翻译界面，如下图所示：\" class=\"headerlink\" title=\"(1).打开有道翻译界面，如下图所示：\"></a>(1).打开有道翻译界面，如下图所示：</h4><p> <img src=\"http://upload-images.jianshu.io/upload_images/2952111-31f629ec53534a43?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"></p>\n<h4 id=\"2-鼠标右键检查，也就是审查元素，如下图所示：\"><a href=\"#2-鼠标右键检查，也就是审查元素，如下图所示：\" class=\"headerlink\" title=\"(2).鼠标右键检查，也就是审查元素，如下图所示：\"></a>(2).鼠标右键检查，也就是审查元素，如下图所示：</h4><p><img src=\"http://upload-images.jianshu.io/upload_images/2952111-f199c9cbcd80b40f?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"></p>\n<h4 id=\"3-选择右侧出现的Network，如下图所示：\"><a href=\"#3-选择右侧出现的Network，如下图所示：\" class=\"headerlink\" title=\"(3).选择右侧出现的Network，如下图所示：\"></a>(3).选择右侧出现的Network，如下图所示：</h4><p> <img src=\"http://upload-images.jianshu.io/upload_images/2952111-4354c17b0169d4b9?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"></p>\n<h4 id=\"4-在左侧输入翻译内容，输入Jack，如下图所示：\"><a href=\"#4-在左侧输入翻译内容，输入Jack，如下图所示：\" class=\"headerlink\" title=\"(4).在左侧输入翻译内容，输入Jack，如下图所示：\"></a>(4).在左侧输入翻译内容，输入Jack，如下图所示：</h4><p> <img src=\"http://upload-images.jianshu.io/upload_images/2952111-1a5e4f785e7bbccf?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"></p>\n<h4 id=\"5-点击自动翻译按钮，我们就可以看到右侧出现的内容，如下图所示：\"><a href=\"#5-点击自动翻译按钮，我们就可以看到右侧出现的内容，如下图所示：\" class=\"headerlink\" title=\"(5).点击自动翻译按钮，我们就可以看到右侧出现的内容，如下图所示：\"></a>(5).点击自动翻译按钮，我们就可以看到右侧出现的内容，如下图所示：</h4><p><img src=\"http://upload-images.jianshu.io/upload_images/2952111-37b47520ec88de2e?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"></p>\n<h4 id=\"6-点击上图红框中的内容，查看它的信息，如下图所示：\"><a href=\"#6-点击上图红框中的内容，查看它的信息，如下图所示：\" class=\"headerlink\" title=\"(6).点击上图红框中的内容，查看它的信息，如下图所示：\"></a>(6).点击上图红框中的内容，查看它的信息，如下图所示：</h4><p> <img src=\"http://upload-images.jianshu.io/upload_images/2952111-9b692ceecf538fd5?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"></p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/2952111-1dc2354ec47dc6ff?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"></p>\n<h4 id=\"7-记住这些信息，这是我们一会儿写程序需要用到的。\"><a href=\"#7-记住这些信息，这是我们一会儿写程序需要用到的。\" class=\"headerlink\" title=\"(7).记住这些信息，这是我们一会儿写程序需要用到的。\"></a>(7).记住这些信息，这是我们一会儿写程序需要用到的。</h4><p>  新建文件translate_test.py，编写如下代码：<br> <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> parse</span><br><span class=\"line\"><span class=\"keyword\">import</span> json</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    <span class=\"comment\">#对应上图的Request URL</span></span><br><span class=\"line\">    Request_URL = <span class=\"string\">'http://fanyi.youdao.com/translate?smartresult=dict&amp;smartresult=rule&amp;sessionFrom=null'</span></span><br><span class=\"line\">    <span class=\"comment\">#创建Form_Data字典，存储上图的Form Data</span></span><br><span class=\"line\">    Form_Data = &#123;&#125;</span><br><span class=\"line\">    Form_Data[<span class=\"string\">'type'</span>] = <span class=\"string\">'AUTO'</span></span><br><span class=\"line\">    Form_Data[<span class=\"string\">'i'</span>] = <span class=\"string\">'Jack'</span></span><br><span class=\"line\">    Form_Data[<span class=\"string\">'doctype'</span>] = <span class=\"string\">'json'</span></span><br><span class=\"line\">    Form_Data[<span class=\"string\">'xmlVersion'</span>] = <span class=\"string\">'1.8'</span></span><br><span class=\"line\">    Form_Data[<span class=\"string\">'keyfrom'</span>] = <span class=\"string\">'fanyi.web'</span></span><br><span class=\"line\">    Form_Data[<span class=\"string\">'ue'</span>] = <span class=\"string\">'ue:UTF-8'</span></span><br><span class=\"line\">    Form_Data[<span class=\"string\">'action'</span>] = <span class=\"string\">'FY_BY_CLICKBUTTON'</span></span><br><span class=\"line\">    <span class=\"comment\">#使用urlencode方法转换标准格式</span></span><br><span class=\"line\">    data = parse.urlencode(Form_Data).encode(<span class=\"string\">'utf-8'</span>)</span><br><span class=\"line\">    <span class=\"comment\">#传递Request对象和转换完格式的数据</span></span><br><span class=\"line\">    response = request.urlopen(Request_URL,data)</span><br><span class=\"line\">    <span class=\"comment\">#读取信息并解码</span></span><br><span class=\"line\">    html = response.read().decode(<span class=\"string\">'utf-8'</span>)</span><br><span class=\"line\">    <span class=\"comment\">#使用JSON</span></span><br><span class=\"line\">    translate_results = json.loads(html)</span><br><span class=\"line\">    <span class=\"comment\">#找到翻译结果</span></span><br><span class=\"line\">    translate_results = translate_results[<span class=\"string\">'translateResult'</span>][<span class=\"number\">0</span>][<span class=\"number\">0</span>][<span class=\"string\">'tgt'</span>]</span><br><span class=\"line\">    <span class=\"comment\">#打印翻译信息</span></span><br><span class=\"line\">    print(<span class=\"string\">\"翻译的结果是：%s\"</span> % translate_results)</span><br></pre></td></tr></table></figure></p>\n<p>运行查看翻译结果</p>"},{"title":"从零开始学爬虫-03","date":"2018-02-27T16:57:01.000Z","_content":"\n## urllib.error\n\nurllib.error可以接收有urllib.request产生的异常。urllib.error有两个方法，URLError和HTTPError。如下图所示：\n![Screen Shot 2018-02-12 at 14.39.09.png](http://upload-images.jianshu.io/upload_images/2952111-165a6b7bb4f6e5af.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\nURLError是OSError的一个子类，HTTPError是URLError的一个子类，服务器上HTTP的响应会返回一个状态码，根据这个HTTP状态码，我们可以知道我们的访问是否成功。例如第二个笔记中提到的200状态码，表示请求成功，再比如常见的404错误等。\n<!-- more -->\n\n### (1).URLError\n\n让我们先看下URLError的异常，创建文件urllib_test05.py，编写如下代码：\n```python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\nfrom urllib import error\n\nif __name__ == \"__main__\":\n    #一个不存在的连接\n    url = \"http://www.dskfclyfiydl.com/\"\n    req = request.Request(url)\n    try:\n        response = request.urlopen(req)\n        html = response.read().decode('utf-8')\n        print(html)\n    except error.URLError as e:\n        print(e.reason)\n```\n\n可以看到如下运行结果：\n\n![Screen Shot 2018-02-12 at 14.35.56.png](http://upload-images.jianshu.io/upload_images/2952111-5e9dfdc6af1af203.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n### (2).HTTPError\n\n再看下HTTPError异常，创建文件urllib_test06.py，编写如下代码：\n```python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\nfrom urllib import error\n\nif __name__ == \"__main__\":\n    #一个不存在的连接\n    url = \"http://www.douyu.com/wkx.html\"\n    req = request.Request(url)\n    try:\n        responese = request.urlopen(req)\n        # html = responese.read()\n    except error.HTTPError as e:\n        print(e.code, '\\n' ,e.reason, '\\n', e.headers)\n```\n\n运行之后，我们可以看到404，这说明请求的资源没有在服务器上找到，www.douyu.com 这个服务器是存在的，但是我们要查找的 Jack_Cui.html 资源是没有的，所以抛出404异常。\n\n![Screen Shot 2018-02-12 at 14.36.07.png](http://upload-images.jianshu.io/upload_images/2952111-877b52f32e81d2cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n### (3).URLError和HTTPError混合使用\n\n最后值得注意的一点是，如果想用HTTPError和URLError一起捕获异常，那么需要将HTTPError放在URLError的前面，因为HTTPError是URLError的一个子类。如果URLError放在前面，出现HTTP异常会先响应URLError，这样HTTPError就捕获不到错误信息了。\n\n![image](http://upload-images.jianshu.io/upload_images/2952111-81c31b50ef0e4f0d?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n如果不用上面的方法，也可以使用hasattr函数判断URLError含有的属性，如果含有reason属性表明是URLError，如果含有code属性表明是HTTPError。创建文件urllib_test07.py，编写代码如下：\n```python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\nfrom urllib import error\n\nif __name__ == \"__main__\":\n    #一个不存在的连接\n    url = \"http://www.douyu.com/wkx.html\"\n    req = request.Request(url)\n    try:\n        responese = request.urlopen(req)\n    except error.URLError as e:\n        if hasattr(e, 'code'):\n            print(\"HTTPError\")\n            print(e.code)\n        elif hasattr(e, 'reason'):\n            print(\"URLError\")\n            print(e.reason)\n```\n\n运行结果如下：\n\n![Screen Shot 2018-02-12 at 14.37.39.png](http://upload-images.jianshu.io/upload_images/2952111-9105667f71cd7051.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n","source":"_posts/从零开始学爬虫-03.md","raw":"---\ntitle: 从零开始学爬虫-03\ndate: 2018-02-28 00:57:01\ntags:\n  - Spider\n  - Urllib\n  - Python3\ncategories:\n  - Spider\n  - Urllib\n---\n\n## urllib.error\n\nurllib.error可以接收有urllib.request产生的异常。urllib.error有两个方法，URLError和HTTPError。如下图所示：\n![Screen Shot 2018-02-12 at 14.39.09.png](http://upload-images.jianshu.io/upload_images/2952111-165a6b7bb4f6e5af.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\nURLError是OSError的一个子类，HTTPError是URLError的一个子类，服务器上HTTP的响应会返回一个状态码，根据这个HTTP状态码，我们可以知道我们的访问是否成功。例如第二个笔记中提到的200状态码，表示请求成功，再比如常见的404错误等。\n<!-- more -->\n\n### (1).URLError\n\n让我们先看下URLError的异常，创建文件urllib_test05.py，编写如下代码：\n```python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\nfrom urllib import error\n\nif __name__ == \"__main__\":\n    #一个不存在的连接\n    url = \"http://www.dskfclyfiydl.com/\"\n    req = request.Request(url)\n    try:\n        response = request.urlopen(req)\n        html = response.read().decode('utf-8')\n        print(html)\n    except error.URLError as e:\n        print(e.reason)\n```\n\n可以看到如下运行结果：\n\n![Screen Shot 2018-02-12 at 14.35.56.png](http://upload-images.jianshu.io/upload_images/2952111-5e9dfdc6af1af203.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n### (2).HTTPError\n\n再看下HTTPError异常，创建文件urllib_test06.py，编写如下代码：\n```python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\nfrom urllib import error\n\nif __name__ == \"__main__\":\n    #一个不存在的连接\n    url = \"http://www.douyu.com/wkx.html\"\n    req = request.Request(url)\n    try:\n        responese = request.urlopen(req)\n        # html = responese.read()\n    except error.HTTPError as e:\n        print(e.code, '\\n' ,e.reason, '\\n', e.headers)\n```\n\n运行之后，我们可以看到404，这说明请求的资源没有在服务器上找到，www.douyu.com 这个服务器是存在的，但是我们要查找的 Jack_Cui.html 资源是没有的，所以抛出404异常。\n\n![Screen Shot 2018-02-12 at 14.36.07.png](http://upload-images.jianshu.io/upload_images/2952111-877b52f32e81d2cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n### (3).URLError和HTTPError混合使用\n\n最后值得注意的一点是，如果想用HTTPError和URLError一起捕获异常，那么需要将HTTPError放在URLError的前面，因为HTTPError是URLError的一个子类。如果URLError放在前面，出现HTTP异常会先响应URLError，这样HTTPError就捕获不到错误信息了。\n\n![image](http://upload-images.jianshu.io/upload_images/2952111-81c31b50ef0e4f0d?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n如果不用上面的方法，也可以使用hasattr函数判断URLError含有的属性，如果含有reason属性表明是URLError，如果含有code属性表明是HTTPError。创建文件urllib_test07.py，编写代码如下：\n```python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\nfrom urllib import error\n\nif __name__ == \"__main__\":\n    #一个不存在的连接\n    url = \"http://www.douyu.com/wkx.html\"\n    req = request.Request(url)\n    try:\n        responese = request.urlopen(req)\n    except error.URLError as e:\n        if hasattr(e, 'code'):\n            print(\"HTTPError\")\n            print(e.code)\n        elif hasattr(e, 'reason'):\n            print(\"URLError\")\n            print(e.reason)\n```\n\n运行结果如下：\n\n![Screen Shot 2018-02-12 at 14.37.39.png](http://upload-images.jianshu.io/upload_images/2952111-9105667f71cd7051.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n","slug":"从零开始学爬虫-03","published":1,"updated":"2018-02-27T17:04:43.829Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cje6mdg0f000rrcottigupu89","content":"<h2 id=\"urllib-error\"><a href=\"#urllib-error\" class=\"headerlink\" title=\"urllib.error\"></a>urllib.error</h2><p>urllib.error可以接收有urllib.request产生的异常。urllib.error有两个方法，URLError和HTTPError。如下图所示：<br><img src=\"http://upload-images.jianshu.io/upload_images/2952111-165a6b7bb4f6e5af.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Screen Shot 2018-02-12 at 14.39.09.png\"></p>\n<p>URLError是OSError的一个子类，HTTPError是URLError的一个子类，服务器上HTTP的响应会返回一个状态码，根据这个HTTP状态码，我们可以知道我们的访问是否成功。例如第二个笔记中提到的200状态码，表示请求成功，再比如常见的404错误等。<br><a id=\"more\"></a></p>\n<h3 id=\"1-URLError\"><a href=\"#1-URLError\" class=\"headerlink\" title=\"(1).URLError\"></a>(1).URLError</h3><p>让我们先看下URLError的异常，创建文件urllib_test05.py，编写如下代码：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> error</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    <span class=\"comment\">#一个不存在的连接</span></span><br><span class=\"line\">    url = <span class=\"string\">\"http://www.dskfclyfiydl.com/\"</span></span><br><span class=\"line\">    req = request.Request(url)</span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        response = request.urlopen(req)</span><br><span class=\"line\">        html = response.read().decode(<span class=\"string\">'utf-8'</span>)</span><br><span class=\"line\">        print(html)</span><br><span class=\"line\">    <span class=\"keyword\">except</span> error.URLError <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">        print(e.reason)</span><br></pre></td></tr></table></figure></p>\n<p>可以看到如下运行结果：</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/2952111-5e9dfdc6af1af203.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Screen Shot 2018-02-12 at 14.35.56.png\"></p>\n<h3 id=\"2-HTTPError\"><a href=\"#2-HTTPError\" class=\"headerlink\" title=\"(2).HTTPError\"></a>(2).HTTPError</h3><p>再看下HTTPError异常，创建文件urllib_test06.py，编写如下代码：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> error</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    <span class=\"comment\">#一个不存在的连接</span></span><br><span class=\"line\">    url = <span class=\"string\">\"http://www.douyu.com/wkx.html\"</span></span><br><span class=\"line\">    req = request.Request(url)</span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        responese = request.urlopen(req)</span><br><span class=\"line\">        <span class=\"comment\"># html = responese.read()</span></span><br><span class=\"line\">    <span class=\"keyword\">except</span> error.HTTPError <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">        print(e.code, <span class=\"string\">'\\n'</span> ,e.reason, <span class=\"string\">'\\n'</span>, e.headers)</span><br></pre></td></tr></table></figure></p>\n<p>运行之后，我们可以看到404，这说明请求的资源没有在服务器上找到，<a href=\"http://www.douyu.com\" target=\"_blank\" rel=\"noopener\">www.douyu.com</a> 这个服务器是存在的，但是我们要查找的 Jack_Cui.html 资源是没有的，所以抛出404异常。</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/2952111-877b52f32e81d2cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Screen Shot 2018-02-12 at 14.36.07.png\"></p>\n<h3 id=\"3-URLError和HTTPError混合使用\"><a href=\"#3-URLError和HTTPError混合使用\" class=\"headerlink\" title=\"(3).URLError和HTTPError混合使用\"></a>(3).URLError和HTTPError混合使用</h3><p>最后值得注意的一点是，如果想用HTTPError和URLError一起捕获异常，那么需要将HTTPError放在URLError的前面，因为HTTPError是URLError的一个子类。如果URLError放在前面，出现HTTP异常会先响应URLError，这样HTTPError就捕获不到错误信息了。</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/2952111-81c31b50ef0e4f0d?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"></p>\n<p>如果不用上面的方法，也可以使用hasattr函数判断URLError含有的属性，如果含有reason属性表明是URLError，如果含有code属性表明是HTTPError。创建文件urllib_test07.py，编写代码如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> error</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    <span class=\"comment\">#一个不存在的连接</span></span><br><span class=\"line\">    url = <span class=\"string\">\"http://www.douyu.com/wkx.html\"</span></span><br><span class=\"line\">    req = request.Request(url)</span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        responese = request.urlopen(req)</span><br><span class=\"line\">    <span class=\"keyword\">except</span> error.URLError <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> hasattr(e, <span class=\"string\">'code'</span>):</span><br><span class=\"line\">            print(<span class=\"string\">\"HTTPError\"</span>)</span><br><span class=\"line\">            print(e.code)</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> hasattr(e, <span class=\"string\">'reason'</span>):</span><br><span class=\"line\">            print(<span class=\"string\">\"URLError\"</span>)</span><br><span class=\"line\">            print(e.reason)</span><br></pre></td></tr></table></figure></p>\n<p>运行结果如下：</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/2952111-9105667f71cd7051.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Screen Shot 2018-02-12 at 14.37.39.png\"></p>\n","site":{"data":{}},"excerpt":"<h2 id=\"urllib-error\"><a href=\"#urllib-error\" class=\"headerlink\" title=\"urllib.error\"></a>urllib.error</h2><p>urllib.error可以接收有urllib.request产生的异常。urllib.error有两个方法，URLError和HTTPError。如下图所示：<br><img src=\"http://upload-images.jianshu.io/upload_images/2952111-165a6b7bb4f6e5af.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Screen Shot 2018-02-12 at 14.39.09.png\"></p>\n<p>URLError是OSError的一个子类，HTTPError是URLError的一个子类，服务器上HTTP的响应会返回一个状态码，根据这个HTTP状态码，我们可以知道我们的访问是否成功。例如第二个笔记中提到的200状态码，表示请求成功，再比如常见的404错误等。<br>","more":"</p>\n<h3 id=\"1-URLError\"><a href=\"#1-URLError\" class=\"headerlink\" title=\"(1).URLError\"></a>(1).URLError</h3><p>让我们先看下URLError的异常，创建文件urllib_test05.py，编写如下代码：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> error</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    <span class=\"comment\">#一个不存在的连接</span></span><br><span class=\"line\">    url = <span class=\"string\">\"http://www.dskfclyfiydl.com/\"</span></span><br><span class=\"line\">    req = request.Request(url)</span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        response = request.urlopen(req)</span><br><span class=\"line\">        html = response.read().decode(<span class=\"string\">'utf-8'</span>)</span><br><span class=\"line\">        print(html)</span><br><span class=\"line\">    <span class=\"keyword\">except</span> error.URLError <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">        print(e.reason)</span><br></pre></td></tr></table></figure></p>\n<p>可以看到如下运行结果：</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/2952111-5e9dfdc6af1af203.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Screen Shot 2018-02-12 at 14.35.56.png\"></p>\n<h3 id=\"2-HTTPError\"><a href=\"#2-HTTPError\" class=\"headerlink\" title=\"(2).HTTPError\"></a>(2).HTTPError</h3><p>再看下HTTPError异常，创建文件urllib_test06.py，编写如下代码：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> error</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    <span class=\"comment\">#一个不存在的连接</span></span><br><span class=\"line\">    url = <span class=\"string\">\"http://www.douyu.com/wkx.html\"</span></span><br><span class=\"line\">    req = request.Request(url)</span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        responese = request.urlopen(req)</span><br><span class=\"line\">        <span class=\"comment\"># html = responese.read()</span></span><br><span class=\"line\">    <span class=\"keyword\">except</span> error.HTTPError <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">        print(e.code, <span class=\"string\">'\\n'</span> ,e.reason, <span class=\"string\">'\\n'</span>, e.headers)</span><br></pre></td></tr></table></figure></p>\n<p>运行之后，我们可以看到404，这说明请求的资源没有在服务器上找到，<a href=\"http://www.douyu.com\" target=\"_blank\" rel=\"noopener\">www.douyu.com</a> 这个服务器是存在的，但是我们要查找的 Jack_Cui.html 资源是没有的，所以抛出404异常。</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/2952111-877b52f32e81d2cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Screen Shot 2018-02-12 at 14.36.07.png\"></p>\n<h3 id=\"3-URLError和HTTPError混合使用\"><a href=\"#3-URLError和HTTPError混合使用\" class=\"headerlink\" title=\"(3).URLError和HTTPError混合使用\"></a>(3).URLError和HTTPError混合使用</h3><p>最后值得注意的一点是，如果想用HTTPError和URLError一起捕获异常，那么需要将HTTPError放在URLError的前面，因为HTTPError是URLError的一个子类。如果URLError放在前面，出现HTTP异常会先响应URLError，这样HTTPError就捕获不到错误信息了。</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/2952111-81c31b50ef0e4f0d?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"image\"></p>\n<p>如果不用上面的方法，也可以使用hasattr函数判断URLError含有的属性，如果含有reason属性表明是URLError，如果含有code属性表明是HTTPError。创建文件urllib_test07.py，编写代码如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> error</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    <span class=\"comment\">#一个不存在的连接</span></span><br><span class=\"line\">    url = <span class=\"string\">\"http://www.douyu.com/wkx.html\"</span></span><br><span class=\"line\">    req = request.Request(url)</span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        responese = request.urlopen(req)</span><br><span class=\"line\">    <span class=\"keyword\">except</span> error.URLError <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> hasattr(e, <span class=\"string\">'code'</span>):</span><br><span class=\"line\">            print(<span class=\"string\">\"HTTPError\"</span>)</span><br><span class=\"line\">            print(e.code)</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> hasattr(e, <span class=\"string\">'reason'</span>):</span><br><span class=\"line\">            print(<span class=\"string\">\"URLError\"</span>)</span><br><span class=\"line\">            print(e.reason)</span><br></pre></td></tr></table></figure></p>\n<p>运行结果如下：</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/2952111-9105667f71cd7051.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Screen Shot 2018-02-12 at 14.37.39.png\"></p>"},{"title":"从零开始学爬虫-04","date":"2018-02-27T17:06:56.000Z","_content":"\n## 说在前面\n\nurllib_test10.py已经无法爬取了，因为原网站已经添加了防爬虫机制\n\n## (一)、为何要设置User Agent\n\n有一些网站不喜欢被爬虫程序访问，所以会检测连接对象，如果是爬虫程序，也就是非人点击访问，它就会不让你继续访问，所以为了要让程序可以正常运行，需要隐藏自己的爬虫程序的身份。此时，我们就可以通过设置User Agent的来达到隐藏身份的目的，User Agent的中文名为用户代理，简称UA。\n\nUser Agent存放于Headers中，服务器就是通过查看Headers中的User Agent来判断是谁在访问。在Python中，如果不设置User Agent，程序将使用默认的参数，那么这个User Agent就会有Python的字样，如果服务器检查User Agent，那么没有设置User Agent的Python程序将无法正常访问网站。\n\nPython允许我们修改这个User Agent来模拟浏览器访问，它的强大毋庸置疑。\n<!-- more -->\n\n## (二)、常见的User Agent\n\n### (1).Android\n\n  - Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19\n  - Mozilla/5.0 (Linux; U; Android 4.0.4; en-gb; GT-I9300 Build/IMM76D) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30\n  - Mozilla/5.0 (Linux; U; Android 2.2; en-gb; GT-P1000 Build/FROYO) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1\n\n### (2).Firefox\n\n  - Mozilla/5.0 (Windows NT 6.2; WOW64; rv:21.0) Gecko/20100101 Firefox/21.0\n  - Mozilla/5.0 (Android; Mobile; rv:14.0) Gecko/14.0 Firefox/14.0\n\n### (3).Google Chrome\n\n  - Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36\n  - Mozilla/5.0 (Linux; Android 4.0.4; Galaxy Nexus Build/IMM76B) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.133 Mobile Safari/535.19\n\n### (4).iOS\n\n  - Mozilla/5.0 (iPad; CPU OS 5_0 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9A334 Safari/7534.48.3\n  - Mozilla/5.0 (iPod; U; CPU like Mac OS X; en) AppleWebKit/420.1 (KHTML, like Gecko) Version/3.0 Mobile/3A101a Safari/419.3\n\n上面列举了Andriod、Firefox、Google Chrome、iOS的一些User Agent，直接copy就能用。\n\n## (三)、设置User Agent的方法\n\n先看下urllib.request.Request()\n\n ![1](http://img.blog.csdn.net/20170303123244632?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n从上图可以看出，在创建Request对象的时候，可以传入headers参数。因此，想要设置User Agent，有两种方法：\n\n- 1.在创建Request对象的时候，填入headers参数(包含User Agent信息)，这个Headers参数要求为字典；\n\n- 2.在创建Request对象的时候不添加headers参数，在创建完成之后，使用add_header()的方法，添加headers。\n\n### 方法一\n\n创建文件urllib_test08.py，使用上面提到的Android的第一个User Agent，在创建Request对象的时候传入headers参数，编写代码如下：\n```python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\n\nif __name__ == \"__main__\":\n    url = 'http://www.csdn.net/'\n    head = {}\n    #写入User Agent信息\n    head['User-Agent'] = 'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19'\n    #创建Request对象\n    req = request.Request(url, headers=head)\n    #传入创建好的Request对象\n    response = request.urlopen(req)\n    #读取响应信息并解码\n    html = response.read().decode('utf-8')\n    #打印信息\n    print(html)\n```\n\n运行结果如下：\n\n ![2](http://img.blog.csdn.net/20170303123738649?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n### 方法二\n\n创建文件urllib_test09.py，使用上面提到的Android的第一个User Agent，在创建Request对象时不传入headers参数，创建之后使用add_header()方法，添加headers，编写代码如下：\n```Python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\n\nif __name__ == \"__main__\":\n    #以CSDN为例，CSDN不更改User Agent是无法访问的\n    url = 'http://www.csdn.net/'\n    #创建Request对象\n    req = request.Request(url)\n    #传入headers\n    req.add_header('User-Agent', 'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19')\n    #传入创建好的Request对象\n    response = request.urlopen(req)\n    #读取响应信息并解码\n    html = response.read().decode('utf-8')\n    #打印信息\n    print(html)\n```\n\n运行结果和上一个方法是一样的。\n\n## (四)、IP代理的使用\n\n### (1).为何使用IP代理\n\nUser Agent已经设置好了，但是还应该考虑一个问题，程序的运行速度是很快的，如果我们利用一个爬虫程序在网站爬取东西，一个固定IP的访问频率就会很高，这不符合人为操作的标准，因为人操作不可能在几ms内，进行如此频繁的访问。所以一些网站会设置一个IP访问频率的阈值，如果一个IP访问频率超过这个阈值，说明这个不是人在访问，而是一个爬虫程序。\n\n### (2).一般步骤说明\n\n一个很简单的解决办法就是设置延时，但是这显然不符合爬虫快速爬取信息的目的，所以另一种更好的方法就是使用IP代理。使用代理的步骤：\n\n**(1)** 调用urlib.request.ProxyHandler()，proxies参数为一个字典。\n\n ![4](http://img.blog.csdn.net/20170303124421012?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n**(2)** 创建Opener(类似于urlopen，这个代开方式是我们自己定制的)\n\n![5](http://img.blog.csdn.net/20170303124447169?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n**(3)** 安装Opener\n\n![引用容](http://img.blog.csdn.net/20170303124507044?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n使用install_opener方法之后，会将程序默认的urlopen方法替换掉。也就是说，如果使用install_opener之后，在该文件中，再次调用urlopen会使用自己创建好的opener。如果不想替换掉，只是想临时使用一下，可以使用opener.open(url)，这样就不会对程序默认的urlopen有影响。\n\n### (3).代理IP选取\n\n在写代码之前，先在代理IP网站选好一个IP地址，推荐西刺代理IP。\n\nURL：[http://www.xicidaili.com/](http://www.xicidaili.com/)\n\n注意：当然也可以写个正则表达式从网站直接爬取IP，但是要记住不要太频繁爬取，加个延时什么的，太频繁给服务器带来压力了，服务器会直接把你block，不让你访问的，我就被封了两天。\n\n从西刺网站选出信号好的IP，我的选择如下：(106.46.136.112:808)\n\n ![6](http://img.blog.csdn.net/20170303124651091?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n编写代码访问[http://www.whatismyip.com.tw/](http://www.whatismyip.com.tw/)，该网站是测试自己IP为多少的网址，服务器会返回访问者的IP。\n\n### (4).代码实例\n\n创建文件urllib_test10.py，编写代码如下：\n```python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\n\nif __name__ == \"__main__\":\n    #访问网址\n    url = 'http://www.whatismyip.com.tw/'\n    #这是代理IP\n    proxy = {'http':'106.46.136.112:808'}\n    #创建ProxyHandler\n    proxy_support = request.ProxyHandler(proxy)\n    #创建Opener\n    opener = request.build_opener(proxy_support)\n    #添加User Angent\n    opener.addheaders = [('User-Agent','Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36')]\n    #安装OPener\n    request.install_opener(opener)\n    #使用自己安装好的Opener\n    response = request.urlopen(url)\n    #读取相应信息并解码\n    html = response.read().decode(\"utf-8\")\n    #打印信息\n    print(html)\n```\n\n运行结果如下：\n![7](http://img.blog.csdn.net/20170303124823038?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n从上图可以看出，访问的IP已经伪装成了106.46.136.112。\n","source":"_posts/从零开始学爬虫-04.md","raw":"---\ntitle: 从零开始学爬虫-04\ndate: 2018-02-28 01:06:56\ntags:\n  - Spider\n  - Urllib\n  - Python3\ncategories:\n  - Spider\n  - Urllib\n---\n\n## 说在前面\n\nurllib_test10.py已经无法爬取了，因为原网站已经添加了防爬虫机制\n\n## (一)、为何要设置User Agent\n\n有一些网站不喜欢被爬虫程序访问，所以会检测连接对象，如果是爬虫程序，也就是非人点击访问，它就会不让你继续访问，所以为了要让程序可以正常运行，需要隐藏自己的爬虫程序的身份。此时，我们就可以通过设置User Agent的来达到隐藏身份的目的，User Agent的中文名为用户代理，简称UA。\n\nUser Agent存放于Headers中，服务器就是通过查看Headers中的User Agent来判断是谁在访问。在Python中，如果不设置User Agent，程序将使用默认的参数，那么这个User Agent就会有Python的字样，如果服务器检查User Agent，那么没有设置User Agent的Python程序将无法正常访问网站。\n\nPython允许我们修改这个User Agent来模拟浏览器访问，它的强大毋庸置疑。\n<!-- more -->\n\n## (二)、常见的User Agent\n\n### (1).Android\n\n  - Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19\n  - Mozilla/5.0 (Linux; U; Android 4.0.4; en-gb; GT-I9300 Build/IMM76D) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30\n  - Mozilla/5.0 (Linux; U; Android 2.2; en-gb; GT-P1000 Build/FROYO) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1\n\n### (2).Firefox\n\n  - Mozilla/5.0 (Windows NT 6.2; WOW64; rv:21.0) Gecko/20100101 Firefox/21.0\n  - Mozilla/5.0 (Android; Mobile; rv:14.0) Gecko/14.0 Firefox/14.0\n\n### (3).Google Chrome\n\n  - Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36\n  - Mozilla/5.0 (Linux; Android 4.0.4; Galaxy Nexus Build/IMM76B) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.133 Mobile Safari/535.19\n\n### (4).iOS\n\n  - Mozilla/5.0 (iPad; CPU OS 5_0 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9A334 Safari/7534.48.3\n  - Mozilla/5.0 (iPod; U; CPU like Mac OS X; en) AppleWebKit/420.1 (KHTML, like Gecko) Version/3.0 Mobile/3A101a Safari/419.3\n\n上面列举了Andriod、Firefox、Google Chrome、iOS的一些User Agent，直接copy就能用。\n\n## (三)、设置User Agent的方法\n\n先看下urllib.request.Request()\n\n ![1](http://img.blog.csdn.net/20170303123244632?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n从上图可以看出，在创建Request对象的时候，可以传入headers参数。因此，想要设置User Agent，有两种方法：\n\n- 1.在创建Request对象的时候，填入headers参数(包含User Agent信息)，这个Headers参数要求为字典；\n\n- 2.在创建Request对象的时候不添加headers参数，在创建完成之后，使用add_header()的方法，添加headers。\n\n### 方法一\n\n创建文件urllib_test08.py，使用上面提到的Android的第一个User Agent，在创建Request对象的时候传入headers参数，编写代码如下：\n```python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\n\nif __name__ == \"__main__\":\n    url = 'http://www.csdn.net/'\n    head = {}\n    #写入User Agent信息\n    head['User-Agent'] = 'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19'\n    #创建Request对象\n    req = request.Request(url, headers=head)\n    #传入创建好的Request对象\n    response = request.urlopen(req)\n    #读取响应信息并解码\n    html = response.read().decode('utf-8')\n    #打印信息\n    print(html)\n```\n\n运行结果如下：\n\n ![2](http://img.blog.csdn.net/20170303123738649?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n### 方法二\n\n创建文件urllib_test09.py，使用上面提到的Android的第一个User Agent，在创建Request对象时不传入headers参数，创建之后使用add_header()方法，添加headers，编写代码如下：\n```Python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\n\nif __name__ == \"__main__\":\n    #以CSDN为例，CSDN不更改User Agent是无法访问的\n    url = 'http://www.csdn.net/'\n    #创建Request对象\n    req = request.Request(url)\n    #传入headers\n    req.add_header('User-Agent', 'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19')\n    #传入创建好的Request对象\n    response = request.urlopen(req)\n    #读取响应信息并解码\n    html = response.read().decode('utf-8')\n    #打印信息\n    print(html)\n```\n\n运行结果和上一个方法是一样的。\n\n## (四)、IP代理的使用\n\n### (1).为何使用IP代理\n\nUser Agent已经设置好了，但是还应该考虑一个问题，程序的运行速度是很快的，如果我们利用一个爬虫程序在网站爬取东西，一个固定IP的访问频率就会很高，这不符合人为操作的标准，因为人操作不可能在几ms内，进行如此频繁的访问。所以一些网站会设置一个IP访问频率的阈值，如果一个IP访问频率超过这个阈值，说明这个不是人在访问，而是一个爬虫程序。\n\n### (2).一般步骤说明\n\n一个很简单的解决办法就是设置延时，但是这显然不符合爬虫快速爬取信息的目的，所以另一种更好的方法就是使用IP代理。使用代理的步骤：\n\n**(1)** 调用urlib.request.ProxyHandler()，proxies参数为一个字典。\n\n ![4](http://img.blog.csdn.net/20170303124421012?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n**(2)** 创建Opener(类似于urlopen，这个代开方式是我们自己定制的)\n\n![5](http://img.blog.csdn.net/20170303124447169?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n**(3)** 安装Opener\n\n![引用容](http://img.blog.csdn.net/20170303124507044?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n使用install_opener方法之后，会将程序默认的urlopen方法替换掉。也就是说，如果使用install_opener之后，在该文件中，再次调用urlopen会使用自己创建好的opener。如果不想替换掉，只是想临时使用一下，可以使用opener.open(url)，这样就不会对程序默认的urlopen有影响。\n\n### (3).代理IP选取\n\n在写代码之前，先在代理IP网站选好一个IP地址，推荐西刺代理IP。\n\nURL：[http://www.xicidaili.com/](http://www.xicidaili.com/)\n\n注意：当然也可以写个正则表达式从网站直接爬取IP，但是要记住不要太频繁爬取，加个延时什么的，太频繁给服务器带来压力了，服务器会直接把你block，不让你访问的，我就被封了两天。\n\n从西刺网站选出信号好的IP，我的选择如下：(106.46.136.112:808)\n\n ![6](http://img.blog.csdn.net/20170303124651091?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n编写代码访问[http://www.whatismyip.com.tw/](http://www.whatismyip.com.tw/)，该网站是测试自己IP为多少的网址，服务器会返回访问者的IP。\n\n### (4).代码实例\n\n创建文件urllib_test10.py，编写代码如下：\n```python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\n\nif __name__ == \"__main__\":\n    #访问网址\n    url = 'http://www.whatismyip.com.tw/'\n    #这是代理IP\n    proxy = {'http':'106.46.136.112:808'}\n    #创建ProxyHandler\n    proxy_support = request.ProxyHandler(proxy)\n    #创建Opener\n    opener = request.build_opener(proxy_support)\n    #添加User Angent\n    opener.addheaders = [('User-Agent','Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36')]\n    #安装OPener\n    request.install_opener(opener)\n    #使用自己安装好的Opener\n    response = request.urlopen(url)\n    #读取相应信息并解码\n    html = response.read().decode(\"utf-8\")\n    #打印信息\n    print(html)\n```\n\n运行结果如下：\n![7](http://img.blog.csdn.net/20170303124823038?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n从上图可以看出，访问的IP已经伪装成了106.46.136.112。\n","slug":"从零开始学爬虫-04","published":1,"updated":"2018-02-27T17:18:25.138Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cje6mdg0i000srcotackwp1mg","content":"<h2 id=\"说在前面\"><a href=\"#说在前面\" class=\"headerlink\" title=\"说在前面\"></a>说在前面</h2><p>urllib_test10.py已经无法爬取了，因为原网站已经添加了防爬虫机制</p>\n<h2 id=\"一-、为何要设置User-Agent\"><a href=\"#一-、为何要设置User-Agent\" class=\"headerlink\" title=\"(一)、为何要设置User Agent\"></a>(一)、为何要设置User Agent</h2><p>有一些网站不喜欢被爬虫程序访问，所以会检测连接对象，如果是爬虫程序，也就是非人点击访问，它就会不让你继续访问，所以为了要让程序可以正常运行，需要隐藏自己的爬虫程序的身份。此时，我们就可以通过设置User Agent的来达到隐藏身份的目的，User Agent的中文名为用户代理，简称UA。</p>\n<p>User Agent存放于Headers中，服务器就是通过查看Headers中的User Agent来判断是谁在访问。在Python中，如果不设置User Agent，程序将使用默认的参数，那么这个User Agent就会有Python的字样，如果服务器检查User Agent，那么没有设置User Agent的Python程序将无法正常访问网站。</p>\n<p>Python允许我们修改这个User Agent来模拟浏览器访问，它的强大毋庸置疑。<br><a id=\"more\"></a></p>\n<h2 id=\"二-、常见的User-Agent\"><a href=\"#二-、常见的User-Agent\" class=\"headerlink\" title=\"(二)、常见的User Agent\"></a>(二)、常见的User Agent</h2><h3 id=\"1-Android\"><a href=\"#1-Android\" class=\"headerlink\" title=\"(1).Android\"></a>(1).Android</h3><ul>\n<li>Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19</li>\n<li>Mozilla/5.0 (Linux; U; Android 4.0.4; en-gb; GT-I9300 Build/IMM76D) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30</li>\n<li>Mozilla/5.0 (Linux; U; Android 2.2; en-gb; GT-P1000 Build/FROYO) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1</li>\n</ul>\n<h3 id=\"2-Firefox\"><a href=\"#2-Firefox\" class=\"headerlink\" title=\"(2).Firefox\"></a>(2).Firefox</h3><ul>\n<li>Mozilla/5.0 (Windows NT 6.2; WOW64; rv:21.0) Gecko/20100101 Firefox/21.0</li>\n<li>Mozilla/5.0 (Android; Mobile; rv:14.0) Gecko/14.0 Firefox/14.0</li>\n</ul>\n<h3 id=\"3-Google-Chrome\"><a href=\"#3-Google-Chrome\" class=\"headerlink\" title=\"(3).Google Chrome\"></a>(3).Google Chrome</h3><ul>\n<li>Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36</li>\n<li>Mozilla/5.0 (Linux; Android 4.0.4; Galaxy Nexus Build/IMM76B) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.133 Mobile Safari/535.19</li>\n</ul>\n<h3 id=\"4-iOS\"><a href=\"#4-iOS\" class=\"headerlink\" title=\"(4).iOS\"></a>(4).iOS</h3><ul>\n<li>Mozilla/5.0 (iPad; CPU OS 5_0 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9A334 Safari/7534.48.3</li>\n<li>Mozilla/5.0 (iPod; U; CPU like Mac OS X; en) AppleWebKit/420.1 (KHTML, like Gecko) Version/3.0 Mobile/3A101a Safari/419.3</li>\n</ul>\n<p>上面列举了Andriod、Firefox、Google Chrome、iOS的一些User Agent，直接copy就能用。</p>\n<h2 id=\"三-、设置User-Agent的方法\"><a href=\"#三-、设置User-Agent的方法\" class=\"headerlink\" title=\"(三)、设置User Agent的方法\"></a>(三)、设置User Agent的方法</h2><p>先看下urllib.request.Request()</p>\n<p> <img src=\"http://img.blog.csdn.net/20170303123244632?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"1\"></p>\n<p>从上图可以看出，在创建Request对象的时候，可以传入headers参数。因此，想要设置User Agent，有两种方法：</p>\n<ul>\n<li><p>1.在创建Request对象的时候，填入headers参数(包含User Agent信息)，这个Headers参数要求为字典；</p>\n</li>\n<li><p>2.在创建Request对象的时候不添加headers参数，在创建完成之后，使用add_header()的方法，添加headers。</p>\n</li>\n</ul>\n<h3 id=\"方法一\"><a href=\"#方法一\" class=\"headerlink\" title=\"方法一\"></a>方法一</h3><p>创建文件urllib_test08.py，使用上面提到的Android的第一个User Agent，在创建Request对象的时候传入headers参数，编写代码如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    url = <span class=\"string\">'http://www.csdn.net/'</span></span><br><span class=\"line\">    head = &#123;&#125;</span><br><span class=\"line\">    <span class=\"comment\">#写入User Agent信息</span></span><br><span class=\"line\">    head[<span class=\"string\">'User-Agent'</span>] = <span class=\"string\">'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19'</span></span><br><span class=\"line\">    <span class=\"comment\">#创建Request对象</span></span><br><span class=\"line\">    req = request.Request(url, headers=head)</span><br><span class=\"line\">    <span class=\"comment\">#传入创建好的Request对象</span></span><br><span class=\"line\">    response = request.urlopen(req)</span><br><span class=\"line\">    <span class=\"comment\">#读取响应信息并解码</span></span><br><span class=\"line\">    html = response.read().decode(<span class=\"string\">'utf-8'</span>)</span><br><span class=\"line\">    <span class=\"comment\">#打印信息</span></span><br><span class=\"line\">    print(html)</span><br></pre></td></tr></table></figure></p>\n<p>运行结果如下：</p>\n<p> <img src=\"http://img.blog.csdn.net/20170303123738649?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"2\"></p>\n<h3 id=\"方法二\"><a href=\"#方法二\" class=\"headerlink\" title=\"方法二\"></a>方法二</h3><p>创建文件urllib_test09.py，使用上面提到的Android的第一个User Agent，在创建Request对象时不传入headers参数，创建之后使用add_header()方法，添加headers，编写代码如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    <span class=\"comment\">#以CSDN为例，CSDN不更改User Agent是无法访问的</span></span><br><span class=\"line\">    url = <span class=\"string\">'http://www.csdn.net/'</span></span><br><span class=\"line\">    <span class=\"comment\">#创建Request对象</span></span><br><span class=\"line\">    req = request.Request(url)</span><br><span class=\"line\">    <span class=\"comment\">#传入headers</span></span><br><span class=\"line\">    req.add_header(<span class=\"string\">'User-Agent'</span>, <span class=\"string\">'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19'</span>)</span><br><span class=\"line\">    <span class=\"comment\">#传入创建好的Request对象</span></span><br><span class=\"line\">    response = request.urlopen(req)</span><br><span class=\"line\">    <span class=\"comment\">#读取响应信息并解码</span></span><br><span class=\"line\">    html = response.read().decode(<span class=\"string\">'utf-8'</span>)</span><br><span class=\"line\">    <span class=\"comment\">#打印信息</span></span><br><span class=\"line\">    print(html)</span><br></pre></td></tr></table></figure></p>\n<p>运行结果和上一个方法是一样的。</p>\n<h2 id=\"四-、IP代理的使用\"><a href=\"#四-、IP代理的使用\" class=\"headerlink\" title=\"(四)、IP代理的使用\"></a>(四)、IP代理的使用</h2><h3 id=\"1-为何使用IP代理\"><a href=\"#1-为何使用IP代理\" class=\"headerlink\" title=\"(1).为何使用IP代理\"></a>(1).为何使用IP代理</h3><p>User Agent已经设置好了，但是还应该考虑一个问题，程序的运行速度是很快的，如果我们利用一个爬虫程序在网站爬取东西，一个固定IP的访问频率就会很高，这不符合人为操作的标准，因为人操作不可能在几ms内，进行如此频繁的访问。所以一些网站会设置一个IP访问频率的阈值，如果一个IP访问频率超过这个阈值，说明这个不是人在访问，而是一个爬虫程序。</p>\n<h3 id=\"2-一般步骤说明\"><a href=\"#2-一般步骤说明\" class=\"headerlink\" title=\"(2).一般步骤说明\"></a>(2).一般步骤说明</h3><p>一个很简单的解决办法就是设置延时，但是这显然不符合爬虫快速爬取信息的目的，所以另一种更好的方法就是使用IP代理。使用代理的步骤：</p>\n<p><strong>(1)</strong> 调用urlib.request.ProxyHandler()，proxies参数为一个字典。</p>\n<p> <img src=\"http://img.blog.csdn.net/20170303124421012?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"4\"></p>\n<p><strong>(2)</strong> 创建Opener(类似于urlopen，这个代开方式是我们自己定制的)</p>\n<p><img src=\"http://img.blog.csdn.net/20170303124447169?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"5\"></p>\n<p><strong>(3)</strong> 安装Opener</p>\n<p><img src=\"http://img.blog.csdn.net/20170303124507044?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"引用容\"></p>\n<p>使用install_opener方法之后，会将程序默认的urlopen方法替换掉。也就是说，如果使用install_opener之后，在该文件中，再次调用urlopen会使用自己创建好的opener。如果不想替换掉，只是想临时使用一下，可以使用opener.open(url)，这样就不会对程序默认的urlopen有影响。</p>\n<h3 id=\"3-代理IP选取\"><a href=\"#3-代理IP选取\" class=\"headerlink\" title=\"(3).代理IP选取\"></a>(3).代理IP选取</h3><p>在写代码之前，先在代理IP网站选好一个IP地址，推荐西刺代理IP。</p>\n<p>URL：<a href=\"http://www.xicidaili.com/\" target=\"_blank\" rel=\"noopener\">http://www.xicidaili.com/</a></p>\n<p>注意：当然也可以写个正则表达式从网站直接爬取IP，但是要记住不要太频繁爬取，加个延时什么的，太频繁给服务器带来压力了，服务器会直接把你block，不让你访问的，我就被封了两天。</p>\n<p>从西刺网站选出信号好的IP，我的选择如下：(106.46.136.112:808)</p>\n<p> <img src=\"http://img.blog.csdn.net/20170303124651091?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"6\"></p>\n<p>编写代码访问<a href=\"http://www.whatismyip.com.tw/\" target=\"_blank\" rel=\"noopener\">http://www.whatismyip.com.tw/</a>，该网站是测试自己IP为多少的网址，服务器会返回访问者的IP。</p>\n<h3 id=\"4-代码实例\"><a href=\"#4-代码实例\" class=\"headerlink\" title=\"(4).代码实例\"></a>(4).代码实例</h3><p>创建文件urllib_test10.py，编写代码如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    <span class=\"comment\">#访问网址</span></span><br><span class=\"line\">    url = <span class=\"string\">'http://www.whatismyip.com.tw/'</span></span><br><span class=\"line\">    <span class=\"comment\">#这是代理IP</span></span><br><span class=\"line\">    proxy = &#123;<span class=\"string\">'http'</span>:<span class=\"string\">'106.46.136.112:808'</span>&#125;</span><br><span class=\"line\">    <span class=\"comment\">#创建ProxyHandler</span></span><br><span class=\"line\">    proxy_support = request.ProxyHandler(proxy)</span><br><span class=\"line\">    <span class=\"comment\">#创建Opener</span></span><br><span class=\"line\">    opener = request.build_opener(proxy_support)</span><br><span class=\"line\">    <span class=\"comment\">#添加User Angent</span></span><br><span class=\"line\">    opener.addheaders = [(<span class=\"string\">'User-Agent'</span>,<span class=\"string\">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'</span>)]</span><br><span class=\"line\">    <span class=\"comment\">#安装OPener</span></span><br><span class=\"line\">    request.install_opener(opener)</span><br><span class=\"line\">    <span class=\"comment\">#使用自己安装好的Opener</span></span><br><span class=\"line\">    response = request.urlopen(url)</span><br><span class=\"line\">    <span class=\"comment\">#读取相应信息并解码</span></span><br><span class=\"line\">    html = response.read().decode(<span class=\"string\">\"utf-8\"</span>)</span><br><span class=\"line\">    <span class=\"comment\">#打印信息</span></span><br><span class=\"line\">    print(html)</span><br></pre></td></tr></table></figure></p>\n<p>运行结果如下：<br><img src=\"http://img.blog.csdn.net/20170303124823038?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"7\"></p>\n<p>从上图可以看出，访问的IP已经伪装成了106.46.136.112。</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"说在前面\"><a href=\"#说在前面\" class=\"headerlink\" title=\"说在前面\"></a>说在前面</h2><p>urllib_test10.py已经无法爬取了，因为原网站已经添加了防爬虫机制</p>\n<h2 id=\"一-、为何要设置User-Agent\"><a href=\"#一-、为何要设置User-Agent\" class=\"headerlink\" title=\"(一)、为何要设置User Agent\"></a>(一)、为何要设置User Agent</h2><p>有一些网站不喜欢被爬虫程序访问，所以会检测连接对象，如果是爬虫程序，也就是非人点击访问，它就会不让你继续访问，所以为了要让程序可以正常运行，需要隐藏自己的爬虫程序的身份。此时，我们就可以通过设置User Agent的来达到隐藏身份的目的，User Agent的中文名为用户代理，简称UA。</p>\n<p>User Agent存放于Headers中，服务器就是通过查看Headers中的User Agent来判断是谁在访问。在Python中，如果不设置User Agent，程序将使用默认的参数，那么这个User Agent就会有Python的字样，如果服务器检查User Agent，那么没有设置User Agent的Python程序将无法正常访问网站。</p>\n<p>Python允许我们修改这个User Agent来模拟浏览器访问，它的强大毋庸置疑。<br>","more":"</p>\n<h2 id=\"二-、常见的User-Agent\"><a href=\"#二-、常见的User-Agent\" class=\"headerlink\" title=\"(二)、常见的User Agent\"></a>(二)、常见的User Agent</h2><h3 id=\"1-Android\"><a href=\"#1-Android\" class=\"headerlink\" title=\"(1).Android\"></a>(1).Android</h3><ul>\n<li>Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19</li>\n<li>Mozilla/5.0 (Linux; U; Android 4.0.4; en-gb; GT-I9300 Build/IMM76D) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30</li>\n<li>Mozilla/5.0 (Linux; U; Android 2.2; en-gb; GT-P1000 Build/FROYO) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1</li>\n</ul>\n<h3 id=\"2-Firefox\"><a href=\"#2-Firefox\" class=\"headerlink\" title=\"(2).Firefox\"></a>(2).Firefox</h3><ul>\n<li>Mozilla/5.0 (Windows NT 6.2; WOW64; rv:21.0) Gecko/20100101 Firefox/21.0</li>\n<li>Mozilla/5.0 (Android; Mobile; rv:14.0) Gecko/14.0 Firefox/14.0</li>\n</ul>\n<h3 id=\"3-Google-Chrome\"><a href=\"#3-Google-Chrome\" class=\"headerlink\" title=\"(3).Google Chrome\"></a>(3).Google Chrome</h3><ul>\n<li>Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36</li>\n<li>Mozilla/5.0 (Linux; Android 4.0.4; Galaxy Nexus Build/IMM76B) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.133 Mobile Safari/535.19</li>\n</ul>\n<h3 id=\"4-iOS\"><a href=\"#4-iOS\" class=\"headerlink\" title=\"(4).iOS\"></a>(4).iOS</h3><ul>\n<li>Mozilla/5.0 (iPad; CPU OS 5_0 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9A334 Safari/7534.48.3</li>\n<li>Mozilla/5.0 (iPod; U; CPU like Mac OS X; en) AppleWebKit/420.1 (KHTML, like Gecko) Version/3.0 Mobile/3A101a Safari/419.3</li>\n</ul>\n<p>上面列举了Andriod、Firefox、Google Chrome、iOS的一些User Agent，直接copy就能用。</p>\n<h2 id=\"三-、设置User-Agent的方法\"><a href=\"#三-、设置User-Agent的方法\" class=\"headerlink\" title=\"(三)、设置User Agent的方法\"></a>(三)、设置User Agent的方法</h2><p>先看下urllib.request.Request()</p>\n<p> <img src=\"http://img.blog.csdn.net/20170303123244632?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"1\"></p>\n<p>从上图可以看出，在创建Request对象的时候，可以传入headers参数。因此，想要设置User Agent，有两种方法：</p>\n<ul>\n<li><p>1.在创建Request对象的时候，填入headers参数(包含User Agent信息)，这个Headers参数要求为字典；</p>\n</li>\n<li><p>2.在创建Request对象的时候不添加headers参数，在创建完成之后，使用add_header()的方法，添加headers。</p>\n</li>\n</ul>\n<h3 id=\"方法一\"><a href=\"#方法一\" class=\"headerlink\" title=\"方法一\"></a>方法一</h3><p>创建文件urllib_test08.py，使用上面提到的Android的第一个User Agent，在创建Request对象的时候传入headers参数，编写代码如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    url = <span class=\"string\">'http://www.csdn.net/'</span></span><br><span class=\"line\">    head = &#123;&#125;</span><br><span class=\"line\">    <span class=\"comment\">#写入User Agent信息</span></span><br><span class=\"line\">    head[<span class=\"string\">'User-Agent'</span>] = <span class=\"string\">'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19'</span></span><br><span class=\"line\">    <span class=\"comment\">#创建Request对象</span></span><br><span class=\"line\">    req = request.Request(url, headers=head)</span><br><span class=\"line\">    <span class=\"comment\">#传入创建好的Request对象</span></span><br><span class=\"line\">    response = request.urlopen(req)</span><br><span class=\"line\">    <span class=\"comment\">#读取响应信息并解码</span></span><br><span class=\"line\">    html = response.read().decode(<span class=\"string\">'utf-8'</span>)</span><br><span class=\"line\">    <span class=\"comment\">#打印信息</span></span><br><span class=\"line\">    print(html)</span><br></pre></td></tr></table></figure></p>\n<p>运行结果如下：</p>\n<p> <img src=\"http://img.blog.csdn.net/20170303123738649?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"2\"></p>\n<h3 id=\"方法二\"><a href=\"#方法二\" class=\"headerlink\" title=\"方法二\"></a>方法二</h3><p>创建文件urllib_test09.py，使用上面提到的Android的第一个User Agent，在创建Request对象时不传入headers参数，创建之后使用add_header()方法，添加headers，编写代码如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    <span class=\"comment\">#以CSDN为例，CSDN不更改User Agent是无法访问的</span></span><br><span class=\"line\">    url = <span class=\"string\">'http://www.csdn.net/'</span></span><br><span class=\"line\">    <span class=\"comment\">#创建Request对象</span></span><br><span class=\"line\">    req = request.Request(url)</span><br><span class=\"line\">    <span class=\"comment\">#传入headers</span></span><br><span class=\"line\">    req.add_header(<span class=\"string\">'User-Agent'</span>, <span class=\"string\">'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19'</span>)</span><br><span class=\"line\">    <span class=\"comment\">#传入创建好的Request对象</span></span><br><span class=\"line\">    response = request.urlopen(req)</span><br><span class=\"line\">    <span class=\"comment\">#读取响应信息并解码</span></span><br><span class=\"line\">    html = response.read().decode(<span class=\"string\">'utf-8'</span>)</span><br><span class=\"line\">    <span class=\"comment\">#打印信息</span></span><br><span class=\"line\">    print(html)</span><br></pre></td></tr></table></figure></p>\n<p>运行结果和上一个方法是一样的。</p>\n<h2 id=\"四-、IP代理的使用\"><a href=\"#四-、IP代理的使用\" class=\"headerlink\" title=\"(四)、IP代理的使用\"></a>(四)、IP代理的使用</h2><h3 id=\"1-为何使用IP代理\"><a href=\"#1-为何使用IP代理\" class=\"headerlink\" title=\"(1).为何使用IP代理\"></a>(1).为何使用IP代理</h3><p>User Agent已经设置好了，但是还应该考虑一个问题，程序的运行速度是很快的，如果我们利用一个爬虫程序在网站爬取东西，一个固定IP的访问频率就会很高，这不符合人为操作的标准，因为人操作不可能在几ms内，进行如此频繁的访问。所以一些网站会设置一个IP访问频率的阈值，如果一个IP访问频率超过这个阈值，说明这个不是人在访问，而是一个爬虫程序。</p>\n<h3 id=\"2-一般步骤说明\"><a href=\"#2-一般步骤说明\" class=\"headerlink\" title=\"(2).一般步骤说明\"></a>(2).一般步骤说明</h3><p>一个很简单的解决办法就是设置延时，但是这显然不符合爬虫快速爬取信息的目的，所以另一种更好的方法就是使用IP代理。使用代理的步骤：</p>\n<p><strong>(1)</strong> 调用urlib.request.ProxyHandler()，proxies参数为一个字典。</p>\n<p> <img src=\"http://img.blog.csdn.net/20170303124421012?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"4\"></p>\n<p><strong>(2)</strong> 创建Opener(类似于urlopen，这个代开方式是我们自己定制的)</p>\n<p><img src=\"http://img.blog.csdn.net/20170303124447169?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"5\"></p>\n<p><strong>(3)</strong> 安装Opener</p>\n<p><img src=\"http://img.blog.csdn.net/20170303124507044?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"引用容\"></p>\n<p>使用install_opener方法之后，会将程序默认的urlopen方法替换掉。也就是说，如果使用install_opener之后，在该文件中，再次调用urlopen会使用自己创建好的opener。如果不想替换掉，只是想临时使用一下，可以使用opener.open(url)，这样就不会对程序默认的urlopen有影响。</p>\n<h3 id=\"3-代理IP选取\"><a href=\"#3-代理IP选取\" class=\"headerlink\" title=\"(3).代理IP选取\"></a>(3).代理IP选取</h3><p>在写代码之前，先在代理IP网站选好一个IP地址，推荐西刺代理IP。</p>\n<p>URL：<a href=\"http://www.xicidaili.com/\" target=\"_blank\" rel=\"noopener\">http://www.xicidaili.com/</a></p>\n<p>注意：当然也可以写个正则表达式从网站直接爬取IP，但是要记住不要太频繁爬取，加个延时什么的，太频繁给服务器带来压力了，服务器会直接把你block，不让你访问的，我就被封了两天。</p>\n<p>从西刺网站选出信号好的IP，我的选择如下：(106.46.136.112:808)</p>\n<p> <img src=\"http://img.blog.csdn.net/20170303124651091?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"6\"></p>\n<p>编写代码访问<a href=\"http://www.whatismyip.com.tw/\" target=\"_blank\" rel=\"noopener\">http://www.whatismyip.com.tw/</a>，该网站是测试自己IP为多少的网址，服务器会返回访问者的IP。</p>\n<h3 id=\"4-代码实例\"><a href=\"#4-代码实例\" class=\"headerlink\" title=\"(4).代码实例\"></a>(4).代码实例</h3><p>创建文件urllib_test10.py，编写代码如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    <span class=\"comment\">#访问网址</span></span><br><span class=\"line\">    url = <span class=\"string\">'http://www.whatismyip.com.tw/'</span></span><br><span class=\"line\">    <span class=\"comment\">#这是代理IP</span></span><br><span class=\"line\">    proxy = &#123;<span class=\"string\">'http'</span>:<span class=\"string\">'106.46.136.112:808'</span>&#125;</span><br><span class=\"line\">    <span class=\"comment\">#创建ProxyHandler</span></span><br><span class=\"line\">    proxy_support = request.ProxyHandler(proxy)</span><br><span class=\"line\">    <span class=\"comment\">#创建Opener</span></span><br><span class=\"line\">    opener = request.build_opener(proxy_support)</span><br><span class=\"line\">    <span class=\"comment\">#添加User Angent</span></span><br><span class=\"line\">    opener.addheaders = [(<span class=\"string\">'User-Agent'</span>,<span class=\"string\">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'</span>)]</span><br><span class=\"line\">    <span class=\"comment\">#安装OPener</span></span><br><span class=\"line\">    request.install_opener(opener)</span><br><span class=\"line\">    <span class=\"comment\">#使用自己安装好的Opener</span></span><br><span class=\"line\">    response = request.urlopen(url)</span><br><span class=\"line\">    <span class=\"comment\">#读取相应信息并解码</span></span><br><span class=\"line\">    html = response.read().decode(<span class=\"string\">\"utf-8\"</span>)</span><br><span class=\"line\">    <span class=\"comment\">#打印信息</span></span><br><span class=\"line\">    print(html)</span><br></pre></td></tr></table></figure></p>\n<p>运行结果如下：<br><img src=\"http://img.blog.csdn.net/20170303124823038?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"7\"></p>\n<p>从上图可以看出，访问的IP已经伪装成了106.46.136.112。</p>"},{"title":"从零开始学爬虫-05","date":"2018-02-27T17:17:37.000Z","_content":"\n## 为什么要使用Cookie\n\nCookie，指某些网站为了辨别用户身份、进行session跟踪而储存在用户本地终端上的数据（通常经过加密)。   \n比如说有些网站需要登录后才能访问某个页面，在登录之前，你想抓取某个页面内容，登陆前与登陆后是不同的，或者不允许的。   \n使用Cookie和使用代理IP一样，也需要创建一个自己的opener。在HTTP包中，提供了cookiejar模块，用于提供对Cookie的支持。\n\n![](http://img.blog.csdn.net/20170409144243654?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n<!-- more -->\n\nhttp.cookiejar功能强大，我们可以利用本模块的CookieJar类的对象来捕获cookie并在后续连接请求时重新发送，比如可以实现模拟登录功能。该模块主要的对象有CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar。\n\n**它们的关系：** CookieJar–派生–>FileCookieJar–派生–>MozillaCookieJar和LWPCookieJar\n\n**工作原理：** 创建一个带有cookie的opener，在访问登录的URL时，将登录后的cookie保存下来，然后利用这个cookie来访问其他网址。查看登录之后才能看到的信息。\n\n同样，我们以实例进行讲解，爬取伯乐在线的面向对象的漂亮MM的邮箱联系方式。\n\n## 实战\n\n### (1).背景介绍\n\n在伯乐在线有这么一个有趣的模块，面向对象，它说白了就是提供了一个程序员(媛)网上相亲的平台。\n\n**URL:** [http://date.jobbole.com/](http://date.jobbole.com/)\n\n它的样子是这样的：\n\n![](http://img.blog.csdn.net/20170409144753813?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n可以看到，这里有很多的相亲贴，随便点进去就会有网上相亲MM的详细信息，想获取MM的联系方式，需要积分，积分可以通过签到的方式获取。如果没有登陆账户，获取联系方式的地方是这个样子的：\n\n![](http://img.blog.csdn.net/20170409144912938?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n如果登陆了账号，获取联系方式的地方是这个样子的：\n\n![](http://img.blog.csdn.net/20170409144955289?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n想要爬取MM的联系邮箱，就需要用到我们本次讲到的知识，Cookie的使用。当然，首先你积分也得够。\n\n在讲解之前，推荐一款抓包工具–Fiddler，可以在Google Chrome的Google商店下载这个插件，它的样子是这样的：\n\n![](http://img.blog.csdn.net/20170409145106869?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n可以看到，通过这个插件，我们可以很容易找到Post的Form Data等信息，很方便，当然也可以用之前讲得浏览器审查元素的方式查看这些信息。\n\n### (2).过程分析\n\n在伯乐在线首页点击登陆的按钮，Fiddler的抓包内容如下：\n\n![](http://img.blog.csdn.net/20170409145240590?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n从上图可以看出，真正请求的url是\n\n [http://www.jobbole.com/wp-admin/admin-ajax.php](http://www.jobbole.com/wp-admin/admin-ajax.php)\n\nForm Data的内容记住，这些是我们编程需要用到的。user_login是用户名，user_pass是用户密码。\n\n在点击取得联系邮箱按钮的时候，Fiddler的抓包内容如下：\n\n![](http://img.blog.csdn.net/20170409145403065?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n从上图可以看出，此刻真正请求的url是\n\n [http://date.jobbole.com/wp-admin/admin-ajax.php](http://date.jobbole.com/wp-admin/admin-ajax.php)\n\n同样Form Data中内容要记下来。postId是每个帖子的id。例如，打开一个相亲贴，它的URL是[http://date.jobbole.com/4128/](http://date.jobbole.com/4128/)，那么它的这个postId就是4128。为了简化程序，这里就不讲解如何自动获取这个postId了，本实例直接指定postId。如果想要自动获取，可以使用beautifulsoup解析[http://date.jobbole.com/](http://date.jobbole.com/)返回的信息。beautifulsoup的使用。有机会的话，会在后面的爬虫笔记中进行讲解。\n\n### (3).测试\n\n**1)将Cookie保存到变量中**\n\n首先，我们先利用CookieJar对象实现获取cookie的功能，存储到变量中，先来感受一下,执行文件cookie01.py：\n\n```python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\nfrom http import cookiejar\n\nif __name__ == '__main__':\n    #声明一个CookieJar对象实例来保存cookie\n    cookie = cookiejar.CookieJar()\n    #利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler\n    handler=request.HTTPCookieProcessor(cookie)\n    #通过CookieHandler创建opener\n    opener = request.build_opener(handler)\n    #此处的open方法打开网页\n    response = opener.open('http://www.baidu.com')\n    #打印cookie信息\n    for item in cookie:\n        print('Name = %s' % item.name)\n        print('Value = %s' % item.value)\n```\n\n我们使用以上方法将cookie保存到变量中，然后打印出了cookie中的值，运行结果如下:\n\n![](http://img.blog.csdn.net/20170409145652613?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n**2)保存Cookie到文件**\n\n在上面的方法中，我们将cookie保存到了cookie这个变量中，如果我们想将cookie保存到文件中该怎么做呢？方便以后直接读取文件使用，这时，我们就要用到FileCookieJar这个对象了，在这里我们使用它的子类MozillaCookieJar来实现Cookie的保存，编写代码如下,文件为cookie02.py：\n```python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\nfrom http import cookiejar\n\nif __name__ == '__main__':\n\n    #设置保存cookie的文件，同级目录下的cookie.txt\n    filename = 'cookie.txt'\n    #声明一个MozillaCookieJar对象实例来保存cookie，之后写入文件\n    cookie = cookiejar.MozillaCookieJar(filename)\n    #利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler\n    handler=request.HTTPCookieProcessor(cookie)\n    #通过CookieHandler创建opener\n    opener = request.build_opener(handler)\n    #此处的open方法打开网页\n    response = opener.open('http://www.baidu.com')\n    #保存cookie到文件\n    cookie.save(ignore_discard=True, ignore_expires=True)\n```\n\ncookie.save的参数说明：\n\n  * ignore_discard的意思是即使cookies将被丢弃也将它保存下来；\n\n  * ignore_expires的意思是如果在该文件中cookies已经存在，则覆盖原文件写入。\n\n在这里，我们将这两个全部设置为True。\n\n运行之后，cookies将被保存到cookie.txt文件中。我们可以查看自己查看下cookie.txt这个文件的内容。\n\n**3)从文件中获取Cookie并访问**\n\n我们已经做到把Cookie保存到文件中了，如果以后想使用，可以利用下面的方法来读取cookie并访问网站，感受一下cookie03.py：\n```python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\nfrom http import cookiejar\n\nif __name__ == '__main__':\n    #设置保存cookie的文件的文件名,相对路径,也就是同级目录下\n    filename = 'cookie.txt'\n    #创建MozillaCookieJar实例对象\n    cookie = cookiejar.MozillaCookieJar()\n    #从文件中读取cookie内容到变量\n    cookie.load(filename, ignore_discard=True, ignore_expires=True)\n    #利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler\n    handler=request.HTTPCookieProcessor(cookie)\n    #通过CookieHandler创建opener\n    opener = request.build_opener(handler)\n    #此用opener的open方法打开网页\n    response = opener.open('http://www.baidu.com')\n    #打印信息\n    print(response.read().decode('utf-8'))\n```\n\n了解到以上内容，我们那就可以开始正式编写模拟登陆伯乐在线的程序了。同时，我们也可以获取相亲MM的联系方式。\n\n### (4).编写代码\n\n我们利用CookieJar对象实现获取cookie的功能，存储到变量中。然后使用这个cookie变量创建opener，使用这个设置好cookie的opener即可模拟登陆，同笔记四中讲到的IP代理的使用方法类似。\n\n创建cookie_test.py文件，编写代码如下：\n```python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\nfrom urllib import error\nfrom urllib import parse\nfrom http import cookiejar\n\nif __name__ == '__main__':\n    #登陆地址\n    login_url = 'http://www.jobbole.com/wp-admin/admin-ajax.php'    \n    #User-Agent信息                   \n    user_agent = r'Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36'\n    #Headers信息\n    head = {'User-Agnet': user_agent, 'Connection': 'keep-alive'}\n    #登陆Form_Data信息\n    Login_Data = {}\n    Login_Data['action'] = 'user_login'\n    Login_Data['redirect_url'] = 'http://www.jobbole.com/'\n    Login_Data['remember_me'] = '0'         #是否一个月内自动登陆\n    Login_Data['user_login'] = '********'       #改成你自己的用户名\n    Login_Data['user_pass'] = '********'        #改成你自己的密码\n    #使用urlencode方法转换标准格式\n    logingpostdata = parse.urlencode(Login_Data).encode('utf-8')\n    #声明一个CookieJar对象实例来保存cookie\n    cookie = cookiejar.CookieJar()\n    #利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler\n    cookie_support = request.HTTPCookieProcessor(cookie)\n    #通过CookieHandler创建opener\n    opener = request.build_opener(cookie_support)\n    #创建Request对象\n    req1 = request.Request(url=login_url, data=logingpostdata, headers=head)\n\n    #面向对象地址\n    date_url = 'http://date.jobbole.com/wp-admin/admin-ajax.php'\n    #面向对象\n    Date_Data = {}\n    Date_Data['action'] = 'get_date_contact'\n    Date_Data['postId'] = '4128'\n    #使用urlencode方法转换标准格式\n    datepostdata = parse.urlencode(Date_Data).encode('utf-8')\n    req2 = request.Request(url=date_url, data=datepostdata, headers=head)\n    try:\n        #使用自己创建的opener的open方法\n        response1 = opener.open(req1)\n        response2 = opener.open(req2)\n        html = response2.read().decode('utf-8')\n        index = html.find('jb_contact_email')\n        #打印查询结果\n        print('联系邮箱:%s' % html[index+19:-2])\n\n    except error.URLError as e:\n        if hasattr(e, 'code'):\n            print(\"HTTPError:%d\" % e.code)\n        elif hasattr(e, 'reason'):\n            print(\"URLError:%s\" % e.reason)\n```\n\n\n### (5).运行结果如下\n\n![](http://img.blog.csdn.net/20170409150252854?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n。\n","source":"_posts/从零开始学爬虫-05.md","raw":"---\ntitle: 从零开始学爬虫-05\ndate: 2018-02-28 01:17:37\ntags:\n  - Spider\n  - Urllib\n  - Python3\ncategories:\n  - Spider\n  - Urllib\n---\n\n## 为什么要使用Cookie\n\nCookie，指某些网站为了辨别用户身份、进行session跟踪而储存在用户本地终端上的数据（通常经过加密)。   \n比如说有些网站需要登录后才能访问某个页面，在登录之前，你想抓取某个页面内容，登陆前与登陆后是不同的，或者不允许的。   \n使用Cookie和使用代理IP一样，也需要创建一个自己的opener。在HTTP包中，提供了cookiejar模块，用于提供对Cookie的支持。\n\n![](http://img.blog.csdn.net/20170409144243654?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n<!-- more -->\n\nhttp.cookiejar功能强大，我们可以利用本模块的CookieJar类的对象来捕获cookie并在后续连接请求时重新发送，比如可以实现模拟登录功能。该模块主要的对象有CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar。\n\n**它们的关系：** CookieJar–派生–>FileCookieJar–派生–>MozillaCookieJar和LWPCookieJar\n\n**工作原理：** 创建一个带有cookie的opener，在访问登录的URL时，将登录后的cookie保存下来，然后利用这个cookie来访问其他网址。查看登录之后才能看到的信息。\n\n同样，我们以实例进行讲解，爬取伯乐在线的面向对象的漂亮MM的邮箱联系方式。\n\n## 实战\n\n### (1).背景介绍\n\n在伯乐在线有这么一个有趣的模块，面向对象，它说白了就是提供了一个程序员(媛)网上相亲的平台。\n\n**URL:** [http://date.jobbole.com/](http://date.jobbole.com/)\n\n它的样子是这样的：\n\n![](http://img.blog.csdn.net/20170409144753813?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n可以看到，这里有很多的相亲贴，随便点进去就会有网上相亲MM的详细信息，想获取MM的联系方式，需要积分，积分可以通过签到的方式获取。如果没有登陆账户，获取联系方式的地方是这个样子的：\n\n![](http://img.blog.csdn.net/20170409144912938?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n如果登陆了账号，获取联系方式的地方是这个样子的：\n\n![](http://img.blog.csdn.net/20170409144955289?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n想要爬取MM的联系邮箱，就需要用到我们本次讲到的知识，Cookie的使用。当然，首先你积分也得够。\n\n在讲解之前，推荐一款抓包工具–Fiddler，可以在Google Chrome的Google商店下载这个插件，它的样子是这样的：\n\n![](http://img.blog.csdn.net/20170409145106869?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n可以看到，通过这个插件，我们可以很容易找到Post的Form Data等信息，很方便，当然也可以用之前讲得浏览器审查元素的方式查看这些信息。\n\n### (2).过程分析\n\n在伯乐在线首页点击登陆的按钮，Fiddler的抓包内容如下：\n\n![](http://img.blog.csdn.net/20170409145240590?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n从上图可以看出，真正请求的url是\n\n [http://www.jobbole.com/wp-admin/admin-ajax.php](http://www.jobbole.com/wp-admin/admin-ajax.php)\n\nForm Data的内容记住，这些是我们编程需要用到的。user_login是用户名，user_pass是用户密码。\n\n在点击取得联系邮箱按钮的时候，Fiddler的抓包内容如下：\n\n![](http://img.blog.csdn.net/20170409145403065?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n从上图可以看出，此刻真正请求的url是\n\n [http://date.jobbole.com/wp-admin/admin-ajax.php](http://date.jobbole.com/wp-admin/admin-ajax.php)\n\n同样Form Data中内容要记下来。postId是每个帖子的id。例如，打开一个相亲贴，它的URL是[http://date.jobbole.com/4128/](http://date.jobbole.com/4128/)，那么它的这个postId就是4128。为了简化程序，这里就不讲解如何自动获取这个postId了，本实例直接指定postId。如果想要自动获取，可以使用beautifulsoup解析[http://date.jobbole.com/](http://date.jobbole.com/)返回的信息。beautifulsoup的使用。有机会的话，会在后面的爬虫笔记中进行讲解。\n\n### (3).测试\n\n**1)将Cookie保存到变量中**\n\n首先，我们先利用CookieJar对象实现获取cookie的功能，存储到变量中，先来感受一下,执行文件cookie01.py：\n\n```python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\nfrom http import cookiejar\n\nif __name__ == '__main__':\n    #声明一个CookieJar对象实例来保存cookie\n    cookie = cookiejar.CookieJar()\n    #利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler\n    handler=request.HTTPCookieProcessor(cookie)\n    #通过CookieHandler创建opener\n    opener = request.build_opener(handler)\n    #此处的open方法打开网页\n    response = opener.open('http://www.baidu.com')\n    #打印cookie信息\n    for item in cookie:\n        print('Name = %s' % item.name)\n        print('Value = %s' % item.value)\n```\n\n我们使用以上方法将cookie保存到变量中，然后打印出了cookie中的值，运行结果如下:\n\n![](http://img.blog.csdn.net/20170409145652613?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n**2)保存Cookie到文件**\n\n在上面的方法中，我们将cookie保存到了cookie这个变量中，如果我们想将cookie保存到文件中该怎么做呢？方便以后直接读取文件使用，这时，我们就要用到FileCookieJar这个对象了，在这里我们使用它的子类MozillaCookieJar来实现Cookie的保存，编写代码如下,文件为cookie02.py：\n```python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\nfrom http import cookiejar\n\nif __name__ == '__main__':\n\n    #设置保存cookie的文件，同级目录下的cookie.txt\n    filename = 'cookie.txt'\n    #声明一个MozillaCookieJar对象实例来保存cookie，之后写入文件\n    cookie = cookiejar.MozillaCookieJar(filename)\n    #利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler\n    handler=request.HTTPCookieProcessor(cookie)\n    #通过CookieHandler创建opener\n    opener = request.build_opener(handler)\n    #此处的open方法打开网页\n    response = opener.open('http://www.baidu.com')\n    #保存cookie到文件\n    cookie.save(ignore_discard=True, ignore_expires=True)\n```\n\ncookie.save的参数说明：\n\n  * ignore_discard的意思是即使cookies将被丢弃也将它保存下来；\n\n  * ignore_expires的意思是如果在该文件中cookies已经存在，则覆盖原文件写入。\n\n在这里，我们将这两个全部设置为True。\n\n运行之后，cookies将被保存到cookie.txt文件中。我们可以查看自己查看下cookie.txt这个文件的内容。\n\n**3)从文件中获取Cookie并访问**\n\n我们已经做到把Cookie保存到文件中了，如果以后想使用，可以利用下面的方法来读取cookie并访问网站，感受一下cookie03.py：\n```python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\nfrom http import cookiejar\n\nif __name__ == '__main__':\n    #设置保存cookie的文件的文件名,相对路径,也就是同级目录下\n    filename = 'cookie.txt'\n    #创建MozillaCookieJar实例对象\n    cookie = cookiejar.MozillaCookieJar()\n    #从文件中读取cookie内容到变量\n    cookie.load(filename, ignore_discard=True, ignore_expires=True)\n    #利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler\n    handler=request.HTTPCookieProcessor(cookie)\n    #通过CookieHandler创建opener\n    opener = request.build_opener(handler)\n    #此用opener的open方法打开网页\n    response = opener.open('http://www.baidu.com')\n    #打印信息\n    print(response.read().decode('utf-8'))\n```\n\n了解到以上内容，我们那就可以开始正式编写模拟登陆伯乐在线的程序了。同时，我们也可以获取相亲MM的联系方式。\n\n### (4).编写代码\n\n我们利用CookieJar对象实现获取cookie的功能，存储到变量中。然后使用这个cookie变量创建opener，使用这个设置好cookie的opener即可模拟登陆，同笔记四中讲到的IP代理的使用方法类似。\n\n创建cookie_test.py文件，编写代码如下：\n```python\n# -*- coding: UTF-8 -*-\nfrom urllib import request\nfrom urllib import error\nfrom urllib import parse\nfrom http import cookiejar\n\nif __name__ == '__main__':\n    #登陆地址\n    login_url = 'http://www.jobbole.com/wp-admin/admin-ajax.php'    \n    #User-Agent信息                   \n    user_agent = r'Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36'\n    #Headers信息\n    head = {'User-Agnet': user_agent, 'Connection': 'keep-alive'}\n    #登陆Form_Data信息\n    Login_Data = {}\n    Login_Data['action'] = 'user_login'\n    Login_Data['redirect_url'] = 'http://www.jobbole.com/'\n    Login_Data['remember_me'] = '0'         #是否一个月内自动登陆\n    Login_Data['user_login'] = '********'       #改成你自己的用户名\n    Login_Data['user_pass'] = '********'        #改成你自己的密码\n    #使用urlencode方法转换标准格式\n    logingpostdata = parse.urlencode(Login_Data).encode('utf-8')\n    #声明一个CookieJar对象实例来保存cookie\n    cookie = cookiejar.CookieJar()\n    #利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler\n    cookie_support = request.HTTPCookieProcessor(cookie)\n    #通过CookieHandler创建opener\n    opener = request.build_opener(cookie_support)\n    #创建Request对象\n    req1 = request.Request(url=login_url, data=logingpostdata, headers=head)\n\n    #面向对象地址\n    date_url = 'http://date.jobbole.com/wp-admin/admin-ajax.php'\n    #面向对象\n    Date_Data = {}\n    Date_Data['action'] = 'get_date_contact'\n    Date_Data['postId'] = '4128'\n    #使用urlencode方法转换标准格式\n    datepostdata = parse.urlencode(Date_Data).encode('utf-8')\n    req2 = request.Request(url=date_url, data=datepostdata, headers=head)\n    try:\n        #使用自己创建的opener的open方法\n        response1 = opener.open(req1)\n        response2 = opener.open(req2)\n        html = response2.read().decode('utf-8')\n        index = html.find('jb_contact_email')\n        #打印查询结果\n        print('联系邮箱:%s' % html[index+19:-2])\n\n    except error.URLError as e:\n        if hasattr(e, 'code'):\n            print(\"HTTPError:%d\" % e.code)\n        elif hasattr(e, 'reason'):\n            print(\"URLError:%s\" % e.reason)\n```\n\n\n### (5).运行结果如下\n\n![](http://img.blog.csdn.net/20170409150252854?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n。\n","slug":"从零开始学爬虫-05","published":1,"updated":"2018-02-27T17:22:51.289Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cje6mdg0n000vrcotuq9jewi1","content":"<h2 id=\"为什么要使用Cookie\"><a href=\"#为什么要使用Cookie\" class=\"headerlink\" title=\"为什么要使用Cookie\"></a>为什么要使用Cookie</h2><p>Cookie，指某些网站为了辨别用户身份、进行session跟踪而储存在用户本地终端上的数据（通常经过加密)。<br>比如说有些网站需要登录后才能访问某个页面，在登录之前，你想抓取某个页面内容，登陆前与登陆后是不同的，或者不允许的。<br>使用Cookie和使用代理IP一样，也需要创建一个自己的opener。在HTTP包中，提供了cookiejar模块，用于提供对Cookie的支持。</p>\n<p><img src=\"http://img.blog.csdn.net/20170409144243654?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"\"><br><a id=\"more\"></a></p>\n<p>http.cookiejar功能强大，我们可以利用本模块的CookieJar类的对象来捕获cookie并在后续连接请求时重新发送，比如可以实现模拟登录功能。该模块主要的对象有CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar。</p>\n<p><strong>它们的关系：</strong> CookieJar–派生–&gt;FileCookieJar–派生–&gt;MozillaCookieJar和LWPCookieJar</p>\n<p><strong>工作原理：</strong> 创建一个带有cookie的opener，在访问登录的URL时，将登录后的cookie保存下来，然后利用这个cookie来访问其他网址。查看登录之后才能看到的信息。</p>\n<p>同样，我们以实例进行讲解，爬取伯乐在线的面向对象的漂亮MM的邮箱联系方式。</p>\n<h2 id=\"实战\"><a href=\"#实战\" class=\"headerlink\" title=\"实战\"></a>实战</h2><h3 id=\"1-背景介绍\"><a href=\"#1-背景介绍\" class=\"headerlink\" title=\"(1).背景介绍\"></a>(1).背景介绍</h3><p>在伯乐在线有这么一个有趣的模块，面向对象，它说白了就是提供了一个程序员(媛)网上相亲的平台。</p>\n<p><strong>URL:</strong> <a href=\"http://date.jobbole.com/\" target=\"_blank\" rel=\"noopener\">http://date.jobbole.com/</a></p>\n<p>它的样子是这样的：</p>\n<p><img src=\"http://img.blog.csdn.net/20170409144753813?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"\"></p>\n<p>可以看到，这里有很多的相亲贴，随便点进去就会有网上相亲MM的详细信息，想获取MM的联系方式，需要积分，积分可以通过签到的方式获取。如果没有登陆账户，获取联系方式的地方是这个样子的：</p>\n<p><img src=\"http://img.blog.csdn.net/20170409144912938?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"\"></p>\n<p>如果登陆了账号，获取联系方式的地方是这个样子的：</p>\n<p><img src=\"http://img.blog.csdn.net/20170409144955289?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"\"></p>\n<p>想要爬取MM的联系邮箱，就需要用到我们本次讲到的知识，Cookie的使用。当然，首先你积分也得够。</p>\n<p>在讲解之前，推荐一款抓包工具–Fiddler，可以在Google Chrome的Google商店下载这个插件，它的样子是这样的：</p>\n<p><img src=\"http://img.blog.csdn.net/20170409145106869?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"\"></p>\n<p>可以看到，通过这个插件，我们可以很容易找到Post的Form Data等信息，很方便，当然也可以用之前讲得浏览器审查元素的方式查看这些信息。</p>\n<h3 id=\"2-过程分析\"><a href=\"#2-过程分析\" class=\"headerlink\" title=\"(2).过程分析\"></a>(2).过程分析</h3><p>在伯乐在线首页点击登陆的按钮，Fiddler的抓包内容如下：</p>\n<p><img src=\"http://img.blog.csdn.net/20170409145240590?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"\"></p>\n<p>从上图可以看出，真正请求的url是</p>\n<p> <a href=\"http://www.jobbole.com/wp-admin/admin-ajax.php\" target=\"_blank\" rel=\"noopener\">http://www.jobbole.com/wp-admin/admin-ajax.php</a></p>\n<p>Form Data的内容记住，这些是我们编程需要用到的。user_login是用户名，user_pass是用户密码。</p>\n<p>在点击取得联系邮箱按钮的时候，Fiddler的抓包内容如下：</p>\n<p><img src=\"http://img.blog.csdn.net/20170409145403065?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"\"></p>\n<p>从上图可以看出，此刻真正请求的url是</p>\n<p> <a href=\"http://date.jobbole.com/wp-admin/admin-ajax.php\" target=\"_blank\" rel=\"noopener\">http://date.jobbole.com/wp-admin/admin-ajax.php</a></p>\n<p>同样Form Data中内容要记下来。postId是每个帖子的id。例如，打开一个相亲贴，它的URL是<a href=\"http://date.jobbole.com/4128/\" target=\"_blank\" rel=\"noopener\">http://date.jobbole.com/4128/</a>，那么它的这个postId就是4128。为了简化程序，这里就不讲解如何自动获取这个postId了，本实例直接指定postId。如果想要自动获取，可以使用beautifulsoup解析<a href=\"http://date.jobbole.com/\" target=\"_blank\" rel=\"noopener\">http://date.jobbole.com/</a>返回的信息。beautifulsoup的使用。有机会的话，会在后面的爬虫笔记中进行讲解。</p>\n<h3 id=\"3-测试\"><a href=\"#3-测试\" class=\"headerlink\" title=\"(3).测试\"></a>(3).测试</h3><p><strong>1)将Cookie保存到变量中</strong></p>\n<p>首先，我们先利用CookieJar对象实现获取cookie的功能，存储到变量中，先来感受一下,执行文件cookie01.py：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"><span class=\"keyword\">from</span> http <span class=\"keyword\">import</span> cookiejar</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    <span class=\"comment\">#声明一个CookieJar对象实例来保存cookie</span></span><br><span class=\"line\">    cookie = cookiejar.CookieJar()</span><br><span class=\"line\">    <span class=\"comment\">#利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler</span></span><br><span class=\"line\">    handler=request.HTTPCookieProcessor(cookie)</span><br><span class=\"line\">    <span class=\"comment\">#通过CookieHandler创建opener</span></span><br><span class=\"line\">    opener = request.build_opener(handler)</span><br><span class=\"line\">    <span class=\"comment\">#此处的open方法打开网页</span></span><br><span class=\"line\">    response = opener.open(<span class=\"string\">'http://www.baidu.com'</span>)</span><br><span class=\"line\">    <span class=\"comment\">#打印cookie信息</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> item <span class=\"keyword\">in</span> cookie:</span><br><span class=\"line\">        print(<span class=\"string\">'Name = %s'</span> % item.name)</span><br><span class=\"line\">        print(<span class=\"string\">'Value = %s'</span> % item.value)</span><br></pre></td></tr></table></figure>\n<p>我们使用以上方法将cookie保存到变量中，然后打印出了cookie中的值，运行结果如下:</p>\n<p><img src=\"http://img.blog.csdn.net/20170409145652613?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"\"></p>\n<p><strong>2)保存Cookie到文件</strong></p>\n<p>在上面的方法中，我们将cookie保存到了cookie这个变量中，如果我们想将cookie保存到文件中该怎么做呢？方便以后直接读取文件使用，这时，我们就要用到FileCookieJar这个对象了，在这里我们使用它的子类MozillaCookieJar来实现Cookie的保存，编写代码如下,文件为cookie02.py：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"><span class=\"keyword\">from</span> http <span class=\"keyword\">import</span> cookiejar</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">#设置保存cookie的文件，同级目录下的cookie.txt</span></span><br><span class=\"line\">    filename = <span class=\"string\">'cookie.txt'</span></span><br><span class=\"line\">    <span class=\"comment\">#声明一个MozillaCookieJar对象实例来保存cookie，之后写入文件</span></span><br><span class=\"line\">    cookie = cookiejar.MozillaCookieJar(filename)</span><br><span class=\"line\">    <span class=\"comment\">#利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler</span></span><br><span class=\"line\">    handler=request.HTTPCookieProcessor(cookie)</span><br><span class=\"line\">    <span class=\"comment\">#通过CookieHandler创建opener</span></span><br><span class=\"line\">    opener = request.build_opener(handler)</span><br><span class=\"line\">    <span class=\"comment\">#此处的open方法打开网页</span></span><br><span class=\"line\">    response = opener.open(<span class=\"string\">'http://www.baidu.com'</span>)</span><br><span class=\"line\">    <span class=\"comment\">#保存cookie到文件</span></span><br><span class=\"line\">    cookie.save(ignore_discard=<span class=\"keyword\">True</span>, ignore_expires=<span class=\"keyword\">True</span>)</span><br></pre></td></tr></table></figure></p>\n<p>cookie.save的参数说明：</p>\n<ul>\n<li><p>ignore_discard的意思是即使cookies将被丢弃也将它保存下来；</p>\n</li>\n<li><p>ignore_expires的意思是如果在该文件中cookies已经存在，则覆盖原文件写入。</p>\n</li>\n</ul>\n<p>在这里，我们将这两个全部设置为True。</p>\n<p>运行之后，cookies将被保存到cookie.txt文件中。我们可以查看自己查看下cookie.txt这个文件的内容。</p>\n<p><strong>3)从文件中获取Cookie并访问</strong></p>\n<p>我们已经做到把Cookie保存到文件中了，如果以后想使用，可以利用下面的方法来读取cookie并访问网站，感受一下cookie03.py：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"><span class=\"keyword\">from</span> http <span class=\"keyword\">import</span> cookiejar</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    <span class=\"comment\">#设置保存cookie的文件的文件名,相对路径,也就是同级目录下</span></span><br><span class=\"line\">    filename = <span class=\"string\">'cookie.txt'</span></span><br><span class=\"line\">    <span class=\"comment\">#创建MozillaCookieJar实例对象</span></span><br><span class=\"line\">    cookie = cookiejar.MozillaCookieJar()</span><br><span class=\"line\">    <span class=\"comment\">#从文件中读取cookie内容到变量</span></span><br><span class=\"line\">    cookie.load(filename, ignore_discard=<span class=\"keyword\">True</span>, ignore_expires=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">    <span class=\"comment\">#利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler</span></span><br><span class=\"line\">    handler=request.HTTPCookieProcessor(cookie)</span><br><span class=\"line\">    <span class=\"comment\">#通过CookieHandler创建opener</span></span><br><span class=\"line\">    opener = request.build_opener(handler)</span><br><span class=\"line\">    <span class=\"comment\">#此用opener的open方法打开网页</span></span><br><span class=\"line\">    response = opener.open(<span class=\"string\">'http://www.baidu.com'</span>)</span><br><span class=\"line\">    <span class=\"comment\">#打印信息</span></span><br><span class=\"line\">    print(response.read().decode(<span class=\"string\">'utf-8'</span>))</span><br></pre></td></tr></table></figure></p>\n<p>了解到以上内容，我们那就可以开始正式编写模拟登陆伯乐在线的程序了。同时，我们也可以获取相亲MM的联系方式。</p>\n<h3 id=\"4-编写代码\"><a href=\"#4-编写代码\" class=\"headerlink\" title=\"(4).编写代码\"></a>(4).编写代码</h3><p>我们利用CookieJar对象实现获取cookie的功能，存储到变量中。然后使用这个cookie变量创建opener，使用这个设置好cookie的opener即可模拟登陆，同笔记四中讲到的IP代理的使用方法类似。</p>\n<p>创建cookie_test.py文件，编写代码如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> error</span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> parse</span><br><span class=\"line\"><span class=\"keyword\">from</span> http <span class=\"keyword\">import</span> cookiejar</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    <span class=\"comment\">#登陆地址</span></span><br><span class=\"line\">    login_url = <span class=\"string\">'http://www.jobbole.com/wp-admin/admin-ajax.php'</span>    </span><br><span class=\"line\">    <span class=\"comment\">#User-Agent信息                   </span></span><br><span class=\"line\">    user_agent = <span class=\"string\">r'Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36'</span></span><br><span class=\"line\">    <span class=\"comment\">#Headers信息</span></span><br><span class=\"line\">    head = &#123;<span class=\"string\">'User-Agnet'</span>: user_agent, <span class=\"string\">'Connection'</span>: <span class=\"string\">'keep-alive'</span>&#125;</span><br><span class=\"line\">    <span class=\"comment\">#登陆Form_Data信息</span></span><br><span class=\"line\">    Login_Data = &#123;&#125;</span><br><span class=\"line\">    Login_Data[<span class=\"string\">'action'</span>] = <span class=\"string\">'user_login'</span></span><br><span class=\"line\">    Login_Data[<span class=\"string\">'redirect_url'</span>] = <span class=\"string\">'http://www.jobbole.com/'</span></span><br><span class=\"line\">    Login_Data[<span class=\"string\">'remember_me'</span>] = <span class=\"string\">'0'</span>         <span class=\"comment\">#是否一个月内自动登陆</span></span><br><span class=\"line\">    Login_Data[<span class=\"string\">'user_login'</span>] = <span class=\"string\">'********'</span>       <span class=\"comment\">#改成你自己的用户名</span></span><br><span class=\"line\">    Login_Data[<span class=\"string\">'user_pass'</span>] = <span class=\"string\">'********'</span>        <span class=\"comment\">#改成你自己的密码</span></span><br><span class=\"line\">    <span class=\"comment\">#使用urlencode方法转换标准格式</span></span><br><span class=\"line\">    logingpostdata = parse.urlencode(Login_Data).encode(<span class=\"string\">'utf-8'</span>)</span><br><span class=\"line\">    <span class=\"comment\">#声明一个CookieJar对象实例来保存cookie</span></span><br><span class=\"line\">    cookie = cookiejar.CookieJar()</span><br><span class=\"line\">    <span class=\"comment\">#利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler</span></span><br><span class=\"line\">    cookie_support = request.HTTPCookieProcessor(cookie)</span><br><span class=\"line\">    <span class=\"comment\">#通过CookieHandler创建opener</span></span><br><span class=\"line\">    opener = request.build_opener(cookie_support)</span><br><span class=\"line\">    <span class=\"comment\">#创建Request对象</span></span><br><span class=\"line\">    req1 = request.Request(url=login_url, data=logingpostdata, headers=head)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">#面向对象地址</span></span><br><span class=\"line\">    date_url = <span class=\"string\">'http://date.jobbole.com/wp-admin/admin-ajax.php'</span></span><br><span class=\"line\">    <span class=\"comment\">#面向对象</span></span><br><span class=\"line\">    Date_Data = &#123;&#125;</span><br><span class=\"line\">    Date_Data[<span class=\"string\">'action'</span>] = <span class=\"string\">'get_date_contact'</span></span><br><span class=\"line\">    Date_Data[<span class=\"string\">'postId'</span>] = <span class=\"string\">'4128'</span></span><br><span class=\"line\">    <span class=\"comment\">#使用urlencode方法转换标准格式</span></span><br><span class=\"line\">    datepostdata = parse.urlencode(Date_Data).encode(<span class=\"string\">'utf-8'</span>)</span><br><span class=\"line\">    req2 = request.Request(url=date_url, data=datepostdata, headers=head)</span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        <span class=\"comment\">#使用自己创建的opener的open方法</span></span><br><span class=\"line\">        response1 = opener.open(req1)</span><br><span class=\"line\">        response2 = opener.open(req2)</span><br><span class=\"line\">        html = response2.read().decode(<span class=\"string\">'utf-8'</span>)</span><br><span class=\"line\">        index = html.find(<span class=\"string\">'jb_contact_email'</span>)</span><br><span class=\"line\">        <span class=\"comment\">#打印查询结果</span></span><br><span class=\"line\">        print(<span class=\"string\">'联系邮箱:%s'</span> % html[index+<span class=\"number\">19</span>:<span class=\"number\">-2</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">except</span> error.URLError <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> hasattr(e, <span class=\"string\">'code'</span>):</span><br><span class=\"line\">            print(<span class=\"string\">\"HTTPError:%d\"</span> % e.code)</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> hasattr(e, <span class=\"string\">'reason'</span>):</span><br><span class=\"line\">            print(<span class=\"string\">\"URLError:%s\"</span> % e.reason)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"5-运行结果如下\"><a href=\"#5-运行结果如下\" class=\"headerlink\" title=\"(5).运行结果如下\"></a>(5).运行结果如下</h3><p><img src=\"http://img.blog.csdn.net/20170409150252854?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"\"><br>。</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"为什么要使用Cookie\"><a href=\"#为什么要使用Cookie\" class=\"headerlink\" title=\"为什么要使用Cookie\"></a>为什么要使用Cookie</h2><p>Cookie，指某些网站为了辨别用户身份、进行session跟踪而储存在用户本地终端上的数据（通常经过加密)。<br>比如说有些网站需要登录后才能访问某个页面，在登录之前，你想抓取某个页面内容，登陆前与登陆后是不同的，或者不允许的。<br>使用Cookie和使用代理IP一样，也需要创建一个自己的opener。在HTTP包中，提供了cookiejar模块，用于提供对Cookie的支持。</p>\n<p><img src=\"http://img.blog.csdn.net/20170409144243654?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"\"><br>","more":"</p>\n<p>http.cookiejar功能强大，我们可以利用本模块的CookieJar类的对象来捕获cookie并在后续连接请求时重新发送，比如可以实现模拟登录功能。该模块主要的对象有CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar。</p>\n<p><strong>它们的关系：</strong> CookieJar–派生–&gt;FileCookieJar–派生–&gt;MozillaCookieJar和LWPCookieJar</p>\n<p><strong>工作原理：</strong> 创建一个带有cookie的opener，在访问登录的URL时，将登录后的cookie保存下来，然后利用这个cookie来访问其他网址。查看登录之后才能看到的信息。</p>\n<p>同样，我们以实例进行讲解，爬取伯乐在线的面向对象的漂亮MM的邮箱联系方式。</p>\n<h2 id=\"实战\"><a href=\"#实战\" class=\"headerlink\" title=\"实战\"></a>实战</h2><h3 id=\"1-背景介绍\"><a href=\"#1-背景介绍\" class=\"headerlink\" title=\"(1).背景介绍\"></a>(1).背景介绍</h3><p>在伯乐在线有这么一个有趣的模块，面向对象，它说白了就是提供了一个程序员(媛)网上相亲的平台。</p>\n<p><strong>URL:</strong> <a href=\"http://date.jobbole.com/\" target=\"_blank\" rel=\"noopener\">http://date.jobbole.com/</a></p>\n<p>它的样子是这样的：</p>\n<p><img src=\"http://img.blog.csdn.net/20170409144753813?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"\"></p>\n<p>可以看到，这里有很多的相亲贴，随便点进去就会有网上相亲MM的详细信息，想获取MM的联系方式，需要积分，积分可以通过签到的方式获取。如果没有登陆账户，获取联系方式的地方是这个样子的：</p>\n<p><img src=\"http://img.blog.csdn.net/20170409144912938?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"\"></p>\n<p>如果登陆了账号，获取联系方式的地方是这个样子的：</p>\n<p><img src=\"http://img.blog.csdn.net/20170409144955289?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"\"></p>\n<p>想要爬取MM的联系邮箱，就需要用到我们本次讲到的知识，Cookie的使用。当然，首先你积分也得够。</p>\n<p>在讲解之前，推荐一款抓包工具–Fiddler，可以在Google Chrome的Google商店下载这个插件，它的样子是这样的：</p>\n<p><img src=\"http://img.blog.csdn.net/20170409145106869?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"\"></p>\n<p>可以看到，通过这个插件，我们可以很容易找到Post的Form Data等信息，很方便，当然也可以用之前讲得浏览器审查元素的方式查看这些信息。</p>\n<h3 id=\"2-过程分析\"><a href=\"#2-过程分析\" class=\"headerlink\" title=\"(2).过程分析\"></a>(2).过程分析</h3><p>在伯乐在线首页点击登陆的按钮，Fiddler的抓包内容如下：</p>\n<p><img src=\"http://img.blog.csdn.net/20170409145240590?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"\"></p>\n<p>从上图可以看出，真正请求的url是</p>\n<p> <a href=\"http://www.jobbole.com/wp-admin/admin-ajax.php\" target=\"_blank\" rel=\"noopener\">http://www.jobbole.com/wp-admin/admin-ajax.php</a></p>\n<p>Form Data的内容记住，这些是我们编程需要用到的。user_login是用户名，user_pass是用户密码。</p>\n<p>在点击取得联系邮箱按钮的时候，Fiddler的抓包内容如下：</p>\n<p><img src=\"http://img.blog.csdn.net/20170409145403065?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"\"></p>\n<p>从上图可以看出，此刻真正请求的url是</p>\n<p> <a href=\"http://date.jobbole.com/wp-admin/admin-ajax.php\" target=\"_blank\" rel=\"noopener\">http://date.jobbole.com/wp-admin/admin-ajax.php</a></p>\n<p>同样Form Data中内容要记下来。postId是每个帖子的id。例如，打开一个相亲贴，它的URL是<a href=\"http://date.jobbole.com/4128/\" target=\"_blank\" rel=\"noopener\">http://date.jobbole.com/4128/</a>，那么它的这个postId就是4128。为了简化程序，这里就不讲解如何自动获取这个postId了，本实例直接指定postId。如果想要自动获取，可以使用beautifulsoup解析<a href=\"http://date.jobbole.com/\" target=\"_blank\" rel=\"noopener\">http://date.jobbole.com/</a>返回的信息。beautifulsoup的使用。有机会的话，会在后面的爬虫笔记中进行讲解。</p>\n<h3 id=\"3-测试\"><a href=\"#3-测试\" class=\"headerlink\" title=\"(3).测试\"></a>(3).测试</h3><p><strong>1)将Cookie保存到变量中</strong></p>\n<p>首先，我们先利用CookieJar对象实现获取cookie的功能，存储到变量中，先来感受一下,执行文件cookie01.py：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"><span class=\"keyword\">from</span> http <span class=\"keyword\">import</span> cookiejar</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    <span class=\"comment\">#声明一个CookieJar对象实例来保存cookie</span></span><br><span class=\"line\">    cookie = cookiejar.CookieJar()</span><br><span class=\"line\">    <span class=\"comment\">#利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler</span></span><br><span class=\"line\">    handler=request.HTTPCookieProcessor(cookie)</span><br><span class=\"line\">    <span class=\"comment\">#通过CookieHandler创建opener</span></span><br><span class=\"line\">    opener = request.build_opener(handler)</span><br><span class=\"line\">    <span class=\"comment\">#此处的open方法打开网页</span></span><br><span class=\"line\">    response = opener.open(<span class=\"string\">'http://www.baidu.com'</span>)</span><br><span class=\"line\">    <span class=\"comment\">#打印cookie信息</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> item <span class=\"keyword\">in</span> cookie:</span><br><span class=\"line\">        print(<span class=\"string\">'Name = %s'</span> % item.name)</span><br><span class=\"line\">        print(<span class=\"string\">'Value = %s'</span> % item.value)</span><br></pre></td></tr></table></figure>\n<p>我们使用以上方法将cookie保存到变量中，然后打印出了cookie中的值，运行结果如下:</p>\n<p><img src=\"http://img.blog.csdn.net/20170409145652613?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"\"></p>\n<p><strong>2)保存Cookie到文件</strong></p>\n<p>在上面的方法中，我们将cookie保存到了cookie这个变量中，如果我们想将cookie保存到文件中该怎么做呢？方便以后直接读取文件使用，这时，我们就要用到FileCookieJar这个对象了，在这里我们使用它的子类MozillaCookieJar来实现Cookie的保存，编写代码如下,文件为cookie02.py：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"><span class=\"keyword\">from</span> http <span class=\"keyword\">import</span> cookiejar</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">#设置保存cookie的文件，同级目录下的cookie.txt</span></span><br><span class=\"line\">    filename = <span class=\"string\">'cookie.txt'</span></span><br><span class=\"line\">    <span class=\"comment\">#声明一个MozillaCookieJar对象实例来保存cookie，之后写入文件</span></span><br><span class=\"line\">    cookie = cookiejar.MozillaCookieJar(filename)</span><br><span class=\"line\">    <span class=\"comment\">#利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler</span></span><br><span class=\"line\">    handler=request.HTTPCookieProcessor(cookie)</span><br><span class=\"line\">    <span class=\"comment\">#通过CookieHandler创建opener</span></span><br><span class=\"line\">    opener = request.build_opener(handler)</span><br><span class=\"line\">    <span class=\"comment\">#此处的open方法打开网页</span></span><br><span class=\"line\">    response = opener.open(<span class=\"string\">'http://www.baidu.com'</span>)</span><br><span class=\"line\">    <span class=\"comment\">#保存cookie到文件</span></span><br><span class=\"line\">    cookie.save(ignore_discard=<span class=\"keyword\">True</span>, ignore_expires=<span class=\"keyword\">True</span>)</span><br></pre></td></tr></table></figure></p>\n<p>cookie.save的参数说明：</p>\n<ul>\n<li><p>ignore_discard的意思是即使cookies将被丢弃也将它保存下来；</p>\n</li>\n<li><p>ignore_expires的意思是如果在该文件中cookies已经存在，则覆盖原文件写入。</p>\n</li>\n</ul>\n<p>在这里，我们将这两个全部设置为True。</p>\n<p>运行之后，cookies将被保存到cookie.txt文件中。我们可以查看自己查看下cookie.txt这个文件的内容。</p>\n<p><strong>3)从文件中获取Cookie并访问</strong></p>\n<p>我们已经做到把Cookie保存到文件中了，如果以后想使用，可以利用下面的方法来读取cookie并访问网站，感受一下cookie03.py：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"><span class=\"keyword\">from</span> http <span class=\"keyword\">import</span> cookiejar</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    <span class=\"comment\">#设置保存cookie的文件的文件名,相对路径,也就是同级目录下</span></span><br><span class=\"line\">    filename = <span class=\"string\">'cookie.txt'</span></span><br><span class=\"line\">    <span class=\"comment\">#创建MozillaCookieJar实例对象</span></span><br><span class=\"line\">    cookie = cookiejar.MozillaCookieJar()</span><br><span class=\"line\">    <span class=\"comment\">#从文件中读取cookie内容到变量</span></span><br><span class=\"line\">    cookie.load(filename, ignore_discard=<span class=\"keyword\">True</span>, ignore_expires=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">    <span class=\"comment\">#利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler</span></span><br><span class=\"line\">    handler=request.HTTPCookieProcessor(cookie)</span><br><span class=\"line\">    <span class=\"comment\">#通过CookieHandler创建opener</span></span><br><span class=\"line\">    opener = request.build_opener(handler)</span><br><span class=\"line\">    <span class=\"comment\">#此用opener的open方法打开网页</span></span><br><span class=\"line\">    response = opener.open(<span class=\"string\">'http://www.baidu.com'</span>)</span><br><span class=\"line\">    <span class=\"comment\">#打印信息</span></span><br><span class=\"line\">    print(response.read().decode(<span class=\"string\">'utf-8'</span>))</span><br></pre></td></tr></table></figure></p>\n<p>了解到以上内容，我们那就可以开始正式编写模拟登陆伯乐在线的程序了。同时，我们也可以获取相亲MM的联系方式。</p>\n<h3 id=\"4-编写代码\"><a href=\"#4-编写代码\" class=\"headerlink\" title=\"(4).编写代码\"></a>(4).编写代码</h3><p>我们利用CookieJar对象实现获取cookie的功能，存储到变量中。然后使用这个cookie变量创建opener，使用这个设置好cookie的opener即可模拟登陆，同笔记四中讲到的IP代理的使用方法类似。</p>\n<p>创建cookie_test.py文件，编写代码如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: UTF-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> request</span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> error</span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib <span class=\"keyword\">import</span> parse</span><br><span class=\"line\"><span class=\"keyword\">from</span> http <span class=\"keyword\">import</span> cookiejar</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    <span class=\"comment\">#登陆地址</span></span><br><span class=\"line\">    login_url = <span class=\"string\">'http://www.jobbole.com/wp-admin/admin-ajax.php'</span>    </span><br><span class=\"line\">    <span class=\"comment\">#User-Agent信息                   </span></span><br><span class=\"line\">    user_agent = <span class=\"string\">r'Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36'</span></span><br><span class=\"line\">    <span class=\"comment\">#Headers信息</span></span><br><span class=\"line\">    head = &#123;<span class=\"string\">'User-Agnet'</span>: user_agent, <span class=\"string\">'Connection'</span>: <span class=\"string\">'keep-alive'</span>&#125;</span><br><span class=\"line\">    <span class=\"comment\">#登陆Form_Data信息</span></span><br><span class=\"line\">    Login_Data = &#123;&#125;</span><br><span class=\"line\">    Login_Data[<span class=\"string\">'action'</span>] = <span class=\"string\">'user_login'</span></span><br><span class=\"line\">    Login_Data[<span class=\"string\">'redirect_url'</span>] = <span class=\"string\">'http://www.jobbole.com/'</span></span><br><span class=\"line\">    Login_Data[<span class=\"string\">'remember_me'</span>] = <span class=\"string\">'0'</span>         <span class=\"comment\">#是否一个月内自动登陆</span></span><br><span class=\"line\">    Login_Data[<span class=\"string\">'user_login'</span>] = <span class=\"string\">'********'</span>       <span class=\"comment\">#改成你自己的用户名</span></span><br><span class=\"line\">    Login_Data[<span class=\"string\">'user_pass'</span>] = <span class=\"string\">'********'</span>        <span class=\"comment\">#改成你自己的密码</span></span><br><span class=\"line\">    <span class=\"comment\">#使用urlencode方法转换标准格式</span></span><br><span class=\"line\">    logingpostdata = parse.urlencode(Login_Data).encode(<span class=\"string\">'utf-8'</span>)</span><br><span class=\"line\">    <span class=\"comment\">#声明一个CookieJar对象实例来保存cookie</span></span><br><span class=\"line\">    cookie = cookiejar.CookieJar()</span><br><span class=\"line\">    <span class=\"comment\">#利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就CookieHandler</span></span><br><span class=\"line\">    cookie_support = request.HTTPCookieProcessor(cookie)</span><br><span class=\"line\">    <span class=\"comment\">#通过CookieHandler创建opener</span></span><br><span class=\"line\">    opener = request.build_opener(cookie_support)</span><br><span class=\"line\">    <span class=\"comment\">#创建Request对象</span></span><br><span class=\"line\">    req1 = request.Request(url=login_url, data=logingpostdata, headers=head)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">#面向对象地址</span></span><br><span class=\"line\">    date_url = <span class=\"string\">'http://date.jobbole.com/wp-admin/admin-ajax.php'</span></span><br><span class=\"line\">    <span class=\"comment\">#面向对象</span></span><br><span class=\"line\">    Date_Data = &#123;&#125;</span><br><span class=\"line\">    Date_Data[<span class=\"string\">'action'</span>] = <span class=\"string\">'get_date_contact'</span></span><br><span class=\"line\">    Date_Data[<span class=\"string\">'postId'</span>] = <span class=\"string\">'4128'</span></span><br><span class=\"line\">    <span class=\"comment\">#使用urlencode方法转换标准格式</span></span><br><span class=\"line\">    datepostdata = parse.urlencode(Date_Data).encode(<span class=\"string\">'utf-8'</span>)</span><br><span class=\"line\">    req2 = request.Request(url=date_url, data=datepostdata, headers=head)</span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        <span class=\"comment\">#使用自己创建的opener的open方法</span></span><br><span class=\"line\">        response1 = opener.open(req1)</span><br><span class=\"line\">        response2 = opener.open(req2)</span><br><span class=\"line\">        html = response2.read().decode(<span class=\"string\">'utf-8'</span>)</span><br><span class=\"line\">        index = html.find(<span class=\"string\">'jb_contact_email'</span>)</span><br><span class=\"line\">        <span class=\"comment\">#打印查询结果</span></span><br><span class=\"line\">        print(<span class=\"string\">'联系邮箱:%s'</span> % html[index+<span class=\"number\">19</span>:<span class=\"number\">-2</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">except</span> error.URLError <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> hasattr(e, <span class=\"string\">'code'</span>):</span><br><span class=\"line\">            print(<span class=\"string\">\"HTTPError:%d\"</span> % e.code)</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> hasattr(e, <span class=\"string\">'reason'</span>):</span><br><span class=\"line\">            print(<span class=\"string\">\"URLError:%s\"</span> % e.reason)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"5-运行结果如下\"><a href=\"#5-运行结果如下\" class=\"headerlink\" title=\"(5).运行结果如下\"></a>(5).运行结果如下</h3><p><img src=\"http://img.blog.csdn.net/20170409150252854?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"\"><br>。</p>"},{"title":"简谈爬虫攻与防","date":"2018-02-27T04:44:38.000Z","_content":"\n\n## 封锁间隔时间破解\n\nScrapy在两次请求之间的时间设置是DOWNLOAD_DELAY。如果不考虑反爬虫的因素，该值当然越小越好。如果DOWNLOAD_DELAY设为0.001，也就是每1毫秒请求一次网页，这简直非人类干的事情。有些网站会检测一个ip的访问时间，异常情况下会封锁该ip。\n\n<!--more-->\n\n## 封锁Cookies\n众所周知，网站是通过Cookie来确定用户身份的，Scrapy在爬取数据时使用同一个Cookies发起请求。该做法和把DOWNLOAD_DELAY设为0.001没有本质区别。在scrapy中，直接社禁用Cookies就可以了。在settings.py中设置\n```python\n# Disable cookies (enabled by default)\nCOOKIES_ENABLED = False\n```\n\n## 封锁user-agent和proxy破解\nuser-agent是浏览器的身份标识。网站通过UA来确定浏览器类型。很多浏览器拒绝不符合一定标准的UA请求网页。同一个UA高频率的访问网站会有被网站列入黑名单的危险。破解的方法很简单，可以准备一个**UA池**，每次请求时随机挑选一个进行请求。\n\n在middlewares.py同级目录下创建UAResource.py,文件内容如下：\n\n```python\n#-*- coding: utf-8 -*-\n\nUserAgents = [\n  \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n  \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)\",\n  \"Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n  \"Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)\",\n  \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)\",\n  \"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)\",\n  \"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)\",\n  \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)\",\n  \"Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6\",\n  \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1\",\n  \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0\",\n  \"Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5\",\n  \"Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6\",\n  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\",\n  \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20\",\n  \"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52\",\n]\n\nProxies = [\n'http://122.114.31.177:808',\n'http://1.2.3.4:80',\n]\n```\n\n修改middlewares.py，添加内容为\n```python\nfrom .UAResource import UserAgents\nfrom .UAResource import Proxies\nimport random\n\nclass RandomProxy(object):\n    def process_request(self, request, spider):\n        proxy = random.choice(Proxies)\n        request.meta['proxy'] = proxy\n\nclass RandomUserAgent(object):\n    \"\"\"docstring for RandomUerAgent.\"\"\"\n    def process_request(self, request, spider):\n        ua = random.choice(UserAgents)\n        request.headers.setdefault('User-Agent', ua)\n```\n\n最后修改setting.py,将RandomUserAgent和RandomProxy添加到DOWNLOADER_MIDDLEWARES中，注意RandomProxy要放到RandomUserAgent之前，即将RandomProxy的值比RandomUserAgent后的值小\n```python\nDOWNLOADER_MIDDLEWARES = {\n   # 'meijutt.middlewares.MeijuttDownloaderMiddleware': 543,\n   'meijutt.middlewares.RandomProxy': 10,\n   'meijutt.middlewares.RandomUserAgent': 30,\n\n   # 禁止内置的(在 DOWNLOADER_MIDDLEWARES_BASE 中设置并默认启用的)中间件\n   'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n}\n\n```\n\n免费代理不够稳定，如果不想用proxy，设置RandomProxy为None,即禁止使用\n```python\n'meijutt.middlewares.RandomProxy': None,\n```\n","source":"_posts/简谈爬虫攻与防.md","raw":"---\ntitle: 简谈爬虫攻与防\ndate: 2018-02-27 12:44:38\ntags:\n  - Scrapy\n  - Spider\ncategories:\n  - Spider\n  - Scrapy\n---\n\n\n## 封锁间隔时间破解\n\nScrapy在两次请求之间的时间设置是DOWNLOAD_DELAY。如果不考虑反爬虫的因素，该值当然越小越好。如果DOWNLOAD_DELAY设为0.001，也就是每1毫秒请求一次网页，这简直非人类干的事情。有些网站会检测一个ip的访问时间，异常情况下会封锁该ip。\n\n<!--more-->\n\n## 封锁Cookies\n众所周知，网站是通过Cookie来确定用户身份的，Scrapy在爬取数据时使用同一个Cookies发起请求。该做法和把DOWNLOAD_DELAY设为0.001没有本质区别。在scrapy中，直接社禁用Cookies就可以了。在settings.py中设置\n```python\n# Disable cookies (enabled by default)\nCOOKIES_ENABLED = False\n```\n\n## 封锁user-agent和proxy破解\nuser-agent是浏览器的身份标识。网站通过UA来确定浏览器类型。很多浏览器拒绝不符合一定标准的UA请求网页。同一个UA高频率的访问网站会有被网站列入黑名单的危险。破解的方法很简单，可以准备一个**UA池**，每次请求时随机挑选一个进行请求。\n\n在middlewares.py同级目录下创建UAResource.py,文件内容如下：\n\n```python\n#-*- coding: utf-8 -*-\n\nUserAgents = [\n  \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n  \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)\",\n  \"Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n  \"Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)\",\n  \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)\",\n  \"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)\",\n  \"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)\",\n  \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)\",\n  \"Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6\",\n  \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1\",\n  \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0\",\n  \"Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5\",\n  \"Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6\",\n  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\",\n  \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20\",\n  \"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52\",\n]\n\nProxies = [\n'http://122.114.31.177:808',\n'http://1.2.3.4:80',\n]\n```\n\n修改middlewares.py，添加内容为\n```python\nfrom .UAResource import UserAgents\nfrom .UAResource import Proxies\nimport random\n\nclass RandomProxy(object):\n    def process_request(self, request, spider):\n        proxy = random.choice(Proxies)\n        request.meta['proxy'] = proxy\n\nclass RandomUserAgent(object):\n    \"\"\"docstring for RandomUerAgent.\"\"\"\n    def process_request(self, request, spider):\n        ua = random.choice(UserAgents)\n        request.headers.setdefault('User-Agent', ua)\n```\n\n最后修改setting.py,将RandomUserAgent和RandomProxy添加到DOWNLOADER_MIDDLEWARES中，注意RandomProxy要放到RandomUserAgent之前，即将RandomProxy的值比RandomUserAgent后的值小\n```python\nDOWNLOADER_MIDDLEWARES = {\n   # 'meijutt.middlewares.MeijuttDownloaderMiddleware': 543,\n   'meijutt.middlewares.RandomProxy': 10,\n   'meijutt.middlewares.RandomUserAgent': 30,\n\n   # 禁止内置的(在 DOWNLOADER_MIDDLEWARES_BASE 中设置并默认启用的)中间件\n   'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n}\n\n```\n\n免费代理不够稳定，如果不想用proxy，设置RandomProxy为None,即禁止使用\n```python\n'meijutt.middlewares.RandomProxy': None,\n```\n","slug":"简谈爬虫攻与防","published":1,"updated":"2018-02-27T17:04:37.804Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cje6mdg0t000wrcoteyj7ai72","content":"<h2 id=\"封锁间隔时间破解\"><a href=\"#封锁间隔时间破解\" class=\"headerlink\" title=\"封锁间隔时间破解\"></a>封锁间隔时间破解</h2><p>Scrapy在两次请求之间的时间设置是DOWNLOAD_DELAY。如果不考虑反爬虫的因素，该值当然越小越好。如果DOWNLOAD_DELAY设为0.001，也就是每1毫秒请求一次网页，这简直非人类干的事情。有些网站会检测一个ip的访问时间，异常情况下会封锁该ip。</p>\n<a id=\"more\"></a>\n<h2 id=\"封锁Cookies\"><a href=\"#封锁Cookies\" class=\"headerlink\" title=\"封锁Cookies\"></a>封锁Cookies</h2><p>众所周知，网站是通过Cookie来确定用户身份的，Scrapy在爬取数据时使用同一个Cookies发起请求。该做法和把DOWNLOAD_DELAY设为0.001没有本质区别。在scrapy中，直接社禁用Cookies就可以了。在settings.py中设置<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Disable cookies (enabled by default)</span></span><br><span class=\"line\">COOKIES_ENABLED = <span class=\"keyword\">False</span></span><br></pre></td></tr></table></figure></p>\n<h2 id=\"封锁user-agent和proxy破解\"><a href=\"#封锁user-agent和proxy破解\" class=\"headerlink\" title=\"封锁user-agent和proxy破解\"></a>封锁user-agent和proxy破解</h2><p>user-agent是浏览器的身份标识。网站通过UA来确定浏览器类型。很多浏览器拒绝不符合一定标准的UA请求网页。同一个UA高频率的访问网站会有被网站列入黑名单的危险。破解的方法很简单，可以准备一个<strong>UA池</strong>，每次请求时随机挑选一个进行请求。</p>\n<p>在middlewares.py同级目录下创建UAResource.py,文件内容如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#-*- coding: utf-8 -*-</span></span><br><span class=\"line\"></span><br><span class=\"line\">UserAgents = [</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52\"</span>,</span><br><span class=\"line\">]</span><br><span class=\"line\"></span><br><span class=\"line\">Proxies = [</span><br><span class=\"line\"><span class=\"string\">'http://122.114.31.177:808'</span>,</span><br><span class=\"line\"><span class=\"string\">'http://1.2.3.4:80'</span>,</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n<p>修改middlewares.py，添加内容为<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> .UAResource <span class=\"keyword\">import</span> UserAgents</span><br><span class=\"line\"><span class=\"keyword\">from</span> .UAResource <span class=\"keyword\">import</span> Proxies</span><br><span class=\"line\"><span class=\"keyword\">import</span> random</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RandomProxy</span><span class=\"params\">(object)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">process_request</span><span class=\"params\">(self, request, spider)</span>:</span></span><br><span class=\"line\">        proxy = random.choice(Proxies)</span><br><span class=\"line\">        request.meta[<span class=\"string\">'proxy'</span>] = proxy</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RandomUserAgent</span><span class=\"params\">(object)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"docstring for RandomUerAgent.\"\"\"</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">process_request</span><span class=\"params\">(self, request, spider)</span>:</span></span><br><span class=\"line\">        ua = random.choice(UserAgents)</span><br><span class=\"line\">        request.headers.setdefault(<span class=\"string\">'User-Agent'</span>, ua)</span><br></pre></td></tr></table></figure></p>\n<p>最后修改setting.py,将RandomUserAgent和RandomProxy添加到DOWNLOADER_MIDDLEWARES中，注意RandomProxy要放到RandomUserAgent之前，即将RandomProxy的值比RandomUserAgent后的值小<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class=\"line\">   <span class=\"comment\"># 'meijutt.middlewares.MeijuttDownloaderMiddleware': 543,</span></span><br><span class=\"line\">   <span class=\"string\">'meijutt.middlewares.RandomProxy'</span>: <span class=\"number\">10</span>,</span><br><span class=\"line\">   <span class=\"string\">'meijutt.middlewares.RandomUserAgent'</span>: <span class=\"number\">30</span>,</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"comment\"># 禁止内置的(在 DOWNLOADER_MIDDLEWARES_BASE 中设置并默认启用的)中间件</span></span><br><span class=\"line\">   <span class=\"string\">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span>: <span class=\"keyword\">None</span>,</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>免费代理不够稳定，如果不想用proxy，设置RandomProxy为None,即禁止使用<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">'meijutt.middlewares.RandomProxy'</span>: <span class=\"keyword\">None</span>,</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"<h2 id=\"封锁间隔时间破解\"><a href=\"#封锁间隔时间破解\" class=\"headerlink\" title=\"封锁间隔时间破解\"></a>封锁间隔时间破解</h2><p>Scrapy在两次请求之间的时间设置是DOWNLOAD_DELAY。如果不考虑反爬虫的因素，该值当然越小越好。如果DOWNLOAD_DELAY设为0.001，也就是每1毫秒请求一次网页，这简直非人类干的事情。有些网站会检测一个ip的访问时间，异常情况下会封锁该ip。</p>","more":"<h2 id=\"封锁Cookies\"><a href=\"#封锁Cookies\" class=\"headerlink\" title=\"封锁Cookies\"></a>封锁Cookies</h2><p>众所周知，网站是通过Cookie来确定用户身份的，Scrapy在爬取数据时使用同一个Cookies发起请求。该做法和把DOWNLOAD_DELAY设为0.001没有本质区别。在scrapy中，直接社禁用Cookies就可以了。在settings.py中设置<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Disable cookies (enabled by default)</span></span><br><span class=\"line\">COOKIES_ENABLED = <span class=\"keyword\">False</span></span><br></pre></td></tr></table></figure></p>\n<h2 id=\"封锁user-agent和proxy破解\"><a href=\"#封锁user-agent和proxy破解\" class=\"headerlink\" title=\"封锁user-agent和proxy破解\"></a>封锁user-agent和proxy破解</h2><p>user-agent是浏览器的身份标识。网站通过UA来确定浏览器类型。很多浏览器拒绝不符合一定标准的UA请求网页。同一个UA高频率的访问网站会有被网站列入黑名单的危险。破解的方法很简单，可以准备一个<strong>UA池</strong>，每次请求时随机挑选一个进行请求。</p>\n<p>在middlewares.py同级目录下创建UAResource.py,文件内容如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#-*- coding: utf-8 -*-</span></span><br><span class=\"line\"></span><br><span class=\"line\">UserAgents = [</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52\"</span>,</span><br><span class=\"line\">]</span><br><span class=\"line\"></span><br><span class=\"line\">Proxies = [</span><br><span class=\"line\"><span class=\"string\">'http://122.114.31.177:808'</span>,</span><br><span class=\"line\"><span class=\"string\">'http://1.2.3.4:80'</span>,</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n<p>修改middlewares.py，添加内容为<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> .UAResource <span class=\"keyword\">import</span> UserAgents</span><br><span class=\"line\"><span class=\"keyword\">from</span> .UAResource <span class=\"keyword\">import</span> Proxies</span><br><span class=\"line\"><span class=\"keyword\">import</span> random</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RandomProxy</span><span class=\"params\">(object)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">process_request</span><span class=\"params\">(self, request, spider)</span>:</span></span><br><span class=\"line\">        proxy = random.choice(Proxies)</span><br><span class=\"line\">        request.meta[<span class=\"string\">'proxy'</span>] = proxy</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RandomUserAgent</span><span class=\"params\">(object)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"docstring for RandomUerAgent.\"\"\"</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">process_request</span><span class=\"params\">(self, request, spider)</span>:</span></span><br><span class=\"line\">        ua = random.choice(UserAgents)</span><br><span class=\"line\">        request.headers.setdefault(<span class=\"string\">'User-Agent'</span>, ua)</span><br></pre></td></tr></table></figure></p>\n<p>最后修改setting.py,将RandomUserAgent和RandomProxy添加到DOWNLOADER_MIDDLEWARES中，注意RandomProxy要放到RandomUserAgent之前，即将RandomProxy的值比RandomUserAgent后的值小<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class=\"line\">   <span class=\"comment\"># 'meijutt.middlewares.MeijuttDownloaderMiddleware': 543,</span></span><br><span class=\"line\">   <span class=\"string\">'meijutt.middlewares.RandomProxy'</span>: <span class=\"number\">10</span>,</span><br><span class=\"line\">   <span class=\"string\">'meijutt.middlewares.RandomUserAgent'</span>: <span class=\"number\">30</span>,</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"comment\"># 禁止内置的(在 DOWNLOADER_MIDDLEWARES_BASE 中设置并默认启用的)中间件</span></span><br><span class=\"line\">   <span class=\"string\">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span>: <span class=\"keyword\">None</span>,</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>免费代理不够稳定，如果不想用proxy，设置RandomProxy为None,即禁止使用<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">'meijutt.middlewares.RandomProxy'</span>: <span class=\"keyword\">None</span>,</span><br></pre></td></tr></table></figure></p>"}],"PostAsset":[],"PostCategory":[{"post_id":"cje6mdfbj0000rcotmuv0oxec","category_id":"cje6mdfcx0002rcot39gjawb6","_id":"cje6mdfdk0006rcotghqlv70o"},{"post_id":"cje6mdfce0001rcotybvv7qma","category_id":"cje6mdfde0004rcoti40x7z8p","_id":"cje6mdfe0000arcotnrt9py00"},{"post_id":"cje6mdfzk000krcotx1hu0fmm","category_id":"cje6mdg0b000prcote0n1s5wa","_id":"cje6mdg0v000xrcotto87s1cg"},{"post_id":"cje6mdg0n000vrcotuq9jewi1","category_id":"cje6mdfzi000ircotguhvdv78","_id":"cje6mdg0z0011rcotyusvzv3h"},{"post_id":"cje6mdg0n000vrcotuq9jewi1","category_id":"cje6mdg0j000trcoto8xi1zlo","_id":"cje6mdg170015rcotb2olfoaz"},{"post_id":"cje6mdfzv000mrcotn6z6n4vo","category_id":"cje6mdfzi000ircotguhvdv78","_id":"cje6mdg190017rcoto5lxo16r"},{"post_id":"cje6mdfzv000mrcotn6z6n4vo","category_id":"cje6mdg0j000trcoto8xi1zlo","_id":"cje6mdg1b001brcotmooh6rvi"},{"post_id":"cje6mdfz1000ercot5f59q97d","category_id":"cje6mdfzi000ircotguhvdv78","_id":"cje6mdg1c001drcotoykk98bn"},{"post_id":"cje6mdfz1000ercot5f59q97d","category_id":"cje6mdg0v000yrcotgf6tkvgw","_id":"cje6mdg1f001hrcote5g3zp6g"},{"post_id":"cje6mdg02000orcotrxst3tvt","category_id":"cje6mdfzi000ircotguhvdv78","_id":"cje6mdg1g001jrcot27z0hzyr"},{"post_id":"cje6mdg02000orcotrxst3tvt","category_id":"cje6mdg0j000trcoto8xi1zlo","_id":"cje6mdg1j001nrcotewkralqm"},{"post_id":"cje6mdg0f000rrcottigupu89","category_id":"cje6mdfzi000ircotguhvdv78","_id":"cje6mdg1k001prcot9omts311"},{"post_id":"cje6mdg0f000rrcottigupu89","category_id":"cje6mdg0j000trcoto8xi1zlo","_id":"cje6mdg1o001srcotgpuxqb20"},{"post_id":"cje6mdg0i000srcotackwp1mg","category_id":"cje6mdfzi000ircotguhvdv78","_id":"cje6mdg1q001urcot16bjqp9f"},{"post_id":"cje6mdg0i000srcotackwp1mg","category_id":"cje6mdg0j000trcoto8xi1zlo","_id":"cje6mdg1q001wrcot92jidzjj"},{"post_id":"cje6mdg0t000wrcoteyj7ai72","category_id":"cje6mdfzi000ircotguhvdv78","_id":"cje6mdg1r001yrcotk3tdtmjp"},{"post_id":"cje6mdg0t000wrcoteyj7ai72","category_id":"cje6mdg1h001lrcotoyqxlvgz","_id":"cje6mdg1s0020rcots5f91r6r"}],"PostTag":[{"post_id":"cje6mdfbj0000rcotmuv0oxec","tag_id":"cje6mdfd90003rcoturovv57k","_id":"cje6mdfdu0008rcot2l0k39k8"},{"post_id":"cje6mdfbj0000rcotmuv0oxec","tag_id":"cje6mdfdh0005rcotrf8yq3u7","_id":"cje6mdfdv0009rcot293zswz6"},{"post_id":"cje6mdfce0001rcotybvv7qma","tag_id":"cje6mdfdl0007rcotpl7z2z7p","_id":"cje6mdfe2000crcotzvnqmx7u"},{"post_id":"cje6mdfce0001rcotybvv7qma","tag_id":"cje6mdfe0000brcotc6yn30x2","_id":"cje6mdfe2000drcotxx07ru8p"},{"post_id":"cje6mdfzk000krcotx1hu0fmm","tag_id":"cje6mdfe0000brcotc6yn30x2","_id":"cje6mdg01000nrcotbto84axe"},{"post_id":"cje6mdfz1000ercot5f59q97d","tag_id":"cje6mdfzj000jrcotkswnzks0","_id":"cje6mdg0y0010rcotfmanufjw"},{"post_id":"cje6mdfz1000ercot5f59q97d","tag_id":"cje6mdg0c000qrcotcdxsfqph","_id":"cje6mdg0z0012rcotpwoewt7j"},{"post_id":"cje6mdfz1000ercot5f59q97d","tag_id":"cje6mdg0k000urcotd6bzwior","_id":"cje6mdg180016rcot9em9y0qh"},{"post_id":"cje6mdfz1000ercot5f59q97d","tag_id":"cje6mdfdh0005rcotrf8yq3u7","_id":"cje6mdg190018rcotf0mes8mq"},{"post_id":"cje6mdfzv000mrcotn6z6n4vo","tag_id":"cje6mdfzj000jrcotkswnzks0","_id":"cje6mdg1b001crcotasn5d6do"},{"post_id":"cje6mdfzv000mrcotn6z6n4vo","tag_id":"cje6mdg0w000zrcote95y8mez","_id":"cje6mdg1d001ercots4khl3fs"},{"post_id":"cje6mdfzv000mrcotn6z6n4vo","tag_id":"cje6mdfdh0005rcotrf8yq3u7","_id":"cje6mdg1f001ircot3frn7p4s"},{"post_id":"cje6mdg02000orcotrxst3tvt","tag_id":"cje6mdfzj000jrcotkswnzks0","_id":"cje6mdg1g001krcotpoxbvdq3"},{"post_id":"cje6mdg02000orcotrxst3tvt","tag_id":"cje6mdg0w000zrcote95y8mez","_id":"cje6mdg1j001orcotnxwl96a1"},{"post_id":"cje6mdg02000orcotrxst3tvt","tag_id":"cje6mdfdh0005rcotrf8yq3u7","_id":"cje6mdg1k001qrcotm252u4d6"},{"post_id":"cje6mdg0f000rrcottigupu89","tag_id":"cje6mdfzj000jrcotkswnzks0","_id":"cje6mdg1o001trcotzov5imiq"},{"post_id":"cje6mdg0f000rrcottigupu89","tag_id":"cje6mdg0w000zrcote95y8mez","_id":"cje6mdg1q001vrcot8yynvg70"},{"post_id":"cje6mdg0f000rrcottigupu89","tag_id":"cje6mdfdh0005rcotrf8yq3u7","_id":"cje6mdg1r001xrcototxmmn60"},{"post_id":"cje6mdg0i000srcotackwp1mg","tag_id":"cje6mdfzj000jrcotkswnzks0","_id":"cje6mdg1r001zrcotycau0gcj"},{"post_id":"cje6mdg0i000srcotackwp1mg","tag_id":"cje6mdg0w000zrcote95y8mez","_id":"cje6mdg1s0021rcothlfgy32g"},{"post_id":"cje6mdg0i000srcotackwp1mg","tag_id":"cje6mdfdh0005rcotrf8yq3u7","_id":"cje6mdg1t0022rcotatgs4d7a"},{"post_id":"cje6mdg0n000vrcotuq9jewi1","tag_id":"cje6mdfzj000jrcotkswnzks0","_id":"cje6mdg1t0023rcot7jgoy5c3"},{"post_id":"cje6mdg0n000vrcotuq9jewi1","tag_id":"cje6mdg0w000zrcote95y8mez","_id":"cje6mdg1u0024rcotdurmbrxt"},{"post_id":"cje6mdg0n000vrcotuq9jewi1","tag_id":"cje6mdfdh0005rcotrf8yq3u7","_id":"cje6mdg1u0025rcotrevb9egh"},{"post_id":"cje6mdg0t000wrcoteyj7ai72","tag_id":"cje6mdg1l001rrcotko1zcwg5","_id":"cje6mdg1v0026rcotj9iqjxv0"},{"post_id":"cje6mdg0t000wrcoteyj7ai72","tag_id":"cje6mdfzj000jrcotkswnzks0","_id":"cje6mdg1v0027rcotr6oh6rxu"}],"Tag":[{"name":"Django","_id":"cje6mdfd90003rcoturovv57k"},{"name":"Python3","_id":"cje6mdfdh0005rcotrf8yq3u7"},{"name":"Hexo","_id":"cje6mdfdl0007rcotpl7z2z7p"},{"name":"Git","_id":"cje6mdfe0000brcotc6yn30x2"},{"name":"Spider","_id":"cje6mdfzj000jrcotkswnzks0"},{"name":"Selenium","_id":"cje6mdg0c000qrcotcdxsfqph"},{"name":"PhantomJS","_id":"cje6mdg0k000urcotd6bzwior"},{"name":"Urllib","_id":"cje6mdg0w000zrcote95y8mez"},{"name":"Scrapy","_id":"cje6mdg1l001rrcotko1zcwg5"}]}}