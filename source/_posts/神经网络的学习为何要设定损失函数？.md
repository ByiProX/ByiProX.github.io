---
title: 神经网络的学习为何要设定损失函数？
date: 2019-10-01 00:17:38
tags:
  - 机器学习
  - 神经网络
categories:
  - 机器学习
---
这里所说的“学习”是指从训练数据中**自动**获取最优权重参数的过程。学习的目的就是以该损失函数为基准，找出能使它的值达到最小的权重参数。

可能会有人问：我们想获得的是能提高识别精度的参数，特意再导入一个损失函数不是有些重复劳动吗?既然我们的目标是获得使识别精度尽可能高的神经网络，那不是应该把识别精度作为指标吗?

<!-- more -->

听起来很有道理！

对于这一疑问，我们可以根据**“导数“**在神经网络学习中的作用来回答。 在神经网络的学习中，寻找最优参数(权重和偏置)时， 要寻找使损失函数的值尽可能小的参数。为了找到使损失函数的值尽可能小 的地方，需要计算参数的导数(确切地讲是梯度)，然后以这个导数为指引， 逐步更新参数的值。

假设有一个神经网络，现在我们来关注这个神经网络中的某一个权重参数。此时，对该权重参数的损失函数求导，此处导数的含义可以理解为“如果稍微改变这个权重参数的值，损失函数的值会如何变化”。**如果导数的值为负，通过使该权重参数向正方向改变，可以减小损失函数的值；反过来，如果导数的值为正， 则通过使该权重参数向负方向改变，可以减小损失函数的值**。不过，当导数为 0 时，无论权重参数向哪个方向变化，损失函数的值都不会改变，此时该权重参数的更新会停在此处。

**在进行神经网络的学习时，不能将识别精度作为指标。因为如果以识别精度为指标，则参数的导数在绝大多数地方都会变为 0，导致参数无法更新。**那为什么用识别精度作为指标时，参数的导数在绝大多数地方都会变成0呢？

为了回答这个问题，我们来思考另一个具体例子。假设某个神经网络正确识别出了 100个训练数据中的32笔，此时识别精度为 32 %。如果以识别精度为指标，即使稍微改变权重参数的值，识别精度也仍将保持在 32 %，不会出现变化。也就是说，仅仅微调参数，是无法改善识别精度的。即便识别精度有所改善，它的值也不会像 32.0123 . . . % 这样连续变化，而是变为 33 %、 34 % 这样的**不连续的、离散的值**。而如果把损失函数作为指标，则当前损失函数的值可以表示为 0.92543 . . . 这样的值。并且，如果稍微改变一下参数 的值，对应的损失函数也会像 0.93432 . . . 这样发生连续性的变化。

作为激活函数的阶跃函数也有同样的情况。出于相同的原因，如果使用阶跃函数作为激活函数，神经网络的学习将无法进行。原因是阶跃函数的导数在绝大多数地方(除了0以外的地方)均为0。 也就是说，如果使用了阶跃函数，那么即便将损失函数作为指标，参数的微小变化也会被阶跃函数**抹杀**，导致损失函数的值不会产生任何变化。

而 sigmoid 函数，不仅函数的输出(竖轴的值)是连续变化的，曲线的斜率(导数) 也是连续变化的。也就是说，sigmoid 函数的导数在任何地方都不为 0。这对神经网络的学习非常重要。得益于这个斜率不会为 0 的性质，神经网络的学习得以正确进行。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20191001001019693.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MTUyMjQ0,size_16,color_FFFFFF,t_70)----
参考数据
深度学习入门-Deep Learning from Scratch
